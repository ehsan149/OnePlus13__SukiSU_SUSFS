diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index 27f11320b..7e773b7f3 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -16,6 +16,20 @@ config MQ_IOSCHED_KYBER
 	  synchronous writes, it will self-tune queue depths to achieve that
 	  goal.
 
+config MQ_IOSCHED_ADIOS
+	tristate "Adaptive Deadline I/O scheduler"
+	default y
+	help
+	  The Adaptive Deadline I/O Scheduler (ADIOS) is a multi-queue I/O
+	  scheduler with learning-based adaptive latency control.
+
+config MQ_IOSCHED_DEFAULT_ADIOS
+	bool "Enable ADIOS I/O scheduler as default MQ I/O scheduler"
+	depends on MQ_IOSCHED_ADIOS=y
+	default y
+	help
+	  Enable the ADIOS I/O scheduler as the default scheduler for MQ I/O.
+	  
 config IOSCHED_BFQ
 	tristate "BFQ I/O scheduler"
 	select BLK_ICQ
diff --git a/block/Makefile b/block/Makefile
index 46ada9dc8..2fd10afba 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -23,6 +23,7 @@ obj-$(CONFIG_BLK_CGROUP_IOLATENCY)	+= blk-iolatency.o
 obj-$(CONFIG_BLK_CGROUP_IOCOST)	+= blk-iocost.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
+obj-$(CONFIG_MQ_IOSCHED_ADIOS)	+= adios.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
 obj-$(CONFIG_IOSCHED_BFQ)	+= bfq.o
 
diff --git a/block/adios.c b/block/adios.c
new file mode 100644
index 000000000..70ddda1f1
--- /dev/null
+++ b/block/adios.c
@@ -0,0 +1,1325 @@
+// ==================== START: ADIOS v2.2.4 - FULLY PATCHED FOR YOUR KERNEL
+// ==================== SPDX-License-Identifier: GPL-2.0
+/*
+ * Adaptive Deadline I/O Scheduler (ADIOS) v2.2.4
+ * Copyright (C) 2025 Masahito Suzuki
+ * Patched by E.S.H for compatibility with target kernel.
+ */
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+#include <linux/compiler.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/math.h>
+#include <linux/module.h>
+#include <linux/percpu.h>
+#include <linux/rbtree.h>
+#include <linux/sbitmap.h>
+#include <linux/slab.h>
+#include <linux/string.h>
+#include <linux/timekeeping.h>
+
+#include "blk-mq-sched.h"
+#include "blk-mq.h"
+#include "blk.h"
+#include "elevator.h"
+
+#define ADIOS_VERSION "2.2.4-Patched"
+
+// From original v2.2.4 source
+enum adios_op_type
+{
+        ADIOS_READ = 0,
+        ADIOS_WRITE = 1,
+        ADIOS_DISCARD = 2,
+        ADIOS_OTHER = 3,
+        ADIOS_OPTYPES = 4
+};
+static u64 default_global_latency_window = 32000000ULL;
+static u8 default_bq_refill_below_ratio = 25;
+static u32 default_lm_shrink_at_kreqs = 5000;
+static u32 default_lm_shrink_at_gbytes = 50;
+static u32 default_lm_shrink_resist = 2;
+static u64 default_latency_target[ADIOS_OPTYPES]
+    = { [ADIOS_READ] = 1ULL * NSEC_PER_MSEC,
+        [ADIOS_WRITE] = 2000ULL * NSEC_PER_MSEC,
+        [ADIOS_DISCARD] = 8000ULL * NSEC_PER_MSEC };
+static u32 default_batch_limit[ADIOS_OPTYPES] = { [ADIOS_READ] = 24,
+                                                  [ADIOS_WRITE] = 48,
+                                                  [ADIOS_DISCARD] = 1,
+                                                  [ADIOS_OTHER] = 1 };
+static u32 default_dl_prio[2] = { 7, 0 };
+#define LM_BLOCK_SIZE_THRESHOLD 4096
+#define LM_SAMPLES_THRESHOLD 1024
+#define LM_INTERVAL_THRESHOLD 1500
+#define LM_OUTLIER_PERCENTILE 99
+#define LM_LAT_BUCKET_COUNT 64
+struct latency_bucket_small
+{
+        u64 sum_latency;
+        u32 count;
+};
+struct latency_bucket_large
+{
+        u64 sum_latency;
+        u64 sum_block_size;
+        u32 count;
+};
+struct lm_buckets
+{
+        struct latency_bucket_small small_bucket[LM_LAT_BUCKET_COUNT];
+        struct latency_bucket_large large_bucket[LM_LAT_BUCKET_COUNT];
+};
+struct latency_model
+{
+        seqlock_t lock;
+        u64 base;
+        u64 slope;
+        u64 small_sum_delay;
+        u64 small_count;
+        u64 large_sum_delay;
+        u64 large_sum_bsize;
+        u64 last_update_jiffies;
+        struct lm_buckets __percpu *pcpu_buckets;
+        u32 lm_shrink_at_kreqs;
+        u32 lm_shrink_at_gbytes;
+        u8 lm_shrink_resist;
+};
+#define ADIOS_BQ_PAGES 2
+#define ADIOS_MAX_INSERTS_PER_LOCK 16
+struct adios_data
+{
+        spinlock_t pq_lock;
+        struct list_head prio_queue;
+        struct rb_root_cached dl_tree[2];
+        spinlock_t lock;
+        u8 dl_queued;
+        s64 dl_bias;
+        s32 dl_prio[2];
+        u64 global_latency_window;
+        u64 latency_target[ADIOS_OPTYPES];
+        u32 batch_limit[ADIOS_OPTYPES];
+        u32 batch_actual_max_size[ADIOS_OPTYPES];
+        u32 batch_actual_max_total;
+        u32 async_depth;
+        u8 bq_refill_below_ratio;
+        bool bq_page;
+        bool more_bq_ready;
+        struct list_head batch_queue[ADIOS_BQ_PAGES][ADIOS_OPTYPES];
+        u32 batch_count[ADIOS_BQ_PAGES][ADIOS_OPTYPES];
+        spinlock_t bq_lock;
+        struct lm_buckets *aggr_buckets;
+        struct latency_model latency_model[ADIOS_OPTYPES];
+        struct timer_list update_timer;
+        atomic64_t total_pred_lat;
+        struct kmem_cache *rq_data_pool;
+        struct kmem_cache *dl_group_pool;
+        struct request_queue *queue;
+};
+struct dl_group
+{
+        struct rb_node node;
+        struct list_head rqs;
+        u64 deadline;
+};
+struct adios_rq_data
+{
+        struct list_head *dl_group;
+        struct list_head dl_node;
+        struct request *rq;
+        u64 deadline;
+        u64 pred_lat;
+        u32 block_size;
+};
+static const int adios_prio_to_weight[40]
+    = { 88761, 71755, 56483, 46273, 36291, 29154, 23254, 18705, 14949, 11916,
+        9548,  7620,  6100,  4904,  3906,  3121,  2501,  1991,  1586,  1277,
+        1024,  820,   655,   526,   423,   335,   272,   215,   172,   137,
+        110,   87,    70,    56,    45,    36,    29,    23,    18,    15 };
+
+// Forward declarations and helper stubs for compatibility
+static void latency_model_update (struct adios_data *ad,
+                                  struct latency_model *model);
+static void
+adios_init_icq (struct io_cq *icq)
+{ /* No-op */
+}
+static void
+adios_exit_icq (struct io_cq *icq)
+{ /* No-op */
+}
+static void
+adios_limit_depth (blk_opf_t opf, struct blk_mq_alloc_data *data)
+{ /* No-op */
+}
+static int
+adios_request_merge (struct request_queue *q, struct request **req,
+                     struct bio *bio)
+{
+        return ELEVATOR_NO_MERGE;
+}
+
+// All functions from the original v2.2.4 source file
+static u32
+lm_count_small_entries (struct latency_bucket_small *buckets)
+{
+        u32 c = 0;
+        u8 i;
+        for (i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+                c += buckets[i].count;
+        return c;
+}
+static bool
+lm_update_small_buckets (struct latency_model *model,
+                         struct latency_bucket_small *buckets, u32 total_count,
+                         bool count_all)
+{
+        u64 sum_latency = 0;
+        u32 sum_count = 0, cumulative_count = 0, threshold_count = 0;
+        u8 outlier_threshold_bucket = 0,
+           outlier_percentile = LM_OUTLIER_PERCENTILE, reduction;
+        if (count_all)
+                outlier_percentile = 100;
+        threshold_count = (total_count * outlier_percentile) / 100;
+        for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+                {
+                        cumulative_count += buckets[i].count;
+                        if (cumulative_count >= threshold_count)
+                                {
+                                        outlier_threshold_bucket = i;
+                                        break;
+                                }
+                }
+        for (u8 i = 0; i <= outlier_threshold_bucket; i++)
+                {
+                        struct latency_bucket_small *b = &buckets[i];
+                        if (i < outlier_threshold_bucket)
+                                {
+                                        sum_latency += b->sum_latency;
+                                        sum_count += b->count;
+                                }
+                        else
+                                {
+                                        u64 rem
+                                            = threshold_count
+                                              - (cumulative_count - b->count);
+                                        if (b->count > 0)
+                                                {
+                                                        sum_latency += div_u64 (
+                                                            (b->sum_latency
+                                                             * rem),
+                                                            b->count);
+                                                        sum_count += rem;
+                                                }
+                                }
+                }
+        if (model->small_count >= 1000ULL * model->lm_shrink_at_kreqs)
+                {
+                        reduction = model->lm_shrink_resist;
+                        if (model->small_count >> reduction)
+                                {
+                                        model->small_sum_delay
+                                            -= model->small_sum_delay
+                                               >> reduction;
+                                        model->small_count
+                                            -= model->small_count >> reduction;
+                                }
+                }
+        model->small_sum_delay += sum_latency;
+        model->small_count += sum_count;
+        return true;
+}
+static u32
+lm_count_large_entries (struct latency_bucket_large *buckets)
+{
+        u32 c = 0;
+        u8 i;
+        for (i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+                c += buckets[i].count;
+        return c;
+}
+static bool
+lm_update_large_buckets (struct latency_model *m,
+                         struct latency_bucket_large *b, u32 total, bool all)
+{
+        s64 sum_lat = 0;
+        u64 sum_bsize = 0, intercept;
+        u32 cum_cnt = 0, thresh_cnt = 0;
+        u8 out_thresh_bkt = 0, out_pct = LM_OUTLIER_PERCENTILE, reduc;
+        if (all)
+                out_pct = 100;
+        thresh_cnt = (total * out_pct) / 100;
+        for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+                {
+                        cum_cnt += b[i].count;
+                        if (cum_cnt >= thresh_cnt)
+                                {
+                                        out_thresh_bkt = i;
+                                        break;
+                                }
+                }
+        for (u8 i = 0; i <= out_thresh_bkt; i++)
+                {
+                        struct latency_bucket_large *p = &b[i];
+                        if (i < out_thresh_bkt)
+                                {
+                                        sum_lat += p->sum_latency;
+                                        sum_bsize += p->sum_block_size;
+                                }
+                        else
+                                {
+                                        u64 rem
+                                            = thresh_cnt - (cum_cnt - p->count);
+                                        if (p->count > 0)
+                                                {
+                                                        sum_lat += div_u64 (
+                                                            (p->sum_latency
+                                                             * rem),
+                                                            p->count);
+                                                        sum_bsize += div_u64 (
+                                                            (p->sum_block_size
+                                                             * rem),
+                                                            p->count);
+                                                }
+                                }
+                }
+        if (m->large_sum_bsize >= 0x40000000ULL * m->lm_shrink_at_gbytes)
+                {
+                        reduc = m->lm_shrink_resist;
+                        if (m->large_sum_bsize >> reduc)
+                                {
+                                        m->large_sum_delay
+                                            -= m->large_sum_delay >> reduc;
+                                        m->large_sum_bsize
+                                            -= m->large_sum_bsize >> reduc;
+                                }
+                }
+        intercept = m->base * thresh_cnt;
+        if (sum_lat > intercept)
+                sum_lat -= intercept;
+        m->large_sum_delay += sum_lat;
+        m->large_sum_bsize += sum_bsize;
+        return true;
+}
+static void
+reset_buckets (struct lm_buckets *buckets)
+{
+        memset (buckets, 0, sizeof (*buckets));
+}
+static void
+lm_reset_pcpu_buckets (struct latency_model *model)
+{
+        int cpu;
+        for_each_possible_cpu (cpu)
+            reset_buckets (per_cpu_ptr (model->pcpu_buckets, cpu));
+}
+static void
+latency_model_update (struct adios_data *ad, struct latency_model *m)
+{
+        u64 now;
+        u32 small_c, large_c;
+        bool time_el, small_p = false, large_p = false;
+        struct lm_buckets *aggr = ad->aggr_buckets;
+        struct latency_bucket_small *asb;
+        struct latency_bucket_large *alb;
+        struct lm_buckets *pcpu_b;
+        unsigned long flags;
+        int cpu;
+        reset_buckets (ad->aggr_buckets);
+        write_seqlock_irqsave (&m->lock, flags);
+        for_each_possible_cpu (cpu)
+        {
+                pcpu_b = per_cpu_ptr (m->pcpu_buckets, cpu);
+                for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+                        {
+                                if (pcpu_b->small_bucket[i].count)
+                                        {
+                                                asb = &aggr->small_bucket[i];
+                                                asb->count
+                                                    += pcpu_b->small_bucket[i]
+                                                           .count;
+                                                asb->sum_latency
+                                                    += pcpu_b->small_bucket[i]
+                                                           .sum_latency;
+                                        }
+                                if (pcpu_b->large_bucket[i].count)
+                                        {
+                                                alb = &aggr->large_bucket[i];
+                                                alb->count
+                                                    += pcpu_b->large_bucket[i]
+                                                           .count;
+                                                alb->sum_latency
+                                                    += pcpu_b->large_bucket[i]
+                                                           .sum_latency;
+                                                alb->sum_block_size
+                                                    += pcpu_b->large_bucket[i]
+                                                           .sum_block_size;
+                                        }
+                        }
+                reset_buckets (pcpu_b);
+        }
+        now = jiffies;
+        time_el = unlikely (!m->base)
+                  || m->last_update_jiffies
+                             + msecs_to_jiffies (LM_INTERVAL_THRESHOLD)
+                         <= now;
+        small_c = lm_count_small_entries (aggr->small_bucket);
+        large_c = lm_count_large_entries (aggr->large_bucket);
+        if (small_c && (time_el || LM_SAMPLES_THRESHOLD <= small_c || !m->base))
+                small_p = lm_update_small_buckets (m, aggr->small_bucket,
+                                                   small_c, !m->base);
+        if (large_c
+            && (time_el || LM_SAMPLES_THRESHOLD <= large_c || !m->slope))
+                large_p = lm_update_large_buckets (m, aggr->large_bucket,
+                                                   large_c, !m->slope);
+        if (small_p && likely (m->small_count))
+                m->base = div_u64 (m->small_sum_delay, m->small_count);
+        if (large_p && likely (m->large_sum_bsize))
+                m->slope
+                    = div_u64 (m->large_sum_delay,
+                               DIV_ROUND_UP_ULL (m->large_sum_bsize, 1024));
+        if (time_el)
+                m->last_update_jiffies = now;
+        write_sequnlock_irqrestore (&m->lock, flags);
+}
+static u8
+lm_input_bucket_index (u64 m, u64 p)
+{
+        if (m < p * 2)
+                return div_u64 ((m * 20), p);
+        else if (m < p * 5)
+                return div_u64 ((m * 10), p) + 20;
+        else
+                return div_u64 ((m * 3), p) + 40;
+}
+static u64
+latency_model_predict (struct latency_model *model, u32 bsize)
+{
+        u64 res, base, slope;
+        unsigned int seq;
+        do
+                {
+                        seq = read_seqbegin (&model->lock);
+                        base = model->base;
+                        slope = model->slope;
+                }
+        while (read_seqretry (&model->lock, seq));
+        res = base;
+        if (bsize > LM_BLOCK_SIZE_THRESHOLD)
+                res += slope
+                       * DIV_ROUND_UP_ULL (bsize - LM_BLOCK_SIZE_THRESHOLD,
+                                           1024);
+        return res;
+}
+static void
+latency_model_input (struct adios_data *ad, struct latency_model *m, u32 bsize,
+                     u64 lat, u64 pred)
+{
+        u8 idx;
+        struct lm_buckets *b;
+        int cpu = get_cpu ();
+        b = per_cpu_ptr (m->pcpu_buckets, cpu);
+        if (bsize <= LM_BLOCK_SIZE_THRESHOLD)
+                {
+                        idx = lm_input_bucket_index (lat, m->base ?: 1);
+                        if (idx >= LM_LAT_BUCKET_COUNT)
+                                idx = LM_LAT_BUCKET_COUNT - 1;
+                        b->small_bucket[idx].count++;
+                        b->small_bucket[idx].sum_latency += lat;
+                        put_cpu ();
+                        if (unlikely (!m->base))
+                                {
+                                        latency_model_update (ad, m);
+                                        return;
+                                }
+                }
+        else
+                {
+                        if (!m->base || !pred)
+                                {
+                                        put_cpu ();
+                                        return;
+                                }
+                        idx = lm_input_bucket_index (lat, pred);
+                        if (idx >= LM_LAT_BUCKET_COUNT)
+                                idx = LM_LAT_BUCKET_COUNT - 1;
+                        b->large_bucket[idx].count++;
+                        b->large_bucket[idx].sum_latency += lat;
+                        b->large_bucket[idx].sum_block_size += bsize;
+                        put_cpu ();
+                }
+}
+static inline struct adios_rq_data *
+get_rq_data (struct request *rq)
+{
+        return rq->elv.priv[0];
+}
+static u8
+adios_optype (struct request *rq)
+{
+        switch (rq->cmd_flags & REQ_OP_MASK)
+                {
+                case REQ_OP_READ:
+                        return ADIOS_READ;
+                case REQ_OP_WRITE:
+                        return ADIOS_WRITE;
+                case REQ_OP_DISCARD:
+                        return ADIOS_DISCARD;
+                default:
+                        return ADIOS_OTHER;
+                }
+}
+static inline u8
+adios_optype_not_read (struct request *rq)
+{
+        return (rq->cmd_flags & REQ_OP_MASK) != REQ_OP_READ;
+}
+static void
+add_to_dl_tree (struct adios_data *ad, bool dl_idx, struct request *rq)
+{
+        struct rb_root_cached *root = &ad->dl_tree[dl_idx];
+        struct rb_node **link = &(root->rb_root.rb_node), *parent = NULL;
+        bool leftmost = true;
+        struct adios_rq_data *rd = get_rq_data (rq);
+        struct dl_group *dlg;
+        rd->block_size = blk_rq_bytes (rq);
+        u8 optype = adios_optype (rq);
+        rd->pred_lat = latency_model_predict (&ad->latency_model[optype],
+                                              rd->block_size);
+        rd->deadline
+            = rq->start_time_ns + ad->latency_target[optype] + rd->pred_lat;
+        while (*link)
+                {
+                        dlg = rb_entry (*link, struct dl_group, node);
+                        s64 diff = rd->deadline - dlg->deadline;
+                        parent = *link;
+                        if (diff < 0)
+                                {
+                                        link = &((*link)->rb_left);
+                                }
+                        else if (diff > 0)
+                                {
+                                        link = &((*link)->rb_right);
+                                        leftmost = false;
+                                }
+                        else
+                                {
+                                        goto found;
+                                }
+                }
+        dlg = rb_entry_safe (parent, struct dl_group, node);
+        if (!dlg || dlg->deadline != rd->deadline)
+                {
+                        dlg = kmem_cache_zalloc (ad->dl_group_pool, GFP_ATOMIC);
+                        if (!dlg)
+                                return;
+                        dlg->deadline = rd->deadline;
+                        INIT_LIST_HEAD (&dlg->rqs);
+                        rb_link_node (&dlg->node, parent, link);
+                        rb_insert_color_cached (&dlg->node, root, leftmost);
+                }
+found:
+        list_add_tail (&rd->dl_node, &dlg->rqs);
+        rd->dl_group = &dlg->rqs;
+        ad->dl_queued |= 1 << dl_idx;
+}
+static void
+del_from_dl_tree (struct adios_data *ad, bool dl_idx, struct request *rq)
+{
+        struct rb_root_cached *root = &ad->dl_tree[dl_idx];
+        struct adios_rq_data *rd = get_rq_data (rq);
+        struct dl_group *dlg
+            = container_of (rd->dl_group, struct dl_group, rqs);
+        list_del_init (&rd->dl_node);
+        if (list_empty (&dlg->rqs))
+                {
+                        rb_erase_cached (&dlg->node, root);
+                        kmem_cache_free (ad->dl_group_pool, dlg);
+                }
+        rd->dl_group = NULL;
+        if (RB_EMPTY_ROOT (&ad->dl_tree[dl_idx].rb_root))
+                ad->dl_queued &= ~(1 << dl_idx);
+}
+static void
+remove_request (struct adios_data *ad, struct request *rq)
+{
+        bool dl_idx = adios_optype_not_read (rq);
+        struct request_queue *q = rq->q;
+        struct adios_rq_data *rd = get_rq_data (rq);
+        list_del_init (&rq->queuelist);
+        if (rd->dl_group)
+                del_from_dl_tree (ad, dl_idx, rq);
+        elv_rqhash_del (q, rq);
+        if (q->last_merge == rq)
+                q->last_merge = NULL;
+}
+static void
+adios_depth_updated (struct blk_mq_hw_ctx *hctx)
+{
+        struct request_queue *q = hctx->queue;
+        struct adios_data *ad = q->elevator->elevator_data;
+        ad->async_depth = q->nr_requests;
+        sbitmap_queue_min_shallow_depth (&hctx->sched_tags->bitmap_tags, 1);
+}
+static void
+adios_request_merged (struct request_queue *q, struct request *req,
+                      enum elv_merge type)
+{
+        bool dl_idx = adios_optype_not_read (req);
+        struct adios_data *ad = q->elevator->elevator_data;
+        del_from_dl_tree (ad, dl_idx, req);
+        add_to_dl_tree (ad, dl_idx, req);
+}
+static void
+adios_merged_requests (struct request_queue *q, struct request *req,
+                       struct request *next)
+{
+        struct adios_data *ad = q->elevator->elevator_data;
+        lockdep_assert_held (&ad->lock);
+        remove_request (ad, next);
+}
+static bool
+adios_bio_merge (struct request_queue *q, struct bio *bio, unsigned int nr_segs)
+{
+        struct adios_data *ad = q->elevator->elevator_data;
+        struct request *free = NULL;
+        bool ret;
+        scoped_guard (spinlock_irqsave, &ad->lock) ret
+            = blk_mq_sched_try_merge (q, bio, nr_segs, &free);
+        if (free)
+                blk_mq_free_request (free);
+        return ret;
+}
+static void
+insert_request (struct blk_mq_hw_ctx *hctx, struct request *rq,
+                blk_insert_t flags, struct list_head *free)
+{
+        bool dl_idx = adios_optype_not_read (rq);
+        struct request_queue *q = hctx->queue;
+        struct adios_data *ad = q->elevator->elevator_data;
+        if (flags & BLK_MQ_INSERT_AT_HEAD)
+                {
+                        scoped_guard (spinlock_irqsave, &ad->pq_lock)
+                            list_add_tail (&rq->queuelist, &ad->prio_queue);
+                        return;
+                }
+        if (blk_mq_sched_try_insert_merge (q, rq, free))
+                return;
+        add_to_dl_tree (ad, dl_idx, rq);
+        if (rq_mergeable (rq))
+                {
+                        elv_rqhash_add (q, rq);
+                        if (!q->last_merge)
+                                q->last_merge = rq;
+                }
+}
+static void
+adios_insert_requests (struct blk_mq_hw_ctx *hctx, struct list_head *list,
+                       blk_insert_t flags)
+{
+        struct request_queue *q = hctx->queue;
+        struct adios_data *ad = q->elevator->elevator_data;
+        struct request *rq;
+        bool stop = false;
+        LIST_HEAD (free);
+        do
+                {
+                        scoped_guard (
+                            spinlock_irqsave,
+                            &ad->lock) for (int i = 0;
+                                            i < ADIOS_MAX_INSERTS_PER_LOCK; i++)
+                        {
+                                if (list_empty (list))
+                                        {
+                                                stop = true;
+                                                break;
+                                        }
+                                rq = list_first_entry (list, struct request,
+                                                       queuelist);
+                                list_del_init (&rq->queuelist);
+                                insert_request (hctx, rq, flags, &free);
+                        }
+                }
+        while (!stop);
+        blk_mq_free_requests (&free);
+}
+static void
+adios_prepare_request (struct request *rq)
+{
+        struct adios_data *ad = rq->q->elevator->elevator_data;
+        struct adios_rq_data *rd;
+        rq->elv.priv[0] = NULL;
+        rd = kmem_cache_zalloc (ad->rq_data_pool, GFP_ATOMIC);
+        if (WARN (!rd, "adios: Failed to allocate memory\n"))
+                return;
+        rd->rq = rq;
+        rq->elv.priv[0] = rd;
+}
+static struct adios_rq_data *
+get_dl_first_rd (struct adios_data *ad, bool idx)
+{
+        struct rb_root_cached *root = &ad->dl_tree[idx];
+        struct rb_node *first = rb_first_cached (root);
+        if (!first)
+                return NULL;
+        struct dl_group *dlg = rb_entry (first, struct dl_group, node);
+        return list_first_entry (&dlg->rqs, struct adios_rq_data, dl_node);
+}
+static bool
+fill_batch_queues (struct adios_data *ad, u64 cur_lat)
+{
+        struct adios_rq_data *rd;
+        struct request *rq;
+        u32 cnt[ADIOS_OPTYPES] = { 0 };
+        u32 count = 0;
+        u8 optype;
+        bool page = !ad->bq_page, dl_idx, bias_idx, reduce_bias;
+        memset (&ad->batch_count[page], 0, sizeof (ad->batch_count[page]));
+        scoped_guard (spinlock_irqsave, &ad->lock) while (true)
+        {
+                if (!ad->dl_queued)
+                        break;
+                dl_idx = ad->dl_queued >> 1;
+                rd = get_dl_first_rd (ad, dl_idx);
+                if (!rd)
+                        {
+                                ad->dl_queued &= ~(1 << dl_idx);
+                                continue;
+                        }
+                bias_idx = ad->dl_bias < 0;
+                if (ad->dl_queued == 0x3)
+                        {
+                                struct adios_rq_data *trd[2]
+                                    = { get_dl_first_rd (ad, 0), rd };
+                                rd = trd[bias_idx];
+                                reduce_bias = (trd[bias_idx]->deadline
+                                               > trd[!bias_idx]->deadline);
+                        }
+                else
+                        reduce_bias = (bias_idx == dl_idx);
+                rq = rd->rq;
+                optype = adios_optype (rq);
+                if (count
+                    && (!ad->latency_model[optype].base
+                        || ad->batch_count[page][optype]
+                               >= ad->batch_limit[optype]
+                        || (cur_lat + rd->pred_lat)
+                               > ad->global_latency_window))
+                        break;
+                if (reduce_bias)
+                        {
+                                s64 sign = ((int)bias_idx << 1) - 1;
+                                if (unlikely (!rd->pred_lat))
+                                        ad->dl_bias = sign;
+                                else
+                                        ad->dl_bias
+                                            += sign
+                                               * (s64)((rd->pred_lat
+                                                        * adios_prio_to_weight
+                                                            [ad->dl_prio
+                                                                 [bias_idx]
+                                                             + 20])
+                                                       >> 10);
+                        }
+                remove_request (ad, rq);
+                list_add_tail (&rq->queuelist, &ad->batch_queue[page][optype]);
+                atomic64_add (rd->pred_lat, &ad->total_pred_lat);
+                cur_lat += rd->pred_lat;
+                ad->batch_count[page][optype]++;
+                cnt[optype]++;
+                count++;
+        }
+        if (count)
+                {
+                        ad->more_bq_ready = true;
+                        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                                if (ad->batch_actual_max_size[i] < cnt[i])
+                                        ad->batch_actual_max_size[i] = cnt[i];
+                        if (ad->batch_actual_max_total < count)
+                                ad->batch_actual_max_total = count;
+                }
+        return count;
+}
+static void
+flip_bq_page (struct adios_data *ad)
+{
+        ad->more_bq_ready = false;
+        ad->bq_page = !ad->bq_page;
+}
+static struct request *
+dispatch_from_bq (struct adios_data *ad)
+{
+        struct request *rq = NULL;
+        u64 tpl;
+        guard (spinlock_irqsave) (&ad->bq_lock);
+        tpl = atomic64_read (&ad->total_pred_lat);
+        if (!ad->more_bq_ready
+            && (!tpl
+                || tpl < div_u64 (ad->global_latency_window
+                                      * ad->bq_refill_below_ratio,
+                                  100)))
+                fill_batch_queues (ad, tpl);
+again:
+        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                {
+                        if (!list_empty (&ad->batch_queue[ad->bq_page][i]))
+                                {
+                                        rq = list_first_entry (
+                                            &ad->batch_queue[ad->bq_page][i],
+                                            struct request, queuelist);
+                                        list_del_init (&rq->queuelist);
+                                        return rq;
+                                }
+                }
+        if (ad->more_bq_ready)
+                {
+                        flip_bq_page (ad);
+                        goto again;
+                }
+        return NULL;
+}
+static struct request *
+dispatch_from_pq (struct adios_data *ad)
+{
+        struct request *rq = NULL;
+        guard (spinlock_irqsave) (&ad->pq_lock);
+        if (!list_empty (&ad->prio_queue))
+                {
+                        rq = list_first_entry (&ad->prio_queue, struct request,
+                                               queuelist);
+                        list_del_init (&rq->queuelist);
+                }
+        return rq;
+}
+static struct request *
+adios_dispatch_request (struct blk_mq_hw_ctx *hctx)
+{
+        struct adios_data *ad = hctx->queue->elevator->elevator_data;
+        struct request *rq;
+        rq = dispatch_from_pq (ad);
+        if (rq)
+                goto found;
+        rq = dispatch_from_bq (ad);
+        if (!rq)
+                return NULL;
+found:
+        rq->rq_flags |= RQF_STARTED;
+        return rq;
+}
+static void
+update_timer_callback (struct timer_list *t)
+{
+        struct adios_data *ad = from_timer (ad, t, update_timer);
+        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                latency_model_update (ad, &ad->latency_model[i]);
+}
+static void
+adios_completed_request (struct request *rq, u64 now)
+{
+        struct adios_data *ad = rq->q->elevator->elevator_data;
+        struct adios_rq_data *rd = get_rq_data (rq);
+        atomic64_sub (rd->pred_lat, &ad->total_pred_lat);
+        if (!rq->io_start_time_ns || !rd->block_size)
+                return;
+        u64 latency = now - rq->io_start_time_ns;
+        u8 optype = adios_optype (rq);
+        latency_model_input (ad, &ad->latency_model[optype], rd->block_size,
+                             latency, rd->pred_lat);
+        timer_reduce (&ad->update_timer, jiffies + msecs_to_jiffies (100));
+}
+static void
+adios_finish_request (struct request *rq)
+{
+        struct adios_data *ad = rq->q->elevator->elevator_data;
+        if (rq->elv.priv[0])
+                {
+                        kmem_cache_free (ad->rq_data_pool, get_rq_data (rq));
+                        rq->elv.priv[0] = NULL;
+                }
+}
+static inline bool
+pq_has_work (struct adios_data *ad)
+{
+        guard (spinlock_irqsave) (&ad->pq_lock);
+        return !list_empty (&ad->prio_queue);
+}
+static inline bool
+bq_has_work (struct adios_data *ad)
+{
+        guard (spinlock_irqsave) (&ad->bq_lock);
+        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                if (!list_empty (&ad->batch_queue[ad->bq_page][i]))
+                        return true;
+        return ad->more_bq_ready;
+}
+static inline bool
+dl_tree_has_work (struct adios_data *ad)
+{
+        guard (spinlock_irqsave) (&ad->lock);
+        return ad->dl_queued;
+}
+static bool
+adios_has_work (struct blk_mq_hw_ctx *hctx)
+{
+        struct adios_data *ad = hctx->queue->elevator->elevator_data;
+        return pq_has_work (ad) || bq_has_work (ad) || dl_tree_has_work (ad);
+}
+static int
+adios_init_hctx (struct blk_mq_hw_ctx *hctx, unsigned int idx)
+{
+        adios_depth_updated (hctx);
+        return 0;
+}
+static void
+adios_exit_sched (struct elevator_queue *e)
+{
+        struct adios_data *ad = e->elevator_data;
+        timer_shutdown_sync (&ad->update_timer);
+        WARN_ON_ONCE (!list_empty (&ad->prio_queue));
+        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                {
+                        struct latency_model *m = &ad->latency_model[i];
+                        free_percpu (m->pcpu_buckets);
+                }
+        kfree (ad->aggr_buckets);
+        if (ad->rq_data_pool)
+                kmem_cache_destroy (ad->rq_data_pool);
+        if (ad->dl_group_pool)
+                kmem_cache_destroy (ad->dl_group_pool);
+        blk_stat_disable_accounting (ad->queue);
+        kfree (ad);
+}
+static int
+adios_init_sched (struct request_queue *q, struct elevator_type *e)
+{
+        struct adios_data *ad;
+        struct elevator_queue *eq;
+        int r = -ENOMEM;
+        int cpu = 0;
+        eq = elevator_alloc (q, e);
+        if (!eq)
+                return r;
+        ad = kzalloc_node (sizeof (*ad), GFP_KERNEL, q->node);
+        if (!ad)
+                goto put_eq;
+        ad->rq_data_pool
+            = kmem_cache_create ("rq_data_pool", sizeof (struct adios_rq_data),
+                                 0, SLAB_HWCACHE_ALIGN, NULL);
+        if (!ad->rq_data_pool)
+                {
+                        pr_err ("adios: Failed to create rq_data_pool\n");
+                        goto free_ad;
+                }
+        ad->dl_group_pool
+            = kmem_cache_create ("dl_group_pool", sizeof (struct dl_group), 0,
+                                 SLAB_HWCACHE_ALIGN, NULL);
+        if (!ad->dl_group_pool)
+                {
+                        pr_err ("adios: Failed to create dl_group_pool\n");
+                        goto destroy_rq_data_pool;
+                }
+        eq->elevator_data = ad;
+        ad->global_latency_window = default_global_latency_window;
+        ad->bq_refill_below_ratio = default_bq_refill_below_ratio;
+        INIT_LIST_HEAD (&ad->prio_queue);
+        for (u8 i = 0; i < 2; i++)
+                ad->dl_tree[i] = RB_ROOT_CACHED;
+        ad->dl_bias = 0;
+        ad->dl_queued = 0x0;
+        for (u8 i = 0; i < 2; i++)
+                ad->dl_prio[i] = default_dl_prio[i];
+        ad->aggr_buckets = kzalloc (sizeof (*ad->aggr_buckets), GFP_KERNEL);
+        if (!ad->aggr_buckets)
+                {
+                        pr_err (
+                            "adios: Failed to allocate aggregation buckets\n");
+                        goto destroy_dl_group_pool;
+                }
+        for (u8 optype = 0; optype < ADIOS_OPTYPES; optype++)
+                {
+                        struct latency_model *m = &ad->latency_model[optype];
+                        seqlock_init (&m->lock);
+                        m->pcpu_buckets = alloc_percpu (struct lm_buckets);
+                        if (!m->pcpu_buckets)
+                                goto free_buckets;
+                        m->last_update_jiffies = jiffies;
+                        m->lm_shrink_at_kreqs = default_lm_shrink_at_kreqs;
+                        m->lm_shrink_at_gbytes = default_lm_shrink_at_gbytes;
+                        m->lm_shrink_resist = default_lm_shrink_resist;
+                        ad->latency_target[optype]
+                            = default_latency_target[optype];
+                        ad->batch_limit[optype] = default_batch_limit[optype];
+                }
+        timer_setup (&ad->update_timer, update_timer_callback, 0);
+        for (u8 page = 0; page < ADIOS_BQ_PAGES; page++)
+                for (u8 optype = 0; optype < ADIOS_OPTYPES; optype++)
+                        INIT_LIST_HEAD (&ad->batch_queue[page][optype]);
+        spin_lock_init (&ad->lock);
+        spin_lock_init (&ad->pq_lock);
+        spin_lock_init (&ad->bq_lock);
+        blk_queue_flag_set (QUEUE_FLAG_SQ_SCHED, q);
+        ad->queue = q;
+        blk_stat_enable_accounting (q);
+        q->elevator = eq;
+        return 0;
+free_buckets:
+        pr_err ("adios: Failed to allocate per-cpu buckets\n");
+        while (--cpu >= 0)
+                {
+                        struct latency_model *pm = &ad->latency_model[cpu];
+                        free_percpu (pm->pcpu_buckets);
+                }
+        kfree (ad->aggr_buckets);
+destroy_dl_group_pool:
+        kmem_cache_destroy (ad->dl_group_pool);
+destroy_rq_data_pool:
+        kmem_cache_destroy (ad->rq_data_pool);
+free_ad:
+        kfree (ad);
+put_eq:
+        kobject_put (&eq->kobj);
+        return r;
+}
+static void
+load_latency_model (struct latency_model *model, u64 base, u64 slope)
+{
+        write_seqlock_bh (&model->lock);
+        model->last_update_jiffies = jiffies;
+        model->base = base;
+        model->small_sum_delay = base;
+        model->small_count = 1;
+        model->slope = slope;
+        model->large_sum_delay = slope;
+        model->large_sum_bsize = 1024;
+        lm_reset_pcpu_buckets (model);
+        write_sequnlock_bh (&model->lock);
+}
+#define SYSFS_OPTYPE_DECL(n, t)                                                \
+        static ssize_t adios_lat_model_##n##_show (struct elevator_queue *e,   \
+                                                   char *p)                    \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                struct latency_model *m = &d->latency_model[t];                \
+                ssize_t l = 0;                                                 \
+                u64 b, s;                                                      \
+                unsigned int q;                                                \
+                do                                                             \
+                        {                                                      \
+                                q = read_seqbegin (&m->lock);                  \
+                                b = m->base;                                   \
+                                s = m->slope;                                  \
+                        }                                                      \
+                while (read_seqretry (&m->lock, q));                           \
+                l += sprintf (p, "base : %llu ns\n", b);                       \
+                l += sprintf (p + l, "slope: %llu ns/KiB\n", s);               \
+                return l;                                                      \
+        }                                                                      \
+        static ssize_t adios_lat_model_##n##_store (struct elevator_queue *e,  \
+                                                    const char *p, size_t c)   \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                struct latency_model *m = &d->latency_model[t];                \
+                u64 b, s;                                                      \
+                int r;                                                         \
+                r = sscanf (p, "%llu %llu", &b, &s);                           \
+                if (r != 2)                                                    \
+                        return -EINVAL;                                        \
+                load_latency_model (m, b, s);                                  \
+                reset_buckets (d->aggr_buckets);                               \
+                return c;                                                      \
+        }                                                                      \
+        static ssize_t adios_lat_target_##n##_store (struct elevator_queue *e, \
+                                                     const char *p, size_t c)  \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                unsigned long val;                                             \
+                int r;                                                         \
+                r = kstrtoul (p, 10, &val);                                    \
+                if (r)                                                         \
+                        return r;                                              \
+                d->latency_model[t].base = 0ULL;                               \
+                d->latency_target[t] = val;                                    \
+                return c;                                                      \
+        }                                                                      \
+        static ssize_t adios_lat_target_##n##_show (struct elevator_queue *e,  \
+                                                    char *p)                   \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                return sprintf (p, "%llu\n", d->latency_target[t]);            \
+        }                                                                      \
+        static ssize_t adios_batch_limit_##n##_store (                         \
+            struct elevator_queue *e, const char *p, size_t c)                 \
+        {                                                                      \
+                unsigned long val;                                             \
+                int r;                                                         \
+                r = kstrtoul (p, 10, &val);                                    \
+                if (r || val == 0)                                             \
+                        return -EINVAL;                                        \
+                struct adios_data *d = e->elevator_data;                       \
+                d->batch_limit[t] = val;                                       \
+                return c;                                                      \
+        }                                                                      \
+        static ssize_t adios_batch_limit_##n##_show (struct elevator_queue *e, \
+                                                     char *p)                  \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                return sprintf (p, "%u\n", d->batch_limit[t]);                 \
+        }
+SYSFS_OPTYPE_DECL (read, ADIOS_READ);
+SYSFS_OPTYPE_DECL (write, ADIOS_WRITE);
+SYSFS_OPTYPE_DECL (discard, ADIOS_DISCARD);
+static ssize_t
+adios_batch_actual_max_show (struct elevator_queue *e, char *p)
+{
+        struct adios_data *d = e->elevator_data;
+        u32 tot, r, w, dsc;
+        tot = d->batch_actual_max_total;
+        r = d->batch_actual_max_size[ADIOS_READ];
+        w = d->batch_actual_max_size[ADIOS_WRITE];
+        dsc = d->batch_actual_max_size[ADIOS_DISCARD];
+        return sprintf (p, "Total: %u\nDiscard: %u\nRead: %u\nWrite: %u\n", tot,
+                        dsc, r, w);
+}
+static ssize_t
+adios_global_latency_window_store (struct elevator_queue *e, const char *p,
+                                   size_t c)
+{
+        struct adios_data *d = e->elevator_data;
+        unsigned long val;
+        int r;
+        r = kstrtoul (p, 10, &val);
+        if (r)
+                return r;
+        d->global_latency_window = val;
+        return c;
+}
+static ssize_t
+adios_global_latency_window_show (struct elevator_queue *e, char *p)
+{
+        struct adios_data *d = e->elevator_data;
+        return sprintf (p, "%llu\n", d->global_latency_window);
+}
+static ssize_t
+adios_bq_refill_below_ratio_show (struct elevator_queue *e, char *p)
+{
+        struct adios_data *d = e->elevator_data;
+        return sprintf (p, "%d\n", d->bq_refill_below_ratio);
+}
+static ssize_t
+adios_bq_refill_below_ratio_store (struct elevator_queue *e, const char *p,
+                                   size_t c)
+{
+        struct adios_data *d = e->elevator_data;
+        int val;
+        int r;
+        r = kstrtoint (p, 10, &val);
+        if (r || val < 0 || val > 100)
+                return -EINVAL;
+        d->bq_refill_below_ratio = val;
+        return c;
+}
+static ssize_t
+adios_read_priority_show (struct elevator_queue *e, char *p)
+{
+        struct adios_data *d = e->elevator_data;
+        return sprintf (p, "%d\n", d->dl_prio[0]);
+}
+static ssize_t
+adios_read_priority_store (struct elevator_queue *e, const char *p, size_t c)
+{
+        struct adios_data *d = e->elevator_data;
+        int val;
+        int r;
+        r = kstrtoint (p, 10, &val);
+        if (r || val < -20 || val > 19)
+                return -EINVAL;
+        guard (spinlock_irqsave) (&d->lock);
+        d->dl_prio[0] = val;
+        d->dl_bias = 0;
+        return c;
+}
+static ssize_t
+adios_reset_bq_stats_store (struct elevator_queue *e, const char *p, size_t c)
+{
+        struct adios_data *d = e->elevator_data;
+        unsigned long val;
+        int r;
+        r = kstrtoul (p, 10, &val);
+        if (r || val != 1)
+                return -EINVAL;
+        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                d->batch_actual_max_size[i] = 0;
+        d->batch_actual_max_total = 0;
+        return c;
+}
+static ssize_t
+adios_reset_lat_model_store (struct elevator_queue *e, const char *p, size_t c)
+{
+        struct adios_data *d = e->elevator_data;
+        struct latency_model *m;
+        int r;
+        if (!strchr (p, ' '))
+                {
+                        unsigned long val;
+                        r = kstrtoul (p, 10, &val);
+                        if (r || val != 1)
+                                return -EINVAL;
+                        for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+                                {
+                                        m = &d->latency_model[i];
+                                        write_seqlock_bh (&m->lock);
+                                        m->last_update_jiffies = jiffies;
+                                        m->base = 0ULL;
+                                        m->slope = 0ULL;
+                                        m->small_sum_delay = 0ULL;
+                                        m->small_count = 0ULL;
+                                        m->large_sum_delay = 0ULL;
+                                        m->large_sum_bsize = 0ULL;
+                                        lm_reset_pcpu_buckets (m);
+                                        write_sequnlock_bh (&m->lock);
+                                }
+                }
+        else
+                {
+                        u64 params[3][2];
+                        r = sscanf (p, "%llu %llu %llu %llu %llu %llu",
+                                    &params[0][0], &params[0][1], &params[1][0],
+                                    &params[1][1], &params[2][0],
+                                    &params[2][1]);
+                        if (r != 6)
+                                return -EINVAL;
+                        for (u8 i = 0; i <= ADIOS_DISCARD; i++)
+                                {
+                                        m = &d->latency_model[i];
+                                        load_latency_model (m, params[i][0],
+                                                            params[i][1]);
+                                }
+                }
+        reset_buckets (d->aggr_buckets);
+        return c;
+}
+static ssize_t
+adios_version_show (struct elevator_queue *e, char *p)
+{
+        return sprintf (p, "%s\n", ADIOS_VERSION);
+}
+#define SHRINK_THRESHOLD_ATTR_RW(n, mf, min, max)                              \
+        static ssize_t adios_shrink_##n##_store (struct elevator_queue *e,     \
+                                                 const char *p, size_t c)      \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                unsigned long v;                                               \
+                int r;                                                         \
+                r = kstrtoul (p, 10, &v);                                      \
+                if (r || v < min || v > max)                                   \
+                        return -EINVAL;                                        \
+                for (u8 i = 0; i < ADIOS_OPTYPES; i++)                         \
+                        {                                                      \
+                                struct latency_model *m                        \
+                                    = &d->latency_model[i];                    \
+                                write_seqlock_bh (&m->lock);                   \
+                                m->mf = v;                                     \
+                                write_sequnlock_bh (&m->lock);                 \
+                        }                                                      \
+                return c;                                                      \
+        }                                                                      \
+        static ssize_t adios_shrink_##n##_show (struct elevator_queue *e,      \
+                                                char *p)                       \
+        {                                                                      \
+                struct adios_data *d = e->elevator_data;                       \
+                u32 v = 0;                                                     \
+                unsigned int q;                                                \
+                struct latency_model *m = &d->latency_model[0];                \
+                do                                                             \
+                        {                                                      \
+                                q = read_seqbegin (&m->lock);                  \
+                                v = m->mf;                                     \
+                        }                                                      \
+                while (read_seqretry (&m->lock, q));                           \
+                return sprintf (p, "%u\n", v);                                 \
+        }
+SHRINK_THRESHOLD_ATTR_RW (at_kreqs, lm_shrink_at_kreqs, 1, 100000)
+SHRINK_THRESHOLD_ATTR_RW (at_gbytes, lm_shrink_at_gbytes, 1, 1000)
+SHRINK_THRESHOLD_ATTR_RW (resist, lm_shrink_resist, 1, 3)
+#define AD_ATTR(n, s, t) __ATTR (n, 0644, s, t)
+#define AD_ATTR_RW(n) __ATTR (n, 0644, adios_##n##_show, adios_##n##_store)
+#define AD_ATTR_RO(n) __ATTR (n, 0444, adios_##n##_show, NULL)
+#define AD_ATTR_WO(n) __ATTR (n, 0200, NULL, adios_##n##_store)
+static struct elv_fs_entry adios_sched_attrs[]
+    = { AD_ATTR_RO (batch_actual_max),
+        AD_ATTR_RW (bq_refill_below_ratio),
+        AD_ATTR_RW (global_latency_window),
+        AD_ATTR_RW (batch_limit_read),
+        AD_ATTR_RW (batch_limit_write),
+        AD_ATTR_RW (batch_limit_discard),
+        AD_ATTR_RW (lat_model_read),
+        AD_ATTR_RW (lat_model_write),
+        AD_ATTR_RW (lat_model_discard),
+        AD_ATTR_RW (lat_target_read),
+        AD_ATTR_RW (lat_target_write),
+        AD_ATTR_RW (lat_target_discard),
+        AD_ATTR_RW (shrink_at_kreqs),
+        AD_ATTR_RW (shrink_at_gbytes),
+        AD_ATTR_RW (shrink_resist),
+        AD_ATTR_RW (read_priority),
+        AD_ATTR_WO (reset_bq_stats),
+        AD_ATTR_WO (reset_lat_model),
+        AD_ATTR (adios_version, adios_version_show, NULL),
+        __ATTR_NULL };
+
+static struct elevator_type mq_adios = {
+	.ops = {
+		.next_request		= elv_rb_latter_request,
+		.former_request		= elv_rb_former_request,
+		.limit_depth		= adios_limit_depth,
+		.depth_updated		= adios_depth_updated,
+		.request_merged		= adios_request_merged,
+		.requests_merged	= adios_merged_requests,
+		.bio_merge			= adios_bio_merge,
+		.request_merge      = adios_request_merge,
+		.insert_requests	= adios_insert_requests,
+		.prepare_request	= adios_prepare_request,
+		.dispatch_request	= adios_dispatch_request,
+		.completed_request	= adios_completed_request,
+		.finish_request		= adios_finish_request,
+		.has_work			= adios_has_work,
+		.init_hctx			= adios_init_hctx,
+		.init_sched			= adios_init_sched,
+		.exit_sched			= adios_exit_sched,
+        .init_icq           = adios_init_icq,
+        .exit_icq           = adios_exit_icq,
+	},
+    .icq_size = sizeof(struct io_cq),
+    .icq_align = __alignof__(struct io_cq),
+	.elevator_attrs = adios_sched_attrs,
+	.elevator_name = "adios",
+	.elevator_owner = THIS_MODULE,
+};
+MODULE_ALIAS ("mq-adios-iosched");
+
+#define ADIOS_PROGNAME "Adaptive Deadline I/O Scheduler"
+#define ADIOS_AUTHOR "Masahito Suzuki"
+
+// Initialize the ADIOS scheduler module
+static int __init
+adios_init (void)
+{
+        printk (KERN_INFO "%s %s by %s\n", ADIOS_PROGNAME, ADIOS_VERSION,
+                ADIOS_AUTHOR);
+        return elv_register (&mq_adios);
+}
+
+// Exit the ADIOS scheduler module
+static void __exit
+adios_exit (void)
+{
+        elv_unregister (&mq_adios);
+}
+
+module_init (adios_init);
+module_exit (adios_exit);
+
+MODULE_AUTHOR (ADIOS_AUTHOR);
+MODULE_LICENSE ("GPL");
+MODULE_DESCRIPTION (ADIOS_PROGNAME);
\ No newline at end of file
