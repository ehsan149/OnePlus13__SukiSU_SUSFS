diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index 27f11320b..7998e76e8 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -16,6 +16,20 @@ config MQ_IOSCHED_KYBER
 	  synchronous writes, it will self-tune queue depths to achieve that
 	  goal.
 
+config MQ_IOSCHED_ADIOS
+	tristate "Adaptive Deadline I/O scheduler"
+	default y
+	help
+	  The Adaptive Deadline I/O Scheduler (ADIOS) is a multi-queue I/O
+	  scheduler with learning-based adaptive latency control.
+
+config MQ_IOSCHED_DEFAULT_ADIOS
+	bool "Enable ADIOS I/O scheduler as default MQ I/O scheduler"
+	depends on MQ_IOSCHED_ADIOS=y
+	default y
+	help
+	  Enable the ADIOS I/O scheduler as the default scheduler for MQ I/O.
+	  	  
 config IOSCHED_BFQ
 	tristate "BFQ I/O scheduler"
 	select BLK_ICQ
diff --git a/block/Makefile b/block/Makefile
index 46ada9dc8..2fd10afba 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -23,6 +23,7 @@ obj-$(CONFIG_BLK_CGROUP_IOLATENCY)	+= blk-iolatency.o
 obj-$(CONFIG_BLK_CGROUP_IOCOST)	+= blk-iocost.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
+obj-$(CONFIG_MQ_IOSCHED_ADIOS)	+= adios.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
 obj-$(CONFIG_IOSCHED_BFQ)	+= bfq.o
 
diff --git a/block/adios.c b/block/adios.c
new file mode 100644
index 000000000..e176ecc86
--- /dev/null
+++ b/block/adios.c
@@ -0,0 +1,1480 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Adaptive Deadline I/O Scheduler (ADIOS)
+ * Copyright (C) 2025 Masahito Suzuki
+ */
+#include <linux/bio.h>
+#include <linux/blkdev.h>
+#include <linux/compiler.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/math.h>
+#include <linux/module.h>
+#include <linux/rbtree.h>
+#include <linux/sbitmap.h>
+#include <linux/slab.h>
+#include <linux/timekeeping.h>
+#include <linux/percpu.h>
+#include <linux/string.h>
+
+#include "elevator.h"
+#include "blk.h"
+#include "blk-mq.h"
+#include "blk-mq-sched.h"
+
+#define ADIOS_VERSION "2.2.5"
+
+// Define operation types supported by ADIOS
+enum adios_op_type {
+	ADIOS_READ    = 0,
+	ADIOS_WRITE   = 1,
+	ADIOS_DISCARD = 2,
+	ADIOS_OTHER   = 3,
+	ADIOS_OPTYPES = 4,
+};
+
+// Global variable to control the latency
+static u64 default_global_latency_window = 32000000ULL;
+// Ratio below which batch queues should be refilled
+static u8  default_bq_refill_below_ratio = 25;
+
+// Dynamic thresholds for shrinkage
+static u32 default_lm_shrink_at_kreqs  =  5000;
+static u32 default_lm_shrink_at_gbytes =    50;
+static u32 default_lm_shrink_resist    =     2;
+
+// Latency targets for each operation type
+static u64 default_latency_target[ADIOS_OPTYPES] = {
+	[ADIOS_READ]    =     1ULL * NSEC_PER_MSEC,
+	[ADIOS_WRITE]   =  2000ULL * NSEC_PER_MSEC,
+	[ADIOS_DISCARD] =  8000ULL * NSEC_PER_MSEC,
+	[ADIOS_OTHER]   =     0ULL * NSEC_PER_MSEC,
+};
+
+// Maximum batch size limits for each operation type
+static u32 default_batch_limit[ADIOS_OPTYPES] = {
+	[ADIOS_READ]    = 24,
+	[ADIOS_WRITE]   = 48,
+	[ADIOS_DISCARD] =  1,
+	[ADIOS_OTHER]   =  1,
+};
+
+static u32 default_dl_prio[2] = {7, 0};
+
+// Thresholds for latency model control
+#define LM_BLOCK_SIZE_THRESHOLD 4096
+#define LM_SAMPLES_THRESHOLD    1024
+#define LM_INTERVAL_THRESHOLD   1500
+#define LM_OUTLIER_PERCENTILE     99
+#define LM_LAT_BUCKET_COUNT       64
+
+// Structure to hold latency bucket data for small requests
+struct latency_bucket_small {
+	u64 sum_latency;
+	u32 count;
+};
+
+// Structure to hold latency bucket data for large requests
+struct latency_bucket_large {
+	u64 sum_latency;
+	u64 sum_block_size;
+	u32 count;
+};
+
+// New structure to hold per-cpu buckets, improving data locality and code clarity.
+struct lm_buckets {
+	struct latency_bucket_small small_bucket[LM_LAT_BUCKET_COUNT];
+	struct latency_bucket_large large_bucket[LM_LAT_BUCKET_COUNT];
+};
+
+// Structure to hold the latency model context data
+struct latency_model {
+	seqlock_t lock;
+	u64 base;
+	u64 slope;
+	u64 small_sum_delay;
+	u64 small_count;
+	u64 large_sum_delay;
+	u64 large_sum_bsize;
+	u64 last_update_jiffies;
+
+	// Per-CPU buckets to avoid lock contention on the completion path
+	struct lm_buckets __percpu *pcpu_buckets;
+
+	u32 lm_shrink_at_kreqs;
+	u32 lm_shrink_at_gbytes;
+	u8  lm_shrink_resist;
+};
+
+#define ADIOS_BQ_PAGES 2
+#define ADIOS_MAX_INSERTS_PER_LOCK 16
+
+// Adios scheduler data
+struct adios_data {
+	spinlock_t pq_lock;
+	struct list_head prio_queue;
+
+	struct rb_root_cached dl_tree[2];
+	spinlock_t lock;
+	u8  dl_queued;
+	s64 dl_bias;
+	s32 dl_prio[2];
+
+	u64 global_latency_window;
+	u64 latency_target[ADIOS_OPTYPES];
+	u32 batch_limit[ADIOS_OPTYPES];
+	u32 batch_actual_max_size[ADIOS_OPTYPES];
+	u32 batch_actual_max_total;
+	u32 async_depth;
+	u8  bq_refill_below_ratio;
+
+	bool bq_page;
+	bool more_bq_ready;
+	struct list_head batch_queue[ADIOS_BQ_PAGES][ADIOS_OPTYPES];
+	u32 batch_count[ADIOS_BQ_PAGES][ADIOS_OPTYPES];
+	spinlock_t bq_lock;
+
+	struct lm_buckets *aggr_buckets;
+
+	struct latency_model latency_model[ADIOS_OPTYPES];
+	struct timer_list update_timer;
+
+	atomic64_t total_pred_lat;
+
+	struct kmem_cache *rq_data_pool;
+	struct kmem_cache *dl_group_pool;
+
+	struct request_queue *queue;
+};
+
+// List of requests with the same deadline in the deadline-sorted tree
+struct dl_group {
+	struct rb_node node;
+	struct list_head rqs;
+	u64 deadline;
+} __attribute__((aligned(64)));
+
+// Structure to hold scheduler-specific data for each request
+struct adios_rq_data {
+	struct list_head *dl_group;
+	struct list_head dl_node;
+
+	struct request *rq;
+	u64 deadline;
+	u64 pred_lat;
+	u32 block_size;
+} __attribute__((aligned(64)));
+
+static const int adios_prio_to_weight[40] = {
+ /* -20 */     88761,     71755,     56483,     46273,     36291,
+ /* -15 */     29154,     23254,     18705,     14949,     11916,
+ /* -10 */      9548,      7620,      6100,      4904,      3906,
+ /*  -5 */      3121,      2501,      1991,      1586,      1277,
+ /*   0 */      1024,       820,       655,       526,       423,
+ /*   5 */       335,       272,       215,       172,       137,
+ /*  10 */       110,        87,        70,        56,        45,
+ /*  15 */        36,        29,        23,        18,        15,
+};
+static void
+adios_init_icq (struct io_cq *icq)
+{ /* No-op */
+}
+static void
+adios_exit_icq (struct io_cq *icq)
+{ /* No-op */
+}
+static int
+adios_request_merge (struct request_queue *q, struct request **req,
+                     struct bio *bio)
+{
+        return ELEVATOR_NO_MERGE;
+}
+// Count the number of entries in aggregated small buckets
+static u32 lm_count_small_entries(struct latency_bucket_small *buckets) {
+	u32 total_count = 0;
+	for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+		total_count += buckets[i].count;
+	return total_count;
+}
+
+// Update the small buckets in the latency model from aggregated data
+static bool lm_update_small_buckets(struct latency_model *model,
+		struct latency_bucket_small *buckets,
+		u32 total_count, bool count_all) {
+	u64 sum_latency = 0;
+	u32 sum_count = 0;
+	u32 cumulative_count = 0, threshold_count = 0;
+	u8  outlier_threshold_bucket = 0;
+	u8  outlier_percentile = LM_OUTLIER_PERCENTILE;
+	u8  reduction;
+
+	if (count_all)
+		outlier_percentile = 100;
+
+	// Calculate the threshold count for outlier detection
+	threshold_count = (total_count * outlier_percentile) / 100;
+
+	// Identify the bucket that corresponds to the outlier threshold
+	for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++) {
+		cumulative_count += buckets[i].count;
+		if (cumulative_count >= threshold_count) {
+			outlier_threshold_bucket = i;
+			break;
+		}
+	}
+
+	// Calculate the average latency, excluding outliers
+	for (u8 i = 0; i <= outlier_threshold_bucket; i++) {
+		struct latency_bucket_small *bucket = &buckets[i];
+		if (i < outlier_threshold_bucket) {
+			sum_latency += bucket->sum_latency;
+			sum_count += bucket->count;
+		} else {
+			// The threshold bucket's contribution is proportional
+			u64 remaining_count =
+				threshold_count - (cumulative_count - bucket->count);
+			if (bucket->count > 0) {
+				sum_latency +=
+					div_u64((bucket->sum_latency * remaining_count), bucket->count);
+				sum_count += remaining_count;
+			}
+		}
+	}
+
+	// Shrink the model if it reaches at the readjustment threshold
+	if (model->small_count >= 1000ULL * model->lm_shrink_at_kreqs) {
+		reduction = model->lm_shrink_resist;
+		if (model->small_count >> reduction) {
+			model->small_sum_delay -= model->small_sum_delay >> reduction;
+			model->small_count     -= model->small_count     >> reduction;
+		}
+	}
+
+	// Accumulate the average latency into the statistics
+	model->small_sum_delay += sum_latency;
+	model->small_count += sum_count;
+
+	return true;
+}
+
+// Count the number of entries in aggregated large buckets
+static u32 lm_count_large_entries(struct latency_bucket_large *buckets) {
+	u32 total_count = 0;
+	for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++)
+		total_count += buckets[i].count;
+	return total_count;
+}
+
+// Update the large buckets in the latency model from aggregated data
+static bool lm_update_large_buckets(struct latency_model *model,
+		struct latency_bucket_large *buckets,
+		u32 total_count, bool count_all) {
+	s64 sum_latency = 0;
+	u64 sum_block_size = 0, intercept;
+	u32 cumulative_count = 0, threshold_count = 0;
+	u8  outlier_threshold_bucket = 0;
+	u8  outlier_percentile = LM_OUTLIER_PERCENTILE;
+	u8  reduction;
+
+	if (count_all)
+		outlier_percentile = 100;
+
+	// Calculate the threshold count for outlier detection
+	threshold_count = (total_count * outlier_percentile) / 100;
+
+	// Identify the bucket that corresponds to the outlier threshold
+	for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++) {
+		cumulative_count += buckets[i].count;
+		if (cumulative_count >= threshold_count) {
+			outlier_threshold_bucket = i;
+			break;
+		}
+	}
+
+	// Calculate the average latency and block size, excluding outliers
+	for (u8 i = 0; i <= outlier_threshold_bucket; i++) {
+		struct latency_bucket_large *bucket = &buckets[i];
+		if (i < outlier_threshold_bucket) {
+			sum_latency += bucket->sum_latency;
+			sum_block_size += bucket->sum_block_size;
+		} else {
+			// The threshold bucket's contribution is proportional
+			u64 remaining_count =
+				threshold_count - (cumulative_count - bucket->count);
+			if (bucket->count > 0) {
+				sum_latency +=
+					div_u64((bucket->sum_latency * remaining_count), bucket->count);
+				sum_block_size +=
+					div_u64((bucket->sum_block_size * remaining_count), bucket->count);
+			}
+		}
+	}
+
+	// Shrink the model if it reaches at the readjustment threshold
+	if (model->large_sum_bsize >= 0x40000000ULL * model->lm_shrink_at_gbytes) {
+		reduction = model->lm_shrink_resist;
+		if (model->large_sum_bsize >> reduction) {
+			model->large_sum_delay -= model->large_sum_delay >> reduction;
+			model->large_sum_bsize -= model->large_sum_bsize >> reduction;
+		}
+	}
+
+	// Accumulate the average delay into the statistics
+	intercept = model->base * threshold_count;
+	if (sum_latency > intercept)
+		sum_latency -= intercept;
+
+	model->large_sum_delay += sum_latency;
+	model->large_sum_bsize += sum_block_size;
+
+	return true;
+}
+
+static void reset_buckets(struct lm_buckets *buckets)
+{ memset(buckets, 0, sizeof(*buckets)); }
+
+static void lm_reset_pcpu_buckets(struct latency_model *model) {
+	int cpu;
+	for_each_possible_cpu(cpu)
+		reset_buckets(per_cpu_ptr(model->pcpu_buckets, cpu));
+}
+
+// Update the latency model parameters and statistics
+static void latency_model_update(
+		struct adios_data *ad, struct latency_model *model) {
+	u64 now;
+	u32 small_count, large_count;
+	bool time_elapsed;
+	bool small_processed = false, large_processed = false;
+	struct lm_buckets *aggr = ad->aggr_buckets;
+	struct latency_bucket_small *asb;
+	struct latency_bucket_large *alb;
+	struct lm_buckets *pcpu_b;
+	unsigned long flags;
+	int cpu;
+
+	reset_buckets(ad->aggr_buckets);
+
+	write_seqlock_irqsave(&model->lock, flags);
+
+	// Aggregate data from all CPUs and reset per-cpu buckets.
+	for_each_possible_cpu(cpu) {
+		pcpu_b = per_cpu_ptr(model->pcpu_buckets, cpu);
+
+		for (u8 i = 0; i < LM_LAT_BUCKET_COUNT; i++) {
+			if (pcpu_b->small_bucket[i].count) {
+				asb = &aggr->small_bucket[i];
+				asb->count          += pcpu_b->small_bucket[i].count;
+				asb->sum_latency    += pcpu_b->small_bucket[i].sum_latency;
+			}
+			if (pcpu_b->large_bucket[i].count) {
+				alb = &aggr->large_bucket[i];
+				alb->count          += pcpu_b->large_bucket[i].count;
+				alb->sum_latency    += pcpu_b->large_bucket[i].sum_latency;
+				alb->sum_block_size += pcpu_b->large_bucket[i].sum_block_size;
+			}
+		}
+		// Reset per-cpu buckets after aggregating
+		reset_buckets(pcpu_b);
+	}
+
+	// Whether enough time has elapsed since the last update
+	now = jiffies;
+	time_elapsed = unlikely(!model->base) || model->last_update_jiffies +
+		msecs_to_jiffies(LM_INTERVAL_THRESHOLD) <= now;
+
+	// Count the number of entries in aggregated buckets
+	small_count = lm_count_small_entries(aggr->small_bucket);
+	large_count = lm_count_large_entries(aggr->large_bucket);
+
+	// Update small buckets
+	if (small_count && (time_elapsed ||
+			LM_SAMPLES_THRESHOLD <= small_count || !model->base))
+		small_processed = lm_update_small_buckets(
+			model, aggr->small_bucket, small_count, !model->base);
+	// Update large buckets
+	if (large_count && (time_elapsed ||
+			LM_SAMPLES_THRESHOLD <= large_count || !model->slope))
+		large_processed = lm_update_large_buckets(
+			model, aggr->large_bucket, large_count, !model->slope);
+
+	// Update the base parameter if small bucket was processed
+	if (small_processed && likely(model->small_count))
+		model->base = div_u64(model->small_sum_delay, model->small_count);
+
+	// Update the slope parameter if large bucket was processed
+	if (large_processed && likely(model->large_sum_bsize))
+		model->slope = div_u64(model->large_sum_delay,
+			DIV_ROUND_UP_ULL(model->large_sum_bsize, 1024));
+
+	// Reset statistics and update last updated jiffies if time has elapsed
+	if (time_elapsed)
+		model->last_update_jiffies = now;
+
+	write_sequnlock_irqrestore(&model->lock, flags);
+}
+
+// Determine the bucket index for a given measured and predicted latency
+static u8 lm_input_bucket_index(u64 measured, u64 predicted) {
+	u8 bucket_index;
+
+	if (measured < predicted * 2)
+		bucket_index = div_u64((measured * 20), predicted);
+	else if (measured < predicted * 5)
+		bucket_index = div_u64((measured * 10), predicted) + 20;
+	else
+		bucket_index = div_u64((measured * 3), predicted) + 40;
+
+	return bucket_index;
+}
+
+// Input latency data into the latency model
+static void latency_model_input(struct adios_data *ad,
+		struct latency_model *model, u32 block_size, u64 latency, u64 pred_lat) {
+	u8 bucket_index;
+	struct lm_buckets *buckets;
+	int cpu = get_cpu();
+
+	buckets = per_cpu_ptr(model->pcpu_buckets, cpu);
+
+	if (block_size <= LM_BLOCK_SIZE_THRESHOLD) {
+		// Handle small requests
+		bucket_index = lm_input_bucket_index(latency, model->base ?: 1);
+
+		if (bucket_index >= LM_LAT_BUCKET_COUNT)
+			bucket_index = LM_LAT_BUCKET_COUNT - 1;
+
+		buckets->small_bucket[bucket_index].count++;
+		buckets->small_bucket[bucket_index].sum_latency += latency;
+
+		put_cpu();
+
+		if (unlikely(!model->base)) {
+			latency_model_update(ad, model);
+			return;
+		}
+	} else {
+		// Handle large requests
+		if (!model->base || !pred_lat) {
+			put_cpu();
+			return;
+		}
+
+		bucket_index = lm_input_bucket_index(latency, pred_lat);
+
+		if (bucket_index >= LM_LAT_BUCKET_COUNT)
+			bucket_index = LM_LAT_BUCKET_COUNT - 1;
+
+		buckets->large_bucket[bucket_index].count++;
+		buckets->large_bucket[bucket_index].sum_latency += latency;
+		buckets->large_bucket[bucket_index].sum_block_size += block_size;
+
+		put_cpu();
+	}
+}
+
+// Predict the latency for a given block size using the latency model
+static u64 latency_model_predict(struct latency_model *model, u32 block_size) {
+	u64 result, base, slope;
+	unsigned int seq;
+
+	do {
+		seq = read_seqbegin(&model->lock);
+		base = model->base;
+		slope = model->slope;
+	} while (read_seqretry(&model->lock, seq));
+
+	result = base;
+	if (block_size > LM_BLOCK_SIZE_THRESHOLD)
+		result += slope *
+			DIV_ROUND_UP_ULL(block_size - LM_BLOCK_SIZE_THRESHOLD, 1024);
+
+	return result;
+}
+
+// Determine the type of operation based on request flags
+static u8 adios_optype(struct request *rq) {
+	switch (rq->cmd_flags & REQ_OP_MASK) {
+	case REQ_OP_READ:
+		return ADIOS_READ;
+	case REQ_OP_WRITE:
+		return ADIOS_WRITE;
+	case REQ_OP_DISCARD:
+		return ADIOS_DISCARD;
+	default:
+		return ADIOS_OTHER;
+	}
+}
+
+static inline u8 adios_optype_not_read(struct request *rq) {
+	return (rq->cmd_flags & REQ_OP_MASK) != REQ_OP_READ;
+}
+
+// Helper function to retrieve adios_rq_data from a request
+static inline struct adios_rq_data *get_rq_data(struct request *rq) {
+	return rq->elv.priv[0];
+}
+
+// Add a request to the deadline-sorted red-black tree
+static void add_to_dl_tree(
+		struct adios_data *ad, bool dl_idx, struct request *rq) {
+	struct rb_root_cached *root = &ad->dl_tree[dl_idx];
+	struct rb_node **link = &(root->rb_root.rb_node), *parent = NULL;
+	bool leftmost = true;
+	struct adios_rq_data *rd = get_rq_data(rq);
+	struct dl_group *dlg;
+
+	rd->block_size = blk_rq_bytes(rq);
+	u8 optype = adios_optype(rq);
+	rd->pred_lat =
+		latency_model_predict(&ad->latency_model[optype], rd->block_size);
+	rd->deadline =
+		rq->start_time_ns + ad->latency_target[optype] + rd->pred_lat;
+
+	while (*link) {
+		dlg = rb_entry(*link, struct dl_group, node);
+		s64 diff = rd->deadline - dlg->deadline;
+
+		parent = *link;
+		if (diff < 0) {
+			link = &((*link)->rb_left);
+		} else if (diff > 0) {
+			link = &((*link)->rb_right);
+			leftmost = false;
+		} else { // diff == 0
+			goto found;
+		}
+	}
+
+	dlg = rb_entry_safe(parent, struct dl_group, node);
+	if (!dlg || dlg->deadline != rd->deadline) {
+		dlg = kmem_cache_zalloc(ad->dl_group_pool, GFP_ATOMIC);
+		if (!dlg)
+			return;
+		dlg->deadline = rd->deadline;
+		INIT_LIST_HEAD(&dlg->rqs);
+		rb_link_node(&dlg->node, parent, link);
+		rb_insert_color_cached(&dlg->node, root, leftmost);
+	}
+found:
+	list_add_tail(&rd->dl_node, &dlg->rqs);
+	rd->dl_group = &dlg->rqs;
+	ad->dl_queued |= 1 << dl_idx;
+}
+
+// Remove a request from the deadline-sorted red-black tree
+static void del_from_dl_tree(
+		struct adios_data *ad, bool dl_idx, struct request *rq) {
+	struct rb_root_cached *root = &ad->dl_tree[dl_idx];
+	struct adios_rq_data *rd = get_rq_data(rq);
+	struct dl_group *dlg = container_of(rd->dl_group, struct dl_group, rqs);
+
+	list_del_init(&rd->dl_node);
+	if (list_empty(&dlg->rqs)) {
+		rb_erase_cached(&dlg->node, root);
+		kmem_cache_free(ad->dl_group_pool, dlg);
+	}
+	rd->dl_group = NULL;
+
+	if (RB_EMPTY_ROOT(&ad->dl_tree[dl_idx].rb_root))
+		ad->dl_queued &= ~(1 << dl_idx);
+}
+
+// Remove a request from the scheduler
+static void remove_request(struct adios_data *ad, struct request *rq) {
+	bool dl_idx = adios_optype_not_read(rq);
+	struct request_queue *q = rq->q;
+	struct adios_rq_data *rd = get_rq_data(rq);
+
+	list_del_init(&rq->queuelist);
+
+	// We might not be on the rbtree, if we are doing an insert merge
+	if (rd->dl_group)
+		del_from_dl_tree(ad, dl_idx, rq);
+
+	elv_rqhash_del(q, rq);
+	if (q->last_merge == rq)
+		q->last_merge = NULL;
+}
+
+// Convert a queue depth to the corresponding word depth for shallow allocation
+
+static inline int to_word_depth(struct blk_mq_hw_ctx *hctx, unsigned int qdepth) {
+	
+	if (!hctx || !hctx->sched_tags)
+		return 1;
+	
+	struct sbitmap_queue *bt = &hctx->sched_tags->bitmap_tags;
+	const unsigned int nrr = hctx->queue->nr_requests;
+	
+	if (unlikely(nrr == 0))
+		return 1;
+
+	return ((qdepth << bt->sb.shift) + nrr - 1) / nrr;
+}
+
+// Limit the depth of request allocation for asynchronous and write requests
+static void adios_limit_depth(blk_opf_t opf, struct blk_mq_alloc_data *data) {
+	struct adios_data *ad = data->q->elevator->elevator_data;
+	
+	if (!ad)
+		return;
+
+	// Do not throttle synchronous reads
+	if (op_is_sync(opf) && !op_is_write(opf))
+		return;
+
+	data->shallow_depth = to_word_depth(data->hctx, ad->async_depth);
+}
+
+// Update async_depth when the number of requests in the queue changes
+static void adios_depth_updated(struct blk_mq_hw_ctx *hctx) {
+	struct request_queue *q = hctx->queue;
+	struct adios_data *ad = q->elevator->elevator_data;
+	struct blk_mq_tags *tags = hctx->sched_tags;
+
+	ad->async_depth = q->nr_requests;
+	if (tags)
+ 	    sbitmap_queue_min_shallow_depth(&tags->bitmap_tags, 1);
+}
+
+// Handle request merging after a merge operation
+static void adios_request_merged(struct request_queue *q, struct request *req,
+				  enum elv_merge type) {
+	bool dl_idx = adios_optype_not_read(req);
+	struct adios_data *ad = q->elevator->elevator_data;
+
+	// Reposition request in the deadline-sorted tree
+	del_from_dl_tree(ad, dl_idx, req);
+	add_to_dl_tree(ad, dl_idx, req);
+}
+
+// Handle merging of requests after one has been merged into another
+static void adios_merged_requests(struct request_queue *q, struct request *req,
+				   struct request *next) {
+	struct adios_data *ad = q->elevator->elevator_data;
+
+	lockdep_assert_held(&ad->lock);
+
+	// kill knowledge of next, this one is a goner
+	remove_request(ad, next);
+}
+
+// Try to merge a bio into an existing rq before associating it with an rq
+static bool adios_bio_merge(struct request_queue *q, struct bio *bio,
+		unsigned int nr_segs) {
+	struct adios_data *ad = q->elevator->elevator_data;
+	struct request *free = NULL;
+	bool ret;
+
+	scoped_guard(spinlock_irqsave, &ad->lock)
+		ret = blk_mq_sched_try_merge(q, bio, nr_segs, &free);
+
+	if (free)
+		blk_mq_free_request(free);
+
+	return ret;
+}
+
+// Insert a request into the scheduler
+static void insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq,
+				  blk_insert_t insert_flags, struct list_head *free) {
+	bool dl_idx = adios_optype_not_read(rq);
+	struct request_queue *q = hctx->queue;
+	struct adios_data *ad = q->elevator->elevator_data;
+
+	if (insert_flags & BLK_MQ_INSERT_AT_HEAD) {
+		scoped_guard(spinlock_irqsave, &ad->pq_lock)
+			list_add_tail(&rq->queuelist, &ad->prio_queue);
+		return;
+	}
+
+	if (blk_mq_sched_try_insert_merge(q, rq, free))
+		return;
+
+	add_to_dl_tree(ad, dl_idx, rq);
+
+	if (rq_mergeable(rq)) {
+		elv_rqhash_add(q, rq);
+		if (!q->last_merge)
+			q->last_merge = rq;
+	}
+}
+
+// Insert multiple requests into the scheduler
+static void adios_insert_requests(struct blk_mq_hw_ctx *hctx,
+				   struct list_head *list,
+				   blk_insert_t insert_flags) {
+	struct request_queue *q = hctx->queue;
+	struct adios_data *ad = q->elevator->elevator_data;
+	struct request *rq;
+	bool stop = false;
+	LIST_HEAD(free);
+
+	do {
+	scoped_guard(spinlock_irqsave, &ad->lock)
+		for (int i = 0; i < ADIOS_MAX_INSERTS_PER_LOCK; i++) {
+			if (list_empty(list)) {
+				stop = true;
+				break;
+			}
+			rq = list_first_entry(list, struct request, queuelist);
+			list_del_init(&rq->queuelist);
+			insert_request(hctx, rq, insert_flags, &free);
+		}
+	} while (!stop);
+
+	blk_mq_free_requests(&free);
+}
+
+// Prepare a request before it is inserted into the scheduler
+static void adios_prepare_request(struct request *rq) {
+	struct adios_data *ad = rq->q->elevator->elevator_data;
+	struct adios_rq_data *rd;
+
+	rq->elv.priv[0] = NULL;
+
+	/* Allocate adios_rq_data from the memory pool */
+	rd = kmem_cache_zalloc(ad->rq_data_pool, GFP_ATOMIC);
+	if (WARN(!rd, "adios_prepare_request: "
+			"Failed to allocate memory from rq_data_pool. rd is NULL\n"))
+		return;
+
+	rd->rq = rq;
+	rq->elv.priv[0] = rd;
+}
+
+static struct adios_rq_data *get_dl_first_rd(struct adios_data *ad, bool idx) {
+	struct rb_root_cached *root = &ad->dl_tree[idx];
+	struct rb_node *first = rb_first_cached(root);
+	if (!first)
+		return NULL;
+	
+	struct dl_group *dl_group = rb_entry(first, struct dl_group, node);
+
+	return list_first_entry(&dl_group->rqs, struct adios_rq_data, dl_node);
+}
+
+// Fill the batch queues with requests from the deadline-sorted red-black tree
+static bool fill_batch_queues(struct adios_data *ad, u64 current_lat) {
+	struct adios_rq_data *rd;
+	struct request *rq;
+	u32 optype_count[ADIOS_OPTYPES] = {0};
+	u32 count = 0;
+	u8 optype;
+	bool page = !ad->bq_page, dl_idx, bias_idx, reduce_bias;
+
+	// Reset batch queue counts for the back page
+	memset(&ad->batch_count[page], 0, sizeof(ad->batch_count[page]));
+
+	scoped_guard(spinlock_irqsave, &ad->lock)
+		while (true) {
+
+			// Check if there are any requests queued in the deadline tree
+			if (!ad->dl_queued)
+				break;
+
+			dl_idx = ad->dl_queued >> 1;
+			// Get the first request from the deadline-sorted tree
+			rd = get_dl_first_rd(ad, dl_idx);
+			bias_idx = ad->dl_bias < 0;
+
+			// If read and write requests are queued, choose one based on bias
+			if (ad->dl_queued == 0x3) {
+				struct adios_rq_data *trd[2] = {get_dl_first_rd(ad, 0), rd};
+				rd = trd[bias_idx];
+
+				reduce_bias = (trd[bias_idx]->deadline > trd[!bias_idx]->deadline);
+			} else
+				reduce_bias = (bias_idx == dl_idx);
+			
+			rq = rd->rq;
+			optype = adios_optype(rq);
+
+			// Check batch size and total predicted latency
+			if (count && (!ad->latency_model[optype].base ||
+					ad->batch_count[page][optype] >= ad->batch_limit[optype] ||
+					(current_lat + rd->pred_lat) > ad->global_latency_window))
+				break;
+
+			if (reduce_bias) {
+				s64 sign = ((int)bias_idx << 1) - 1;
+				if (unlikely(!rd->pred_lat))
+					ad->dl_bias = sign;
+				else
+					// Adjust the bias based on the predicted latency
+					ad->dl_bias += sign * (s64)((rd->pred_lat *
+						adios_prio_to_weight[ad->dl_prio[bias_idx] + 20]) >> 10);
+			}
+
+			remove_request(ad, rq);
+
+			// Add request to the corresponding batch queue
+			list_add_tail(&rq->queuelist, &ad->batch_queue[page][optype]);
+			atomic64_add(rd->pred_lat, &ad->total_pred_lat);
+			current_lat += rd->pred_lat;
+			ad->batch_count[page][optype]++;
+			optype_count[optype]++;
+			count++;
+		}
+
+	if (count) {
+		ad->more_bq_ready = true;
+		for (u8 optype = 0; optype < ADIOS_OPTYPES; optype++)
+			if (ad->batch_actual_max_size[optype] < optype_count[optype])
+				ad->batch_actual_max_size[optype] = optype_count[optype];
+		if (ad->batch_actual_max_total < count)
+			ad->batch_actual_max_total = count;
+	}
+	return count;
+}
+
+// Flip to the next batch queue page
+static void flip_bq_page(struct adios_data *ad) {
+	ad->more_bq_ready = false;
+	ad->bq_page = !ad->bq_page;
+}
+
+// Dispatch a request from the batch queues
+static struct request *dispatch_from_bq(struct adios_data *ad) {
+	struct request *rq = NULL;
+	u64 tpl;
+
+	guard(spinlock_irqsave)(&ad->bq_lock);
+
+	tpl = atomic64_read(&ad->total_pred_lat);
+
+	if (!ad->more_bq_ready && (!tpl || tpl < div_u64(
+			ad->global_latency_window * ad->bq_refill_below_ratio, 100)))
+		fill_batch_queues(ad, tpl);
+
+again:
+	// Check if there are any requests in the batch queues
+	for (u8 i = 0; i < ADIOS_OPTYPES; i++) {
+		if (!list_empty(&ad->batch_queue[ad->bq_page][i])) {
+			rq = list_first_entry(
+				&ad->batch_queue[ad->bq_page][i], struct request, queuelist);
+			list_del_init(&rq->queuelist);
+			return rq;
+		}
+	}
+
+	// If there's more batch queue page available, flip to it and retry
+	if (ad->more_bq_ready) {
+		flip_bq_page(ad);
+		goto again;
+	}
+
+	return NULL;
+}
+
+// Dispatch a request from the priority queue
+static struct request *dispatch_from_pq(struct adios_data *ad) {
+	struct request *rq = NULL;
+
+	guard(spinlock_irqsave)(&ad->pq_lock);
+
+	if (!list_empty(&ad->prio_queue)) {
+		rq = list_first_entry(&ad->prio_queue, struct request, queuelist);
+		list_del_init(&rq->queuelist);
+	}
+	return rq;
+}
+
+// Dispatch a request to the hardware queue
+static struct request *adios_dispatch_request(struct blk_mq_hw_ctx *hctx) {
+	struct adios_data *ad = hctx->queue->elevator->elevator_data;
+	struct request *rq;
+
+	rq = dispatch_from_pq(ad);
+	if (rq) goto found;
+	rq = dispatch_from_bq(ad);
+	if (!rq) return NULL;
+found:
+	rq->rq_flags |= RQF_STARTED;
+	return rq;
+}
+
+// Timer callback function to periodically update latency models
+static void update_timer_callback(struct timer_list *t) {
+	struct adios_data *ad = from_timer(ad, t, update_timer);
+
+	for (u8 optype = 0; optype < ADIOS_OPTYPES; optype++)
+		latency_model_update(ad, &ad->latency_model[optype]);
+}
+
+// Handle the completion of a request
+static void adios_completed_request(struct request *rq, u64 now) {
+	struct adios_data *ad = rq->q->elevator->elevator_data;
+	struct adios_rq_data *rd = get_rq_data(rq);
+
+	atomic64_sub(rd->pred_lat, &ad->total_pred_lat);
+
+	if (!rq->io_start_time_ns || !rd->block_size)
+		return;
+	u64 latency = now - rq->io_start_time_ns;
+	u8 optype = adios_optype(rq);
+	latency_model_input(ad, &ad->latency_model[optype],
+		rd->block_size, latency, rd->pred_lat);
+	timer_reduce(&ad->update_timer, jiffies + msecs_to_jiffies(100));
+}
+
+// Clean up after a request is finished
+static void adios_finish_request(struct request *rq) {
+	struct adios_data *ad = rq->q->elevator->elevator_data;
+
+	if (rq->elv.priv[0]) {
+		// Free adios_rq_data back to the memory pool
+		kmem_cache_free(ad->rq_data_pool, get_rq_data(rq));
+		rq->elv.priv[0] = NULL;
+	}
+}
+
+static inline bool pq_has_work(struct adios_data *ad) {
+	guard(spinlock_irqsave)(&ad->pq_lock);
+	return !list_empty(&ad->prio_queue);
+}
+
+static inline bool bq_has_work(struct adios_data *ad) {
+	guard(spinlock_irqsave)(&ad->bq_lock);
+
+	for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+		if (!list_empty(&ad->batch_queue[ad->bq_page][i]))
+			return true;
+
+	return ad->more_bq_ready;
+}
+
+static inline bool dl_tree_has_work(struct adios_data *ad) {
+	guard(spinlock_irqsave)(&ad->lock);
+	return ad->dl_queued;
+}
+
+// Check if there are any requests available for dispatch
+static bool adios_has_work(struct blk_mq_hw_ctx *hctx) {
+	struct adios_data *ad = hctx->queue->elevator->elevator_data;
+
+	return pq_has_work(ad) || bq_has_work(ad) || dl_tree_has_work(ad);
+}
+
+// Initialize the scheduler-specific data for a hardware queue
+static int adios_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx) {
+	adios_depth_updated(hctx);
+	return 0;
+}
+
+// Initialize the scheduler-specific data when initializing the request queue
+static int adios_init_sched(struct request_queue *q, struct elevator_type *e) {
+	struct adios_data *ad;
+	struct elevator_queue *eq;
+	int ret = -ENOMEM;
+	u8 optype = 0;
+
+	eq = elevator_alloc(q, e);
+	if (!eq)
+		return ret;
+
+	ad = kzalloc_node(sizeof(*ad), GFP_KERNEL, q->node);
+	if (!ad)
+		goto put_eq;
+
+	// Create a memory pool for adios_rq_data
+	ad->rq_data_pool = kmem_cache_create("rq_data_pool",
+						sizeof(struct adios_rq_data),
+						0, SLAB_HWCACHE_ALIGN, NULL);
+	if (!ad->rq_data_pool) {
+		pr_err("adios: Failed to create rq_data_pool\n");
+		goto free_ad;
+	}
+
+	/* Create a memory pool for dl_group */
+	ad->dl_group_pool = kmem_cache_create("dl_group_pool",
+						sizeof(struct dl_group),
+						0, SLAB_HWCACHE_ALIGN, NULL);
+	if (!ad->dl_group_pool) {
+		pr_err("adios: Failed to create dl_group_pool\n");
+		goto destroy_rq_data_pool;
+	}
+
+	eq->elevator_data = ad;
+
+	ad->global_latency_window = default_global_latency_window;
+	ad->bq_refill_below_ratio = default_bq_refill_below_ratio;
+
+	INIT_LIST_HEAD(&ad->prio_queue);
+	for (u8 i = 0; i < 2; i++)
+		ad->dl_tree[i] = RB_ROOT_CACHED;
+	ad->dl_bias = 0;
+	ad->dl_queued = 0x0;
+	for (u8 i = 0; i < 2; i++)
+		ad->dl_prio[i] = default_dl_prio[i];
+
+	ad->aggr_buckets = kzalloc(sizeof(*ad->aggr_buckets), GFP_KERNEL);
+	if (!ad->aggr_buckets) {
+		pr_err("adios: Failed to allocate aggregation buckets\n");
+		goto destroy_dl_group_pool;
+	}
+
+	for (optype = 0; optype < ADIOS_OPTYPES; optype++) {
+		struct latency_model *model = &ad->latency_model[optype];
+		seqlock_init(&model->lock);
+
+		model->pcpu_buckets = alloc_percpu(struct lm_buckets);
+		if (!model->pcpu_buckets)
+			goto free_buckets;
+
+		model->last_update_jiffies = jiffies;
+		model->lm_shrink_at_kreqs  = default_lm_shrink_at_kreqs;
+		model->lm_shrink_at_gbytes = default_lm_shrink_at_gbytes;
+		model->lm_shrink_resist    = default_lm_shrink_resist;
+
+		ad->latency_target[optype] = default_latency_target[optype];
+		ad->batch_limit[optype] = default_batch_limit[optype];
+	}
+	timer_setup(&ad->update_timer, update_timer_callback, 0);
+	
+	for (u8 page = 0; page < ADIOS_BQ_PAGES; page++)
+		for (u8 optype = 0; optype < ADIOS_OPTYPES; optype++)
+			INIT_LIST_HEAD(&ad->batch_queue[page][optype]);
+
+	spin_lock_init(&ad->lock);
+	spin_lock_init(&ad->pq_lock);
+	spin_lock_init(&ad->bq_lock);
+
+	/* We dispatch from request queue wide instead of hw queue */
+	blk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);
+
+	ad->queue = q;
+	blk_stat_enable_accounting(q);
+
+	q->elevator = eq;
+	return 0;
+
+free_buckets:
+	pr_err("adios: Failed to allocate per-cpu buckets\n");
+	while (--optype >= 0) {
+		struct latency_model *prev_model = &ad->latency_model[optype];
+		free_percpu(prev_model->pcpu_buckets);
+	}
+	kfree(ad->aggr_buckets);
+destroy_dl_group_pool:
+	kmem_cache_destroy(ad->dl_group_pool);
+destroy_rq_data_pool:
+	kmem_cache_destroy(ad->rq_data_pool);
+free_ad:
+	kfree(ad);
+put_eq:
+	kobject_put(&eq->kobj);
+	return ret;
+}
+
+// Clean up and free resources when exiting the scheduler
+static void adios_exit_sched(struct elevator_queue *e) {
+	struct adios_data *ad = e->elevator_data;
+
+	timer_shutdown_sync(&ad->update_timer);
+
+	WARN_ON_ONCE(!list_empty(&ad->prio_queue));
+
+	for (u8 i = 0; i < ADIOS_OPTYPES; i++) {
+		struct latency_model *model = &ad->latency_model[i];
+		free_percpu(model->pcpu_buckets);
+	}
+	kfree(ad->aggr_buckets);
+
+	if (ad->rq_data_pool)
+		kmem_cache_destroy(ad->rq_data_pool);
+
+	if (ad->dl_group_pool)
+		kmem_cache_destroy(ad->dl_group_pool);
+
+	blk_stat_disable_accounting(ad->queue);
+
+	kfree(ad);
+}
+
+static void load_latency_model(struct latency_model *model, u64 base, u64 slope)
+{
+	write_seqlock_bh(&model->lock);
+	model->last_update_jiffies = jiffies;
+
+	// Initialize base and its statistics as a single sample.
+	model->base = base;
+	model->small_sum_delay = base;
+	model->small_count = 1;
+
+	// Initialize slope and its statistics as a single sample.
+	model->slope = slope;
+	model->large_sum_delay = slope;
+	model->large_sum_bsize = 1024; /* Corresponds to 1 KiB */
+
+	lm_reset_pcpu_buckets(model);
+
+	write_sequnlock_bh(&model->lock);
+}
+
+// Define sysfs attributes for operation types
+#define SYSFS_OPTYPE_DECL(name, optype) \
+static ssize_t adios_lat_model_##name##_show( \
+		struct elevator_queue *e, char *page) { \
+	struct adios_data *ad = e->elevator_data; \
+	struct latency_model *model = &ad->latency_model[optype]; \
+	ssize_t len = 0; \
+	u64 base, slope; \
+	unsigned int seq; \
+	do { \
+		seq = read_seqbegin(&model->lock); \
+		base = model->base; \
+		slope = model->slope; \
+	} while (read_seqretry(&model->lock, seq)); \
+	len += sprintf(page,       "base : %llu ns\n", base); \
+	len += sprintf(page + len, "slope: %llu ns/KiB\n", slope); \
+	return len; \
+} \
+static ssize_t adios_lat_model_##name##_store( \
+		struct elevator_queue *e, const char *page, size_t count) { \
+	struct adios_data *ad = e->elevator_data; \
+	struct latency_model *model = &ad->latency_model[optype]; \
+	u64 base, slope; \
+	int ret; \
+	ret = sscanf(page, "%llu %llu", &base, &slope); \
+	if (ret != 2) \
+		return -EINVAL; \
+	load_latency_model(model, base, slope); \
+	reset_buckets(ad->aggr_buckets); \
+	return count; \
+} \
+static ssize_t adios_lat_target_##name##_store( \
+		struct elevator_queue *e, const char *page, size_t count) { \
+	struct adios_data *ad = e->elevator_data; \
+	unsigned long nsec; \
+	int ret; \
+	ret = kstrtoul(page, 10, &nsec); \
+	if (ret) \
+		return ret; \
+	ad->latency_model[optype].base = 0ULL; \
+	ad->latency_target[optype] = nsec; \
+	return count; \
+} \
+static ssize_t adios_lat_target_##name##_show( \
+		struct elevator_queue *e, char *page) { \
+	struct adios_data *ad = e->elevator_data; \
+	return sprintf(page, "%llu\n", ad->latency_target[optype]); \
+} \
+static ssize_t adios_batch_limit_##name##_store( \
+		struct elevator_queue *e, const char *page, size_t count) { \
+	unsigned long max_batch; \
+	int ret; \
+	ret = kstrtoul(page, 10, &max_batch); \
+	if (ret || max_batch == 0) \
+		return -EINVAL; \
+	struct adios_data *ad = e->elevator_data; \
+	ad->batch_limit[optype] = max_batch; \
+	return count; \
+} \
+static ssize_t adios_batch_limit_##name##_show( \
+		struct elevator_queue *e, char *page) { \
+	struct adios_data *ad = e->elevator_data; \
+	return sprintf(page, "%u\n", ad->batch_limit[optype]); \
+}
+
+SYSFS_OPTYPE_DECL(read, ADIOS_READ);
+SYSFS_OPTYPE_DECL(write, ADIOS_WRITE);
+SYSFS_OPTYPE_DECL(discard, ADIOS_DISCARD);
+
+// Show the maximum batch size actually achieved for each operation type
+static ssize_t adios_batch_actual_max_show(
+		struct elevator_queue *e, char *page) {
+	struct adios_data *ad = e->elevator_data;
+	u32 total_count, read_count, write_count, discard_count;
+
+	total_count = ad->batch_actual_max_total;
+	read_count = ad->batch_actual_max_size[ADIOS_READ];
+	write_count = ad->batch_actual_max_size[ADIOS_WRITE];
+	discard_count = ad->batch_actual_max_size[ADIOS_DISCARD];
+
+	return sprintf(page,
+		"Total  : %u\nDiscard: %u\nRead   : %u\nWrite  : %u\n",
+		total_count, discard_count, read_count, write_count);
+}
+
+// Set the global latency window
+static ssize_t adios_global_latency_window_store(
+		struct elevator_queue *e, const char *page, size_t count) {
+	struct adios_data *ad = e->elevator_data;
+	unsigned long nsec;
+	int ret;
+
+	ret = kstrtoul(page, 10, &nsec);
+	if (ret)
+		return ret;
+
+	ad->global_latency_window = nsec;
+
+	return count;
+}
+
+// Show the global latency window
+static ssize_t adios_global_latency_window_show(
+		struct elevator_queue *e, char *page) {
+	struct adios_data *ad = e->elevator_data;
+	return sprintf(page, "%llu\n", ad->global_latency_window);
+}
+
+// Show the bq_refill_below_ratio
+static ssize_t adios_bq_refill_below_ratio_show(
+		struct elevator_queue *e, char *page) {
+	struct adios_data *ad = e->elevator_data;
+	return sprintf(page, "%d\n", ad->bq_refill_below_ratio);
+}
+
+// Set the bq_refill_below_ratio
+static ssize_t adios_bq_refill_below_ratio_store(
+		struct elevator_queue *e, const char *page, size_t count) {
+	struct adios_data *ad = e->elevator_data;
+	int ratio;
+	int ret;
+
+	ret = kstrtoint(page, 10, &ratio);
+	if (ret || ratio < 0 || ratio > 100)
+		return -EINVAL;
+
+	ad->bq_refill_below_ratio = ratio;
+
+	return count;
+}
+
+// Show the read priority
+static ssize_t adios_read_priority_show(
+		struct elevator_queue *e, char *page) {
+	struct adios_data *ad = e->elevator_data;
+	return sprintf(page, "%d\n", ad->dl_prio[0]);
+}
+
+// Set the read priority
+static ssize_t adios_read_priority_store(
+		struct elevator_queue *e, const char *page, size_t count) {
+	struct adios_data *ad = e->elevator_data;
+	int prio;
+	int ret;
+
+	ret = kstrtoint(page, 10, &prio);
+	if (ret || prio < -20 || prio > 19)
+		return -EINVAL;
+
+	guard(spinlock_irqsave)(&ad->lock);
+	ad->dl_prio[0] = prio;
+	ad->dl_bias = 0;
+
+	return count;
+}
+
+// Reset batch queue statistics
+static ssize_t adios_reset_bq_stats_store(
+		struct elevator_queue *e, const char *page, size_t count) {
+	struct adios_data *ad = e->elevator_data;
+	unsigned long val;
+	int ret;
+
+	ret = kstrtoul(page, 10, &val);
+	if (ret || val != 1)
+		return -EINVAL;
+
+	for (u8 i = 0; i < ADIOS_OPTYPES; i++)
+		ad->batch_actual_max_size[i] = 0;
+
+	ad->batch_actual_max_total = 0;
+
+	return count;
+}
+
+// Reset the latency model parameters or load them from user input
+static ssize_t adios_reset_lat_model_store(
+		struct elevator_queue *e, const char *page, size_t count)
+{
+	struct adios_data *ad = e->elevator_data;
+	struct latency_model *model;
+	int ret;
+
+	/*
+	 * Differentiate between two modes based on input format:
+	 * 1. "1": Fully reset the model (backward compatibility).
+	 * 2. "R_base R_slope W_base W_slope D_base D_slope": Load values.
+	 */
+	if (!strchr(page, ' ')) {
+		// Mode 1: Full reset.
+		unsigned long val;
+
+		ret = kstrtoul(page, 10, &val);
+		if (ret || val != 1)
+			return -EINVAL;
+
+		for (u8 i = 0; i < ADIOS_OPTYPES; i++) {
+			model = &ad->latency_model[i];
+			write_seqlock_bh(&model->lock);
+			model->last_update_jiffies = jiffies;
+			model->base = 0ULL;
+			model->slope = 0ULL;
+			model->small_sum_delay = 0ULL;
+			model->small_count = 0ULL;
+			model->large_sum_delay = 0ULL;
+			model->large_sum_bsize = 0ULL;
+			lm_reset_pcpu_buckets(model);
+			write_sequnlock_bh(&model->lock);
+		}
+	} else {
+		// Mode 2: Load initial values for all latency models.
+		u64 params[3][2]; /* 0:base, 1:slope for R, W, D */
+
+		ret = sscanf(page, "%llu %llu %llu %llu %llu %llu",
+			&params[ADIOS_READ   ][0], &params[ADIOS_READ   ][1],
+			&params[ADIOS_WRITE  ][0], &params[ADIOS_WRITE  ][1],
+			&params[ADIOS_DISCARD][0], &params[ADIOS_DISCARD][1]);
+
+		if (ret != 6)
+			return -EINVAL;
+
+		for (u8 i = ADIOS_READ; i <= ADIOS_DISCARD; i++) {
+			model = &ad->latency_model[i];
+			load_latency_model(model, params[i][0], params[i][1]);
+		}
+	}
+	reset_buckets(ad->aggr_buckets);
+
+	return count;
+}
+
+// Show the ADIOS version
+static ssize_t adios_version_show(struct elevator_queue *e, char *page) {
+	return sprintf(page, "%s\n", ADIOS_VERSION);
+}
+
+// Define sysfs attributes for dynamic thresholds
+#define SHRINK_THRESHOLD_ATTR_RW(name, model_field, min_value, max_value) \
+static ssize_t adios_shrink_##name##_store( \
+		struct elevator_queue *e, const char *page, size_t count) { \
+	struct adios_data *ad = e->elevator_data; \
+	unsigned long val; \
+	int ret; \
+	ret = kstrtoul(page, 10, &val); \
+	if (ret || val < min_value || val > max_value) \
+		return -EINVAL; \
+	for (u8 i = 0; i < ADIOS_OPTYPES; i++) { \
+		struct latency_model *model = &ad->latency_model[i]; \
+		write_seqlock_bh(&model->lock); \
+		model->model_field = val; \
+		write_sequnlock_bh(&model->lock); \
+	} \
+	return count; \
+} \
+static ssize_t adios_shrink_##name##_show( \
+		struct elevator_queue *e, char *page) { \
+	struct adios_data *ad = e->elevator_data; \
+	u32 val = 0; \
+	unsigned int seq;						\
+	struct latency_model *model = &ad->latency_model[0]; \
+	do { \
+		seq = read_seqbegin(&model->lock); \
+		val = model->model_field; \
+	} while (read_seqretry(&model->lock, seq)); \
+	return sprintf(page, "%u\n", val); \
+}
+
+SHRINK_THRESHOLD_ATTR_RW(at_kreqs,  lm_shrink_at_kreqs,  1, 100000)
+SHRINK_THRESHOLD_ATTR_RW(at_gbytes, lm_shrink_at_gbytes, 1,   1000)
+SHRINK_THRESHOLD_ATTR_RW(resist,    lm_shrink_resist,    1,      3)
+
+// Define sysfs attributes
+#define AD_ATTR(name, show_func, store_func) \
+	__ATTR(name, 0644, show_func, store_func)
+#define AD_ATTR_RW(name) \
+	__ATTR(name, 0644, adios_##name##_show, adios_##name##_store)
+#define AD_ATTR_RO(name) \
+	__ATTR(name, 0444, adios_##name##_show, NULL)
+#define AD_ATTR_WO(name) \
+	__ATTR(name, 0200, NULL, adios_##name##_store)
+
+// Define sysfs attributes for ADIOS scheduler
+static struct elv_fs_entry adios_sched_attrs[] = {
+	AD_ATTR_RO(batch_actual_max),
+	AD_ATTR_RW(bq_refill_below_ratio),
+	AD_ATTR_RW(global_latency_window),
+
+	AD_ATTR_RW(batch_limit_read),
+	AD_ATTR_RW(batch_limit_write),
+	AD_ATTR_RW(batch_limit_discard),
+
+	AD_ATTR_RW(lat_model_read),
+	AD_ATTR_RW(lat_model_write),
+	AD_ATTR_RW(lat_model_discard),
+
+	AD_ATTR_RW(lat_target_read),
+	AD_ATTR_RW(lat_target_write),
+	AD_ATTR_RW(lat_target_discard),
+
+	AD_ATTR_RW(shrink_at_kreqs),
+	AD_ATTR_RW(shrink_at_gbytes),
+	AD_ATTR_RW(shrink_resist),
+
+	AD_ATTR_RW(read_priority),
+
+	AD_ATTR_WO(reset_bq_stats),
+	AD_ATTR_WO(reset_lat_model),
+	AD_ATTR(adios_version, adios_version_show, NULL),
+
+	__ATTR_NULL
+};
+
+// Define the ADIOS scheduler type
+static struct elevator_type mq_adios = {
+	.ops = {
+		.next_request		= elv_rb_latter_request,
+		.former_request		= elv_rb_former_request,
+		.limit_depth		= adios_limit_depth,
+		.depth_updated		= adios_depth_updated,
+		.request_merged		= adios_request_merged,
+		.requests_merged	= adios_merged_requests,
+		.bio_merge			= adios_bio_merge,
+		.request_merge      = adios_request_merge,
+		.insert_requests	= adios_insert_requests,
+		.prepare_request	= adios_prepare_request,
+		.dispatch_request	= adios_dispatch_request,
+		.completed_request	= adios_completed_request,
+		.finish_request		= adios_finish_request,
+		.has_work			= adios_has_work,
+		.init_hctx			= adios_init_hctx,
+		.init_sched			= adios_init_sched,
+		.exit_sched			= adios_exit_sched,
+		.init_icq           = adios_init_icq,
+		.exit_icq           = adios_exit_icq,
+	},
+	.icq_size = sizeof(struct io_cq),
+    .icq_align = __alignof__(struct io_cq),
+	.elevator_attrs = adios_sched_attrs,
+	.elevator_name = "adios",
+	.elevator_owner = THIS_MODULE,
+};
+MODULE_ALIAS("mq-adios-iosched");
+
+#define ADIOS_PROGNAME "Adaptive Deadline I/O Scheduler"
+#define ADIOS_AUTHOR   "Masahito Suzuki"
+
+// Initialize the ADIOS scheduler module
+static int __init adios_init(void) {
+	printk(KERN_INFO "%s %s by %s\n",
+		ADIOS_PROGNAME, ADIOS_VERSION, ADIOS_AUTHOR);
+	return elv_register(&mq_adios);
+}
+
+// Exit the ADIOS scheduler module
+static void __exit adios_exit(void) {
+	elv_unregister(&mq_adios);
+}
+
+module_init(adios_init);
+module_exit(adios_exit);
+
+MODULE_AUTHOR(ADIOS_AUTHOR);
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION(ADIOS_PROGNAME);
\ No newline at end of file
diff --git a/block/elevator.c b/block/elevator.c
index 5ff093cb3..3c0cb7669 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -569,6 +569,10 @@ static inline bool elv_support_iosched(struct request_queue *q)
  */
 static struct elevator_type *elevator_get_default(struct request_queue *q)
 {
+#ifdef CONFIG_MQ_IOSCHED_DEFAULT_ADIOS
+    return elevator_find_get(q, "adios");
+#endif
+	
 	if (q->tag_set && q->tag_set->flags & BLK_MQ_F_NO_SCHED_BY_DEFAULT)
 		return NULL;
 
diff --git a/drivers/Kconfig b/drivers/Kconfig
index efb66e25f..2c11c1d07 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -243,4 +243,6 @@ source "drivers/hte/Kconfig"
 
 source "drivers/cdx/Kconfig"
 
+source "drivers/auth_ctl/Kconfig"
+
 endmenu
diff --git a/drivers/Makefile b/drivers/Makefile
index 1bec7819a..a38383ecf 100644
--- a/drivers/Makefile
+++ b/drivers/Makefile
@@ -199,3 +199,5 @@ obj-$(CONFIG_DRM_ACCEL)		+= accel/
 obj-$(CONFIG_CDX_BUS)		+= cdx/
 
 obj-$(CONFIG_S390)		+= s390/
+
+obj-$(CONFIG_AUTHORITY_CTRL)		+= auth_ctl/
diff --git a/drivers/auth_ctl/Kconfig b/drivers/auth_ctl/Kconfig
new file mode 100644
index 000000000..9762fe634
--- /dev/null
+++ b/drivers/auth_ctl/Kconfig
@@ -0,0 +1,55 @@
+# SPDX-License-Identifier: GPL-2.0
+config AUTHORITY_CTRL
+	tristate "Authority Control for RTG & QOS"
+	default y
+	help
+	  Control thread's authority for specific kenrel feature such as RTG
+	  or QOS. Use uid as the authentication granularity. Status switching
+	  will change uid's authority, and would trigger additional actions
+	  registered by specific kernel feature.
+
+config QOS_CTRL
+	bool "Multiple Level Qos Control for thread"
+	default y
+	depends on AUTHORITY_CTRL
+	help
+	  If set, thread can apply qos for less execution latency and get more
+	  cpu supply. Permission and absolute supply aggressiveness was controlled
+	  by AUTHORITY_CTRL.
+
+config RTG_AUTHORITY
+	bool "Authority Control for SCHED_RTG_FRAME"
+	default n
+	depends on AUTHORITY_CTRL
+	depends on SCHED_RTG_FRAME
+	help
+	  Authority control for SCHED_RTG_FRAME. If set, access to SCHED_RTG_FRAME's
+	  ioctl cmd will be restricted.
+
+config QOS_AUTHORITY
+	bool "Authority Control for QOS_CTRL"
+	default y
+	depends on AUTHORITY_CTRL
+	depends on QOS_CTRL
+	help
+	  Authority control for QOS_CTRL. If set, access to QOS_CTRL's ioctl cmd will
+	  be restricted.
+
+config AUTH_QOS_DEBUG
+	bool "Debug fs for qos_ctrl and auth_ctrl"
+	default n
+	depends on AUTHORITY_CTRL
+	depends on RTG_AUTHORITY
+	depends on QOS_AUTHORITY
+	help
+	  If set, debug node will show auth and qos info
+
+config QOS_POLICY_MAX_NR
+	int "Number of supported qos policy"
+	range 5 20
+	default 5
+	depends on QOS_CTRL
+	help
+	  Qos policy number limit. Truly initialized qos policy could small then
+	  this value.
+
diff --git a/drivers/auth_ctl/Makefile b/drivers/auth_ctl/Makefile
new file mode 100644
index 000000000..548ec4535
--- /dev/null
+++ b/drivers/auth_ctl/Makefile
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0-only
+obj-$(CONFIG_AUTHORITY_CTRL)		+= auth_qos_ctrl.o
+auth_qos_ctrl-$(CONFIG_AUTHORITY_CTRL)	+= auth_ctrl.o
+auth_qos_ctrl-$(CONFIG_QOS_CTRL)	+= qos_ctrl.o
+auth_qos_ctrl-$(CONFIG_AUTH_QOS_DEBUG)	+= auth_qos_debug.o
diff --git a/drivers/auth_ctl/auth_ctrl.c b/drivers/auth_ctl/auth_ctrl.c
new file mode 100644
index 000000000..04728c04a
--- /dev/null
+++ b/drivers/auth_ctl/auth_ctrl.c
@@ -0,0 +1,661 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/auth_ctl/auth_ctrl.c
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ *
+ */
+
+#include <linux/cred.h>
+#include <linux/mutex.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/sched/auth_ctrl.h>
+#include <linux/sched/rtg_auth.h>
+#include <linux/sched/qos_ctrl.h>
+#include <linux/sched/qos_auth.h>
+
+#include "auth_ctrl.h"
+#ifdef CONFIG_QOS_CTRL
+#include "qos_ctrl.h"
+#endif
+
+typedef long (*auth_ctrl_func)(int abi, void __user *arg);
+
+static long ctrl_auth_basic_operation(int abi, void __user *uarg);
+
+static auth_ctrl_func g_func_array[AUTH_CTRL_MAX_NR] = {
+	NULL, /* reserved */
+	ctrl_auth_basic_operation,
+};
+
+/*
+ * uid-based authority idr table
+ */
+static struct idr *ua_idr;
+
+struct idr *get_auth_ctrl_idr(void)
+{
+	return ua_idr;
+}
+
+static DEFINE_MUTEX(ua_idr_mutex);
+
+struct mutex *get_auth_idr_mutex(void)
+{
+	return &ua_idr_mutex;
+}
+
+/*
+ * change auth's status to SYSTEM and enable all feature access
+ */
+static void change_to_super(struct auth_struct *auth)
+{
+#ifdef CONFIG_RTG_AUTHORITY
+	auth->rtg_auth_flag = AF_RTG_ALL;
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	auth->qos_auth_flag = AF_QOS_ALL;
+#endif
+	auth->status = AUTH_STATUS_SYSTEM_SERVER;
+}
+
+static void init_authority_record(struct auth_struct *auth)
+{
+#ifdef CONFIG_QOS_AUTHORITY
+	int i;
+#endif
+
+#ifdef CONFIG_RTG_AUTHORITY
+	auth->rtg_auth_flag = 0;
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	auth->qos_auth_flag = 0;
+#endif
+	auth->status = AUTH_STATUS_DISABLED;
+	mutex_init(&auth->mutex);
+	refcount_set(&auth->usage, 1);
+#ifdef CONFIG_QOS_CTRL
+	for (i = QOS_POLICY_MIN_LEVEL; i < NR_QOS; ++i) {
+		INIT_LIST_HEAD(&auth->tasks[i]);
+		auth->num[i] = 0;
+	}
+#endif
+}
+
+void get_auth_struct(struct auth_struct *auth)
+{
+	refcount_inc(&auth->usage);
+}
+
+static void __put_auth_struct(struct auth_struct *auth)
+
+{
+	WARN_ON(auth->status != AUTH_STATUS_DEAD);
+	WARN_ON(refcount_read(&auth->usage));
+
+#ifdef CONFIG_QOS_CTRL
+	/* refcount is zero here, no contend, no lock. */
+	remove_qos_tasks(auth);
+#endif
+	kfree(auth);
+}
+
+void put_auth_struct(struct auth_struct *auth)
+{
+	if (refcount_dec_and_test(&auth->usage))
+		__put_auth_struct(auth);
+}
+
+static int init_ua_idr(void)
+{
+	ua_idr = kzalloc(sizeof(*ua_idr), GFP_ATOMIC);
+	if (ua_idr == NULL) {
+		pr_err("[AUTH_CTRL] auth idr init failed, no memory!\n");
+		return -ENOMEM;
+	}
+
+	idr_init(ua_idr);
+	
+	return 0;
+}
+
+static int init_super_authority(unsigned int auth_tgid)
+{
+	int ret;
+	struct auth_struct *auth_super;
+
+	auth_super = kzalloc(sizeof(*auth_super), GFP_ATOMIC);
+	if(auth_super == NULL) {
+		pr_err("[AUTH_CTRL] auth struct alloc failed\n");
+		return -ENOMEM;
+	}
+	init_authority_record(auth_super);
+	change_to_super(auth_super);
+
+	ret = idr_alloc(ua_idr, auth_super, auth_tgid, auth_tgid + 1, GFP_ATOMIC);
+	if(ret != auth_tgid) {
+		pr_err("[AUTH_CTRL] authority for super init failed! ret=%d\n", ret);
+		kfree(auth_super);
+		return ret;
+	}
+
+	return 0;
+}
+
+int authority_remove_handler(int id, void *p, void *para)
+{
+	struct auth_struct *auth = (struct auth_struct *)p;
+
+	mutex_lock(&auth->mutex);
+#ifdef CONFIG_QOS_CTRL
+	qos_switch(auth, AUTH_STATUS_DISABLED);
+#endif
+	auth->status = AUTH_STATUS_DEAD;
+	mutex_unlock(&auth->mutex);
+	put_auth_struct(auth);
+
+	return 0;
+}
+
+void remove_authority_control(void)
+{
+	int ret;
+
+	mutex_lock(&ua_idr_mutex);
+	ret = idr_for_each(ua_idr, authority_remove_handler, NULL);
+	if (ret < 0)
+		pr_err("[AUTH_CTRL] authority item remove failed\n");
+
+	idr_destroy(ua_idr);
+	kfree(ua_idr);
+
+	mutex_unlock(&ua_idr_mutex);
+}
+
+/*
+ * constrain user assigned auth_flag to kernel accepted auth_flag
+ */
+static int generic_auth_trim(unsigned int orig_flag, unsigned int constrain)
+{
+	return orig_flag & constrain;
+}
+
+static inline void set_auth_flag(struct auth_ctrl_data *data, struct auth_struct *auth_to_enable)
+{
+#ifdef CONFIG_RTG_AUTHORITY
+	auth_to_enable->rtg_auth_flag = generic_auth_trim(data->rtg_ua_flag, AF_RTG_DELEGATED);
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	auth_to_enable->qos_auth_flag = generic_auth_trim(data->qos_ua_flag, AF_QOS_ALL);
+#endif
+}
+
+static int auth_enable(struct auth_ctrl_data *data)
+{
+	struct auth_struct *auth_to_enable;
+	unsigned int tgid = data->pid;
+	int status = data->status;
+	int ret;
+
+	mutex_lock(&ua_idr_mutex);
+	auth_to_enable = idr_find(ua_idr, tgid);
+	/* auth exist, just resume the task's qos request */
+	if (auth_to_enable) {
+		get_auth_struct(auth_to_enable);
+		mutex_unlock(&ua_idr_mutex);
+
+		mutex_lock(&auth_to_enable->mutex);
+		if (auth_to_enable->status == AUTH_STATUS_DEAD) {
+			mutex_unlock(&auth_to_enable->mutex);
+			put_auth_struct(auth_to_enable);
+			return -INVALID_AUTH;
+		}
+
+		set_auth_flag(data, auth_to_enable);
+#ifdef CONFIG_QOS_CTRL
+		qos_switch(auth_to_enable, status);
+#endif
+		auth_to_enable->status = status;
+		mutex_unlock(&auth_to_enable->mutex);
+		ret = 0;
+		put_auth_struct(auth_to_enable);
+		goto out;
+	}
+
+	/* auth not exist, build a new auth, then insert to idr */
+	auth_to_enable = kzalloc(sizeof(*auth_to_enable), GFP_ATOMIC);
+	if (!auth_to_enable) {
+		mutex_unlock(&ua_idr_mutex);
+		pr_err("[AUTH_CTRL] alloc auth data failed, no memory!\n");
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	init_authority_record(auth_to_enable);
+
+	/* no one could get the auth from idr now, no need to lock */
+	set_auth_flag(data, auth_to_enable);
+	auth_to_enable->status = status;
+
+	ret = idr_alloc(ua_idr, auth_to_enable, tgid, tgid + 1, GFP_ATOMIC);
+	if (ret < 0) {
+		pr_err("[AUTH_CTRL] add auth to idr failed, no memory!\n");
+		kfree(auth_to_enable);
+	}
+
+	mutex_unlock(&ua_idr_mutex);
+
+out:
+	return ret;
+}
+
+static int auth_delete(struct auth_ctrl_data *data)
+{
+	struct auth_struct *auth_to_delete;
+	unsigned int tgid = data->pid;
+
+	mutex_lock(&ua_idr_mutex);
+	auth_to_delete = (struct auth_struct *)idr_remove(ua_idr, tgid);
+	if (!auth_to_delete) {
+		mutex_unlock(&ua_idr_mutex);
+		pr_err("[AUTH_CTRL] no auth data for this pid=%d, delete failed\n", tgid);
+		return -PID_NOT_FOUND;
+	}
+	mutex_unlock(&ua_idr_mutex);
+
+	mutex_lock(&auth_to_delete->mutex);
+#ifdef CONFIG_QOS_CTRL
+	qos_switch(auth_to_delete, AUTH_STATUS_DISABLED);
+#endif
+	auth_to_delete->status = AUTH_STATUS_DEAD;
+	mutex_unlock(&auth_to_delete->mutex);
+
+	put_auth_struct(auth_to_delete);
+
+	return 0;
+}
+
+static int auth_get(struct auth_ctrl_data *data)
+{
+	struct auth_struct *auth_to_get;
+	unsigned int tgid = data->pid;
+
+	mutex_lock(&ua_idr_mutex);
+	auth_to_get = idr_find(ua_idr, tgid);
+	if (!auth_to_get) {
+		mutex_unlock(&ua_idr_mutex);
+		pr_err("[AUTH_CTRL] no auth data for this pid=%d to get\n", tgid);
+		return -PID_NOT_FOUND;
+	}
+	get_auth_struct(auth_to_get);
+	mutex_unlock(&ua_idr_mutex);
+
+	mutex_lock(&auth_to_get->mutex);
+	if (auth_to_get->status == AUTH_STATUS_DEAD) {
+		mutex_unlock(&auth_to_get->mutex);
+		put_auth_struct(auth_to_get);
+		return -INVALID_AUTH;
+	}
+#ifdef CONFIG_RTG_AUTHORITY
+	data->rtg_ua_flag = auth_to_get->rtg_auth_flag;
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	data->qos_ua_flag = auth_to_get->qos_auth_flag;
+#endif
+	data->status = auth_to_get->status;
+	mutex_unlock(&auth_to_get->mutex);
+
+	put_auth_struct(auth_to_get);
+
+	return 0;
+}
+
+static int auth_switch(struct auth_ctrl_data *data)
+{
+	struct auth_struct *auth;
+	unsigned int tgid = data->pid;
+	unsigned int status = data->status;
+
+	if (status == 0 || status >= AUTH_STATUS_MAX_NR) {
+		pr_err("[AUTH_CTRL] not valied status %d\n", status);
+		return -ARG_INVALID;
+	}
+
+	mutex_lock(&ua_idr_mutex);
+	auth = idr_find(ua_idr, tgid);
+	if (!auth) {
+		mutex_unlock(&ua_idr_mutex);
+		pr_err("[AUTH_CTRL] no auth data for this pid=%d to switch\n", tgid);
+		return -PID_NOT_FOUND;
+	}
+	get_auth_struct(auth);
+	mutex_unlock(&ua_idr_mutex);
+
+	mutex_lock(&auth->mutex);
+	if (auth->status == AUTH_STATUS_DEAD) {
+		mutex_unlock(&auth->mutex);
+		put_auth_struct(auth);
+		return -INVALID_AUTH;
+	}
+
+	set_auth_flag(data, auth);
+#ifdef CONFIG_QOS_CTRL
+	qos_switch(auth, status);
+#endif
+	auth->status = status;
+	mutex_unlock(&auth->mutex);
+
+	put_auth_struct(auth);
+
+	return 0;
+}
+
+typedef int (*auth_manipulate_func)(struct auth_ctrl_data *data);
+
+static auth_manipulate_func auth_func_array[AUTH_MAX_NR] = {
+	/*
+	 * auth_enable: Start authority control for specific tgid.
+	 * auth_delte:  End authroity control, remove statistic datas.
+	 * auth_get:    Get auth info, deprecated.
+	 * auth_switch: Change authority flag and status for specific tgid.
+	 */
+	NULL,
+	auth_enable,
+	auth_delete,
+	auth_get,
+	auth_switch,
+};
+
+static long do_auth_manipulate(struct auth_ctrl_data *data)
+{
+	long ret = 0;
+	unsigned int type = data->type;
+
+	if (type >= AUTH_MAX_NR) {
+		pr_err("[AUTH_CTRL] BASIC_AUTH_CTRL_OPERATION type not valid\n");
+		return -ARG_INVALID;
+	}
+
+	if (auth_func_array[type])
+		ret = (long)(*auth_func_array[type])(data);
+
+	return ret;
+}
+
+static long ctrl_auth_basic_operation(int abi, void __user *uarg)
+{
+	struct auth_ctrl_data auth_data;
+	long ret = -1;
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpointer-to-int-cast"
+
+	switch (abi) {
+	case AUTH_IOCTL_ABI_ARM32:
+		ret = copy_from_user(&auth_data,
+				(void __user *)compat_ptr((compat_uptr_t)uarg),
+				sizeof(struct auth_ctrl_data));
+		break;
+	case AUTH_IOCTL_ABI_AARCH64:
+		ret = copy_from_user(&auth_data, uarg, sizeof(struct auth_ctrl_data));
+		break;
+	default:
+		pr_err("[AUTH_CTRL] abi format error\n");
+		break;
+	}
+
+#pragma GCC diagnostic pop
+
+	if (ret) {
+		pr_err("[AUTH_RTG] %s copy user data failed\n", __func__);
+		return ret;
+	}
+
+	ret = do_auth_manipulate(&auth_data);
+	if (ret < 0) {
+		pr_err("[AUTH_CTRL] BASIC_AUTH_CTRL_OPERATION failed\n");
+		return ret;
+	}
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpointer-to-int-cast"
+
+	switch (abi) {
+	case AUTH_IOCTL_ABI_ARM32:
+		ret = copy_to_user((void __user *)compat_ptr((compat_uptr_t)uarg),
+				&auth_data,
+				sizeof(struct auth_ctrl_data));
+		break;
+	case AUTH_IOCTL_ABI_AARCH64:
+		ret = copy_to_user(uarg, &auth_data, sizeof(struct auth_ctrl_data));
+		break;
+	default:
+		pr_err("[AUTH_CTRL] abi format error\n");
+		break;
+	}
+
+#pragma GCC diagnostic pop
+
+	if (ret) {
+		pr_err("[AUTH_RTG] %s copy user data failed\n", __func__);
+		return ret;
+	}
+
+	return 0;
+}
+
+long do_auth_ctrl_ioctl(int abi, struct file *file, unsigned int cmd, unsigned long arg)
+{
+	void __user *uarg = (void __user *)arg;
+	unsigned int func_cmd = _IOC_NR(cmd);
+
+	if (uarg == NULL) {
+		pr_err("%s: invalid user uarg\n", __func__);
+		return -EINVAL;
+	}
+
+	if (_IOC_TYPE(cmd) != AUTH_CTRL_IPC_MAGIG) {
+		pr_err("%s: authority ctrl magic fail, TYPE=%d\n",
+		       __func__, _IOC_TYPE(cmd));
+		return -EINVAL;
+	}
+
+	if (func_cmd >= AUTH_CTRL_MAX_NR) {
+		pr_err("%s: authority ctrl cmd error, cmd:%d\n",
+		       __func__, _IOC_TYPE(cmd));
+		return -EINVAL;
+	}
+
+	if (g_func_array[func_cmd])
+		return (*g_func_array[func_cmd])(abi, uarg);
+
+	return -EINVAL;
+}
+
+#define get_authority_flag(func_id)	(1 << (func_id - 1))
+
+static inline unsigned int get_true_uid(struct task_struct *p)
+{
+	if (!p)
+		return get_uid(current_user())->uid.val;
+
+	return task_uid(p).val;
+}
+
+/*
+ * Return 1000 for both SYSTEM and ROOT
+ * Return current's uid if p is NULL
+ */
+static inline unsigned int get_authority_uid(struct task_struct *p)
+{
+	unsigned int uid = get_true_uid(p);
+
+	if (super_uid(uid))
+		uid = SUPER_UID;
+
+	return uid;
+}
+
+static unsigned int auth_flag(struct auth_struct *auth, unsigned int type)
+{
+	switch (type) {
+#ifdef CONFIG_RTG_AUTHORITY
+	case RTG_AUTH_FLAG:
+		return auth->rtg_auth_flag;
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	case QOS_AUTH_FLAG:
+		return auth->qos_auth_flag;
+#endif
+	default:
+		pr_err("[AUTH_CTRL] not valid auth type\n");
+		return INVALIED_AUTH_FLAG;
+	}
+}
+
+bool check_authorized(unsigned int func_id, unsigned int type)
+{
+	bool authorized = false;
+	struct auth_struct *auth;
+	unsigned int af = get_authority_flag(func_id);
+	unsigned int uid = get_authority_uid(NULL);
+	unsigned int tgid = task_tgid_nr(current);
+
+	mutex_lock(&ua_idr_mutex);
+	if (!ua_idr) {
+		mutex_unlock(&ua_idr_mutex);
+		pr_err("[AUTH_CTRL] authority idr table missed, auth failed\n");
+		return authorized;
+	}
+
+	auth = (struct auth_struct *)idr_find(ua_idr, tgid);
+	if (!auth) {
+		if (uid != SUPER_UID) {
+			mutex_unlock(&ua_idr_mutex);
+			pr_err("[AUTH_CTRL] no auth data for this pid = %d\n", tgid);
+			return authorized;
+		} else if (init_super_authority(tgid)) {
+			mutex_unlock(&ua_idr_mutex);
+			pr_err("[AUTH_CTRL] init super authority failed\n");
+			return authorized;
+		}
+
+		//the auth must exist
+		auth = (struct auth_struct *)idr_find(ua_idr, tgid);
+		if (!auth)
+			return authorized;
+	}
+
+	get_auth_struct(auth);
+	mutex_unlock(&ua_idr_mutex);
+
+	mutex_lock(&auth->mutex);
+	if (auth->status == AUTH_STATUS_DEAD) {
+		mutex_unlock(&auth->mutex);
+		pr_info("[AUTH_CTRL] not valid auth for pid %d\n", tgid);
+		put_auth_struct(auth);
+		return authorized;
+	}
+	if (auth && (auth_flag(auth, type) & af))
+		authorized = true;
+
+	mutex_unlock(&auth->mutex);
+
+	put_auth_struct(auth);
+
+	return authorized;
+}
+
+/*
+ * Return authority info for given task
+ * return current's auth if p is NULL
+ * refcount will inc if this call return the valid auth
+ * make sure to call put_auth_struct before the calling end
+ */
+struct auth_struct *get_authority(struct task_struct *p)
+{
+	unsigned int tgid;
+	struct auth_struct *auth;
+
+	tgid = (p == NULL ? current->tgid : p->tgid);
+
+	mutex_lock(&ua_idr_mutex);
+	auth = idr_find(ua_idr, tgid);
+	if (auth)
+		get_auth_struct(auth);
+
+	mutex_unlock(&ua_idr_mutex);
+
+	return auth;
+}
+
+long proc_auth_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	return do_auth_ctrl_ioctl(AUTH_IOCTL_ABI_AARCH64, file, cmd, arg);
+}
+
+#ifdef CONFIG_COMPAT
+long proc_auth_compat_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	return do_auth_ctrl_ioctl(AUTH_IOCTL_ABI_ARM32, file, cmd,
+				(unsigned long)(compat_ptr((compat_uptr_t)arg)));
+}
+#endif
+
+static const struct file_operations auth_ctrl_fops = {
+	.owner		= THIS_MODULE,
+	.unlocked_ioctl = proc_auth_ioctl,
+#ifdef CONFIG_COMPAT
+	.compat_ioctl   = proc_auth_compat_ioctl,
+#endif
+};
+
+static struct miscdevice auth_ctrl_device = {
+	.minor		= MISC_DYNAMIC_MINOR,
+	.name		= "auth_ctrl",
+	.fops		= &auth_ctrl_fops,
+};
+
+static __init int auth_ctrl_init_module(void)
+{
+	int err;
+
+	err = misc_register(&auth_ctrl_device);
+	if (err < 0) {
+		pr_err("auth_ctrl register failed\n");
+		return err;
+	}
+
+	pr_info("auth_ctrl init success\n");
+
+	BUG_ON(init_ua_idr());
+
+#ifdef CONFIG_QOS_CTRL
+	init_qos_ctrl();
+#endif
+
+	init_sched_auth_debug_procfs();
+
+	return 0;
+}
+
+static void auth_ctrl_exit_module(void)
+{
+	remove_authority_control();
+	misc_deregister(&auth_ctrl_device);
+}
+
+/* module entry points */
+module_init(auth_ctrl_init_module);
+module_exit(auth_ctrl_exit_module);
+
+MODULE_LICENSE("GPL v2");
+
diff --git a/drivers/auth_ctl/auth_ctrl.h b/drivers/auth_ctl/auth_ctrl.h
new file mode 100644
index 000000000..e55c39abe
--- /dev/null
+++ b/drivers/auth_ctl/auth_ctrl.h
@@ -0,0 +1,52 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * drivers/auth_ctl/auth_ctrl.h
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ *
+ */
+
+#ifndef __AUTH_CTRL_H
+#define __AUTH_CTRL_H
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+#include <linux/spinlock.h>
+#include <linux/list.h>
+#include <linux/refcount.h>
+
+#include <linux/sched/qos_ctrl.h>
+
+struct auth_struct {
+	struct mutex mutex;
+	refcount_t usage;
+	unsigned int status;
+#ifdef CONFIG_RTG_AUTHORITY
+	unsigned int rtg_auth_flag;
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	unsigned int qos_auth_flag;
+#endif
+#ifdef CONFIG_QOS_CTRL
+	unsigned int num[NR_QOS];
+	struct list_head tasks[NR_QOS];
+#endif
+};
+
+/*
+ * for debug fs
+ */
+struct idr *get_auth_ctrl_idr(void);
+struct mutex *get_auth_idr_mutex(void);
+
+#ifdef CONFIG_AUTH_QOS_DEBUG
+int __init init_sched_auth_debug_procfs(void);
+#else
+static inline int init_sched_auth_debug_procfs(void)
+{
+	return 0;
+}
+#endif
+
+#endif /* __AUTH_CTRL_H */
+
diff --git a/drivers/auth_ctl/auth_qos_debug.c b/drivers/auth_ctl/auth_qos_debug.c
new file mode 100644
index 000000000..b140b50ea
--- /dev/null
+++ b/drivers/auth_ctl/auth_qos_debug.c
@@ -0,0 +1,157 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/auth_ctl/auth_qos_debug.c
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ *
+ */
+#include <linux/cred.h>
+#include <linux/mutex.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/sched/auth_ctrl.h>
+#include <linux/sched/rtg_auth.h>
+#include <linux/sched/qos_ctrl.h>
+#include <linux/sched/qos_auth.h>
+
+#include "auth_ctrl.h"
+#include "qos_ctrl.h"
+
+#define seq_printf_auth(m, x...) \
+do { \
+	if (m) \
+		seq_printf(m, x); \
+	else \
+		printk(x); \
+} while (0)
+
+static void print_auth_id(struct seq_file *file,
+	const int tgid)
+{
+	seq_printf_auth(file, "AUTH_PID            :%d\n", tgid);
+}
+
+static void print_auth_info(struct seq_file *file,
+	const struct auth_struct *auth)
+{
+	seq_printf_auth(file, "AUTH_STATUS        :%d\n", auth->status);
+#ifdef CONFIG_RTG_AUTHORITY
+	seq_printf_auth(file, "RTG_FLAG           :%04x\n", auth->rtg_auth_flag);
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	seq_printf_auth(file, "QOS_FLAG           :%04x\n", auth->qos_auth_flag);
+#endif
+}
+
+static void print_qos_count(struct seq_file *file,
+	const struct auth_struct *auth)
+{
+	int i;
+
+	for (i = QOS_POLICY_MIN_LEVEL; i < NR_QOS; ++i)
+		seq_printf_auth(file, "QOS level %d thread nr  :%d\n", i, auth->num[i]);
+}
+
+static void print_qos_thread(struct seq_file *file,
+	const struct auth_struct *auth)
+{
+	struct qos_task_struct *tmp, *next;
+	struct task_struct *p;
+	int i;
+
+	for (i = QOS_POLICY_MIN_LEVEL; i < NR_QOS; ++i) {
+		seq_printf_auth(file, "QOS level %d threads:", i);
+		list_for_each_entry_safe(tmp, next, &auth->tasks[i], qos_list) {
+			p = container_of(tmp, struct task_struct, qts);
+			seq_printf_auth(file, "%d ", p->pid);
+		}
+		seq_printf_auth(file, "\n");
+	}
+
+}
+
+static inline void print_auth_struct(struct seq_file *file, struct auth_struct *auth)
+{
+	print_auth_info(file, auth);
+	seq_printf_auth(file, "\n");
+	print_qos_count(file, auth);
+	seq_printf_auth(file, "\n");
+#ifdef CONFIG_QOS_CTRL
+	print_qos_thread(file, auth);
+#endif
+	seq_printf_auth(file, "---------------------------------------------------------\n");
+
+}
+
+int authority_printf_handler(int id, void *p, void *para)
+{
+	struct auth_struct *auth = (struct auth_struct *)p;
+	struct seq_file *file = (struct seq_file *)para;
+
+	/*
+	 * data consistency is not that important here
+	 */
+	seq_printf_auth(file, "\n\n");
+	print_auth_id(file, id);
+	seq_printf_auth(file, "\n");
+
+	/* no need to add refcount here, auth must alive in ua_idr_mutex */
+	print_auth_struct(file, auth);
+
+	return 0;
+}
+
+static int sched_auth_debug_show(struct seq_file *file, void *param)
+{
+	struct idr *ua_idr = get_auth_ctrl_idr();
+	struct mutex *ua_idr_mutex = get_auth_idr_mutex();
+	/*
+	 * NOTICE:
+	 * if mutex in authority_printf_handler, sleep may occur
+	 * change ths spin_lock to mutex, or remove mutex in handler
+	 */
+
+	mutex_lock(ua_idr_mutex);
+	/* will never return 0 here, auth in ua_idr must alive */
+	idr_for_each(ua_idr, authority_printf_handler, file);
+	mutex_unlock(ua_idr_mutex);
+
+	return 0;
+}
+
+static int sched_auth_debug_release(struct inode *inode, struct file *file)
+{
+	seq_release(inode, file);
+	return 0;
+}
+
+static int sched_auth_debug_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, sched_auth_debug_show, NULL);
+}
+
+static const struct proc_ops sched_auth_debug_fops = {
+	.proc_open = sched_auth_debug_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = sched_auth_debug_release,
+};
+
+int __init init_sched_auth_debug_procfs(void)
+{
+	struct proc_dir_entry *pe = NULL;
+
+	pe = proc_create("sched_auth_qos_debug",
+		0400, NULL, &sched_auth_debug_fops);
+	if (unlikely(!pe))
+		return -ENOMEM;
+	return 0;
+}
+
diff --git a/drivers/auth_ctl/qos_ctrl.c b/drivers/auth_ctl/qos_ctrl.c
new file mode 100644
index 000000000..b5cd8206e
--- /dev/null
+++ b/drivers/auth_ctl/qos_ctrl.c
@@ -0,0 +1,764 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * drivers/auth_ctl/auth_ctrl.c
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ *
+ */
+#include <linux/sched.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/stop_machine.h>
+#include <linux/sched/auth_ctrl.h>
+#include <linux/sched/rtg_auth.h>
+#include <linux/sched/qos_ctrl.h>
+#include <linux/sched/qos_auth.h>
+#include <uapi/linux/sched/types.h>
+
+#include "auth_ctrl.h"
+#include "qos_ctrl.h"
+
+typedef long (*qos_ctrl_func)(int abi, void __user *uarg);
+
+static long ctrl_qos_operation(int abi, void __user *uarg);
+static long ctrl_qos_policy(int abi, void __user *uarg);
+
+#define QOS_LEVEL_SET_MAX 5
+
+static qos_ctrl_func g_func_array[QOS_CTRL_MAX_NR] = {
+	NULL, /* reserved */
+	ctrl_qos_operation,
+	ctrl_qos_policy,
+};
+
+static struct qos_policy_map qos_policy_array[QOS_POLICY_MAX_NR];
+
+void remove_qos_tasks(struct auth_struct *auth)
+{
+	int i;
+	struct qos_task_struct *tmp, *next;
+	struct task_struct *p;
+
+	mutex_lock(&auth->mutex);
+	for (i = QOS_POLICY_MIN_LEVEL; i < NR_QOS; ++i) {
+		list_for_each_entry_safe(tmp, next, &auth->tasks[i], qos_list) {
+			struct qos_task_struct **tmp_ptr = &tmp;
+			p = container_of(tmp_ptr, struct task_struct, qts);
+			if (!list_empty(&tmp->qos_list)) {
+				list_del_init(&tmp->qos_list);
+				tmp->in_qos = NO_QOS;
+				put_task_struct(p);
+			}
+		}
+	}
+	mutex_unlock(&auth->mutex);
+}
+
+static void init_sched_attr(struct sched_attr *attr)
+{
+	memset(attr, 0, sizeof(struct sched_attr));
+}
+
+static inline bool is_system(unsigned int uid)
+{
+	return uid == SYSTEM_UID;
+}
+
+/* This function must be called when p is valid. That means the p's refcount must exist */
+static int sched_set_task_qos_attr(struct task_struct *p, int level, int status)
+{
+	struct qos_policy_item *item;
+	struct qos_policy_map *policy_map;
+	struct sched_attr attr;
+
+	read_lock(&qos_policy_array[status].lock);
+	if (!qos_policy_array[status].initialized) {
+		pr_err("[QOS_CTRL] dirty qos policy, pid=%d, uid=%d, status=%d\n",
+		       p->pid, p->cred->uid.val, status);
+		read_unlock(&qos_policy_array[status].lock);
+		return -DIRTY_QOS_POLICY;
+	}
+
+	policy_map = &qos_policy_array[status];
+	item = &policy_map->levels[level];
+
+	init_sched_attr(&attr);
+	attr.size			= sizeof(struct sched_attr);
+	attr.sched_policy		= SCHED_NORMAL;
+
+	if (policy_map->policy_flag & QOS_FLAG_NICE)
+		attr.sched_nice = item->nice;
+
+
+	if ((policy_map->policy_flag & QOS_FLAG_RT) && item->rt_sched_priority) {
+		attr.sched_policy = SCHED_FIFO;
+		attr.sched_flags |= SCHED_FLAG_RESET_ON_FORK;
+		attr.sched_priority = item->rt_sched_priority;
+	}
+
+	read_unlock(&qos_policy_array[status].lock);
+
+	if (unlikely(p->flags & PF_EXITING)) {
+		pr_info("[QOS_CTRL] dying task, no need to set qos\n");
+		return -THREAD_EXITING;
+	}
+
+	return sched_setattr_nocheck(p, &attr);
+}
+
+/*
+ * Switch qos mode when status changed.
+ * Lock auth before calling this function
+ */
+void qos_switch(struct auth_struct *auth, int target_status)
+{
+	int i;
+	int ret;
+	struct task_struct *task;
+	struct qos_task_struct *qts;
+
+	if (!auth) {
+		pr_err("[QOS_CTRL] auth no exist, qos switch failed\n");
+		return;
+	}
+
+	lockdep_assert_held(&auth->mutex);
+
+	if (auth->status == target_status) {
+		pr_info("[QOS_CTRL] same status, no need to switch qos\n");
+		return;
+	}
+
+	for (i = QOS_POLICY_MIN_LEVEL; i < NR_QOS; ++i) {
+		list_for_each_entry(qts, &auth->tasks[i], qos_list) {
+			struct qos_task_struct **qts_ptr = &qts;
+			task = container_of(qts_ptr, struct task_struct, qts);
+			ret = sched_set_task_qos_attr(task, i, target_status);
+			if (ret)
+				pr_err("[QOS_CTRL] set qos attr failed, qos switch failed\n");
+		}
+	}
+}
+
+static int qos_insert_task(struct task_struct *p, struct list_head *head, unsigned int level)
+{
+	struct qos_task_struct *qts = p->qts;
+
+	if (qts->in_qos > NO_QOS) {
+		pr_err("[QOS_CTRL] qos apply still active, no duplicate add\n");
+		return -PID_DUPLICATE;
+	}
+
+	if (likely(list_empty(&qts->qos_list))) {
+		get_task_struct(p);
+		list_add(&qts->qos_list, head);
+		qts->in_qos = level;
+	}
+
+	return 0;
+}
+
+static int qos_remove_task(struct task_struct *p)
+{
+	struct qos_task_struct *qts = (struct qos_task_struct *) &p->qts;
+
+	if (qts->in_qos == NO_QOS) {
+		pr_err("[QOS_CTRL] task not in qos, no need to remove\n");
+		return -PID_NOT_EXIST;
+	}
+
+	if (likely(!list_empty(&qts->qos_list))) {
+		list_del_init(&qts->qos_list);
+		qts->in_qos = NO_QOS;
+		put_task_struct(p);
+	}
+
+	return 0;
+}
+
+static inline bool super_user(struct task_struct *p)
+{
+	return super_uid(task_uid(p).val);
+}
+
+/*
+ * judge permission for changing tasks' qos
+ */
+static bool can_change_qos(struct task_struct *p, unsigned int qos_level)
+{
+	struct auth_struct *auth;
+	auth = get_authority(p);
+	/* just system & root user can set(be setted) high qos level */
+	if (!auth || (auth && !super_user(p) && qos_level > QOS_LEVEL_SET_MAX)) {
+		pr_err("[QOS_CTRL] %d have no permission to change qos\n", p->pid);
+		return false;
+	}
+
+	return true;
+}
+
+int qos_apply(struct qos_ctrl_data *data)
+{
+	unsigned int level = data->level;
+	struct auth_struct *auth;
+	struct task_struct *p;
+	struct qos_task_struct *qts;
+	int pid = data->pid;
+	int ret;
+
+	if (level >= NR_QOS || level == NO_QOS) {
+		pr_err("[QOS_CTRL] no this qos level, qos apply failed\n");
+		ret = -ARG_INVALID;
+		goto out;
+	}
+
+	p = find_get_task_by_vpid((pid_t)pid);
+	if (unlikely(!p)) {
+		pr_err("[QOS_CTRL] no matching task for this pid, qos apply failed\n");
+		ret = -ESRCH;
+		goto out;
+	}
+
+	if (unlikely(p->flags & PF_EXITING)) {
+		pr_info("[QOS_CTRL] dying task, no need to set qos\n");
+		ret = -THREAD_EXITING;
+		goto out_put_task;
+	}
+
+	if (!can_change_qos(current, level)) {
+		pr_err("[QOS_CTRL] QOS apply not permit\n");
+		ret = -ARG_INVALID;
+		goto out_put_task;
+	}
+
+	auth = get_authority(p);
+	if (!auth) {
+		pr_err("[QOS_CTRL] no auth data for pid=%d(%s), qos apply failed\n",
+		       p->tgid, p->comm);
+		ret = -PID_NOT_FOUND;
+		goto out_put_task;
+	}
+
+	mutex_lock(&auth->mutex);
+	if (auth->status == AUTH_STATUS_DEAD) {
+		pr_err("[QOS_CTRL] this auth data has been deleted\n");
+		ret = -INVALID_AUTH;
+		goto out_unlock;
+	}
+
+	if (auth->num[level] >= QOS_NUM_MAX) {
+		pr_err("[QOS_CTRL] qos num exceeds limit, cached only\n");
+		ret = -QOS_THREAD_NUM_EXCEED_LIMIT;
+		goto out_unlock;
+	}
+
+	qts = (struct qos_task_struct *) &p->qts;
+
+	if (rt_task(p) && qts->in_qos == NO_QOS) {
+		pr_err("[QOS_CTRL] can not apply qos for native rt task\n");
+		ret = -ALREADY_RT_TASK;
+		goto out_unlock;
+	}
+
+	/* effective qos must in range [NO_QOS, NR_QOS) */
+	if (qts->in_qos != NO_QOS) {
+		if (qts->in_qos == level) {
+			ret = 0;
+			goto out_unlock;
+		}
+
+		--auth->num[qts->in_qos];
+		qos_remove_task(p);
+	}
+
+	ret = qos_insert_task(p, &auth->tasks[level], level);
+	if (ret < 0) {
+		pr_err("[QOS_CTRL] insert task to qos list %d failed\n", level);
+		goto out_unlock;
+	}
+
+	++auth->num[level];
+
+	ret = sched_set_task_qos_attr(p, level, auth->status);
+	if (ret) {
+		pr_err("[QOS_CTRL] set qos_level %d for thread %d on status %d failed\n",
+		       level, p->pid, auth->status);
+		--auth->num[level];
+		qos_remove_task(p);
+	}
+
+out_unlock:
+	mutex_unlock(&auth->mutex);
+	put_auth_struct(auth);
+out_put_task:
+	put_task_struct(p);
+out:
+	return ret;
+}
+
+int qos_leave(struct qos_ctrl_data *data)
+{
+	unsigned int level;
+	struct auth_struct *auth;
+	struct task_struct *p;
+	struct qos_task_struct *qts;
+	int pid = data->pid;
+	int ret;
+
+	p = find_get_task_by_vpid((pid_t)pid);
+	if (!p) {
+		pr_err("[QOS_CTRL] no matching task for this pid, qos apply failed\n");
+		ret = -ESRCH;
+		goto out;
+	}
+
+	if (unlikely(p->flags & PF_EXITING)) {
+		pr_info("[QOS_CTRL] dying task, no need to set qos\n");
+		ret = -THREAD_EXITING;
+		goto out_put_task;
+	}
+
+	auth = get_authority(p);
+	if (!auth) {
+		pr_err("[QOS_CTRL] no auth data for pid=%d(%s), qos stop failed\n",
+		       p->tgid, p->comm);
+		ret = -PID_NOT_FOUND;
+		goto out_put_task;
+	}
+
+	mutex_lock(&auth->mutex);
+
+	qts = (struct qos_task_struct *) &p->qts;
+
+	level = qts->in_qos;
+	if (level == NO_QOS) {
+		pr_err("[QOS_CTRL] task not in qos list, qos stop failed\n");
+		ret = -ARG_INVALID;
+		goto out_unlock;
+	}
+
+	if (!can_change_qos(current, 0)) {
+		pr_err("[QOS_CTRL] apply for others not permit\n");
+		ret = -ARG_INVALID;
+		goto out_unlock;
+	}
+
+	if (auth->status == AUTH_STATUS_DEAD) {
+		pr_err("[QOS_CTRL] this auth data has been deleted\n");
+		ret = -INVALID_AUTH;
+		goto out_unlock;
+	}
+
+	ret = qos_remove_task(p);
+	if (ret < 0) {
+		pr_err("[QOS_CTRL] remove task from qos list %d failed\n", level);
+		goto out_unlock;
+	}
+
+	--auth->num[level];
+
+	/*
+	 * NO NEED to judge whether current status is AUTH_STATUS_DISABLE.
+	 * In the auth destoring context, the removing of thread's sched attr was protected by
+	 * auth->mutex, AUTH_STATUS_DISABLED will never appear here.
+	 *
+	 * The second param 3 means nothing, actually you can use any valid level here, cause the
+	 * policy matching AUTH_STATUS_DISABLED has default parameters for all qos level, which can
+	 * keep a powerful thread to behave like a ordinary thread.
+	 */
+	ret = sched_set_task_qos_attr(p, 3, AUTH_STATUS_DISABLED);
+	if (ret)
+		pr_err("[QOS_CTRL] set qos_level %d for thread %d on status %d to default failed\n",
+		       level, p->pid, auth->status);
+
+out_unlock:
+	mutex_unlock(&auth->mutex);
+	put_auth_struct(auth);
+out_put_task:
+	put_task_struct(p);
+out:
+	return ret;
+}
+
+int qos_get(struct qos_ctrl_data *data)
+{
+	struct task_struct *p;
+	struct qos_task_struct *qts;
+	int pid = data->pid;
+	int ret = 0;
+
+	p = find_get_task_by_vpid((pid_t)pid);
+	if (unlikely(!p)) {
+		pr_err("[QOS_CTRL] no matching task for this pid, qos get failed\n");
+		ret = -ESRCH;
+		goto out;
+	}
+
+	if (unlikely(p->flags & PF_EXITING)) {
+		pr_info("[QOS_CTRL] dying task, no need to set qos\n");
+		ret = -THREAD_EXITING;
+		goto out_put_task;
+	}
+
+	qts = (struct qos_task_struct *) &p->qts;
+	data->qos = qts->in_qos;
+
+out_put_task:
+	put_task_struct(p);
+out:
+	return ret;
+}
+
+void init_task_qos(struct task_struct *p)
+{
+    p->qts = kzalloc(sizeof(struct qos_task_struct), GFP_KERNEL);
+    if (p->qts) {
+        INIT_LIST_HEAD(&p->qts->qos_list);
+        p->qts->in_qos = NO_QOS;
+    }
+}
+
+/*
+ * Remove statistic info in auth when task exit
+ */
+void sched_exit_qos_list(struct task_struct *p)
+{
+	struct auth_struct *auth;
+	struct qos_task_struct *qts = (struct qos_task_struct *) &p->qts;
+
+	/*
+	 * For common tasks(the vast majority):
+	 * skip get authority, fast return here.
+	 *
+	 * For qos tasks:
+	 * If contend with auth_delete() happens,
+	 * 1. function return here, auth_delete() will do the clean up
+	 * 2. function go on, either no auth return, either do clean up here
+	 * Both cases guarantee data synchronization
+	 */
+	if (likely(qts->in_qos == NO_QOS))
+		return;
+
+	auth = get_authority(p);
+	if (!auth)
+		goto out;
+
+	mutex_lock(&auth->mutex);
+	if (qts->in_qos == NO_QOS) {
+		mutex_unlock(&auth->mutex);
+		goto out_put_auth;
+	}
+	if (qts && qts->in_qos != NO_QOS) {
+    	--auth->num[qts->in_qos];
+    	list_del_init(&qts->qos_list);
+    	qts->in_qos = NO_QOS;
+    	put_task_struct(p);
+	}
+	mutex_unlock(&auth->mutex);
+	kfree(p->qts); // 释放 qts 内存
+	p->qts = NULL;
+
+out_put_auth:
+	put_auth_struct(auth);
+out:
+	return;
+}
+
+typedef int (*qos_manipulate_func)(struct qos_ctrl_data *data);
+
+static qos_manipulate_func qos_func_array[QOS_OPERATION_CMD_MAX_NR] = {
+	NULL,
+	qos_apply,  //1
+	qos_leave,
+	qos_get,
+};
+
+static long do_qos_manipulate(struct qos_ctrl_data *data)
+{
+	long ret = 0;
+	unsigned int type = data->type;
+
+	if (type <= 0 || type >= QOS_OPERATION_CMD_MAX_NR) {
+		pr_err("[QOS_CTRL] CMD_ID_QOS_MANIPULATE type not valid\n");
+		return -ARG_INVALID;
+	}
+
+	if (qos_func_array[type])
+		ret = (long)(*qos_func_array[type])(data);
+
+	return ret;
+}
+
+static long ctrl_qos_operation(int abi, void __user *uarg)
+{
+	struct qos_ctrl_data qos_data;
+	int ret = -1;
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpointer-to-int-cast"
+
+	switch (abi) {
+	case QOS_IOCTL_ABI_ARM32:
+		ret = copy_from_user(&qos_data,
+				(void __user *)compat_ptr((compat_uptr_t)uarg),
+				sizeof(struct qos_ctrl_data));
+		break;
+	case QOS_IOCTL_ABI_AARCH64:
+		ret = copy_from_user(&qos_data, uarg, sizeof(struct qos_ctrl_data));
+		break;
+	default:
+		pr_err("[QOS_CTRL] abi format error\n");
+		break;
+	}
+
+#pragma GCC diagnostic pop
+
+	if (ret) {
+		pr_err("[QOS_CTRL] %s copy user data failed\n", __func__);
+		return ret;
+	}
+
+	ret = do_qos_manipulate(&qos_data);
+	if (ret < 0) {
+		pr_err("[QOS_CTRL] CMD_ID_QOS_MANIPULATE failed\n");
+		return ret;
+	}
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpointer-to-int-cast"
+
+	switch (abi) {
+	case QOS_IOCTL_ABI_ARM32:
+		ret = copy_to_user((void __user *)compat_ptr((compat_uptr_t)uarg),
+				&qos_data, sizeof(struct qos_ctrl_data));
+		break;
+	case QOS_IOCTL_ABI_AARCH64:
+		ret = copy_to_user(uarg, &qos_data, sizeof(struct qos_ctrl_data));
+		break;
+	default:
+		pr_err("[QOS_CTRL] abi format error\n");
+		break;
+	}
+
+#pragma GCC diagnostic pop
+
+	if (ret) {
+		pr_err("[QOS_CTRL] %s copy to user failed\n", __func__);
+		return ret;
+	}
+	return 0;
+}
+
+#define MAX_LATENCY_NICE	19
+#define MIN_LATENCY_NICE	-20
+
+static inline bool valid_nice(int nice)
+{
+	return nice >= MIN_NICE && nice <= MAX_NICE;
+}
+
+static inline bool valid_latency_nice(int latency_nice)
+{
+	return latency_nice >= MIN_LATENCY_NICE && latency_nice <= MAX_LATENCY_NICE;
+}
+
+static inline bool valid_uclamp(int uclamp_min, int uclamp_max)
+{
+	if (uclamp_min > uclamp_max)
+		return false;
+	if (uclamp_max > SCHED_CAPACITY_SCALE)
+		return false;
+
+	return true;
+}
+
+static inline bool valid_rt(int sched_priority)
+{
+	if (sched_priority > MAX_USER_RT_PRIO - 1 || sched_priority < 0)
+		return false;
+
+	return true;
+}
+
+static bool valid_qos_flag(unsigned int qos_flag)
+{
+	if (qos_flag & ~QOS_FLAG_ALL)
+		return false;
+
+	return true;
+}
+
+static inline bool valid_qos_item(struct qos_policy_datas *datas)
+{
+	int i;
+	int type = datas->policy_type;
+	struct qos_policy_data *data;
+
+	if (type <= 0 || type >= QOS_POLICY_MAX_NR) {
+		pr_err("[QOS_CTRL] not valid qos policy type, policy change failed\n");
+		goto out_failed;
+	}
+
+	if (!valid_qos_flag(datas->policy_flag)) {
+		pr_err("[QOS_CTRL] not valid qos flag, policy change failed\n");
+		goto out_failed;
+	}
+
+	/* check user space qos polcicy data, level 0 reserved */
+	for (i = 0; i < NR_QOS; ++i) {
+		data = &datas->policys[i];
+
+		if (!valid_nice(data->nice)) {
+			pr_err("[QOS_CTRL] invalid nice, policy change failed\n");
+			goto out_failed;
+		}
+
+		if (!valid_latency_nice(data->latency_nice)) {
+			pr_err("[QOS_CTRL] invalid latency_nice, policy change failed\n");
+			goto out_failed;
+		}
+
+		if (!valid_uclamp(data->uclamp_min, data->uclamp_max)) {
+			pr_err("[QOS_CTRL] invalid uclamp, policy change failed\n");
+			goto out_failed;
+		}
+
+		if (!valid_rt(data->rt_sched_priority)) {
+			pr_err("[QOS_CTRL] invalid rt, policy change failed\n");
+			goto out_failed;
+		}
+	}
+
+	return true;
+
+out_failed:
+	pr_err("[QOS_CTRL] not valid qos policy params\n");
+	return false;
+}
+
+static long do_qos_policy_change(struct qos_policy_datas *datas)
+{
+	long ret = 0;
+	int i;
+	struct qos_policy_item *item;
+	struct qos_policy_data *data;
+	int type = datas->policy_type;
+
+	if (type >= QOS_POLICY_MAX_NR) {
+		pr_err("[QOS_CTRL] not valid policy type\n");
+		goto out_failed;
+	}
+
+	if (!valid_qos_item(datas))
+		goto out_failed;
+
+	write_lock(&qos_policy_array[type].lock);
+	for (i = QOS_POLICY_MIN_LEVEL; i < NR_QOS; ++i) {
+		item = &qos_policy_array[type].levels[i];
+
+		/* user space policy params */
+		data = &datas->policys[i];
+
+		item->nice = data->nice;
+		item->latency_nice = data->latency_nice;
+		item->uclamp_min = data->uclamp_min;
+		item->uclamp_max = data->uclamp_max;
+		/* only specific qos level could use SCHED_FIFO */
+		item->rt_sched_priority = (i < MIN_RT_QOS_LEVEL) ? 0 :
+					  data->rt_sched_priority;
+	}
+	qos_policy_array[type].policy_flag = datas->policy_flag;
+	qos_policy_array[type].initialized = true;
+	write_unlock(&qos_policy_array[type].lock);
+
+	return ret;
+
+out_failed:
+	return -ARG_INVALID;
+}
+
+static long ctrl_qos_policy(int abi, void __user *uarg)
+{
+	struct qos_policy_datas policy_datas;
+	long ret = -1;
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wpointer-to-int-cast"
+
+	switch (abi) {
+	case QOS_IOCTL_ABI_ARM32:
+		ret = copy_from_user(&policy_datas,
+				(void __user *)compat_ptr((compat_uptr_t)uarg),
+				sizeof(struct qos_policy_datas));
+		break;
+	case QOS_IOCTL_ABI_AARCH64:
+		ret = copy_from_user(&policy_datas, uarg, sizeof(struct qos_policy_datas));
+		break;
+	default:
+		pr_err("[QOS_CTRL] abi format error\n");
+		break;
+	}
+
+#pragma GCC diagnostic pop
+
+	if (ret) {
+		pr_err("[QOS_RTG] %s copy user data failed\n", __func__);
+		return ret;
+	}
+
+	return do_qos_policy_change(&policy_datas);
+}
+
+long do_qos_ctrl_ioctl(int abi, struct file *file, unsigned int cmd, unsigned long arg)
+{
+	void __user *uarg = (void __user *)arg;
+	unsigned int func_cmd = _IOC_NR(cmd);
+
+	if (uarg == NULL) {
+		pr_err("%s: invalid user uarg\n", __func__);
+		return -EINVAL;
+	}
+
+	if (_IOC_TYPE(cmd) != QOS_CTRL_IPC_MAGIG) {
+		pr_err("%s: qos ctrl magic fail, TYPE=%d\n",
+		       __func__, _IOC_TYPE(cmd));
+		return -EINVAL;
+	}
+
+	if (func_cmd >= QOS_CTRL_MAX_NR) {
+		pr_err("%s: qos ctrl cmd error, cmd:%d\n",
+		       __func__, _IOC_TYPE(cmd));
+		return -EINVAL;
+	}
+
+#ifdef CONFIG_QOS_AUTHORITY
+	if (!check_authorized(func_cmd, QOS_AUTH_FLAG)) {
+		pr_err("[QOS_CTRL] %s: pid not authorized\n", __func__);
+		return -PID_NOT_AUTHORIZED;
+	}
+#endif
+
+	if (g_func_array[func_cmd])
+		return (*g_func_array[func_cmd])(abi, uarg);
+
+	return -EINVAL;
+}
+
+static void init_qos_policy_array(void)
+{
+	int i;
+
+	/* index 0 reserved */
+	for (i = 1; i < QOS_POLICY_MAX_NR; ++i)
+		rwlock_init(&qos_policy_array[i].lock);
+
+	pr_info("[QOS_CTRL] lock in qos policy initialized\n");
+}
+
+int __init init_qos_ctrl(void)
+{
+	init_qos_policy_array();
+
+	return 0;
+}
+
diff --git a/drivers/auth_ctl/qos_ctrl.h b/drivers/auth_ctl/qos_ctrl.h
new file mode 100644
index 000000000..c3fb6b9e7
--- /dev/null
+++ b/drivers/auth_ctl/qos_ctrl.h
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * drivers/auth_ctl/qos_ctrl.h
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ *
+ */
+
+#ifndef __QOS_CTRL_H
+#define __QOS_CTRL_H
+
+#include "../../kernel/sched/sched.h"
+
+#include <linux/sched/qos_ctrl.h>
+
+/* min qos level used in kernel space, begin index for LOOP */
+#define QOS_POLICY_MIN_LEVEL 0
+
+#ifndef MAX_USER_RT_PRIO
+#define MAX_USER_RT_PRIO 100
+#endif
+
+struct qos_policy_item {
+	int nice;
+	int latency_nice;
+	int uclamp_min;
+	int uclamp_max;
+	int rt_sched_priority;
+	int policy;
+};
+
+struct qos_policy_map {
+	rwlock_t lock;
+	bool initialized;
+	unsigned int policy_flag;
+	struct qos_policy_item levels[NR_QOS];
+};
+
+int __init init_qos_ctrl(void);
+
+#endif /* __OQS_CTRL_H */
+
diff --git a/fs/proc/base.c b/fs/proc/base.c
index 08808cbbe..4652b1c0a 100644
--- a/fs/proc/base.c
+++ b/fs/proc/base.c
@@ -103,6 +103,10 @@
 #include "internal.h"
 #include "fd.h"
 
+#ifdef CONFIG_QOS_CTRL
+#include <linux/sched/qos_ctrl.h>
+#endif
+
 #include "../../lib/kstrtox.h"
 
 /* NOTE:
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 464131256..115a4c8fc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -737,6 +737,20 @@ struct kmap_ctrl {
 #endif
 };
 
+#ifdef CONFIG_QOS_CTRL
+struct qos_task_struct {
+	/*
+	 * 'in_qos' marks the qos level o current task, greater value for
+	 * greater qos, range from (NO_QOS, NR_QOS)
+	 *
+	 *
+	 * 'qos_list' use to track task with qos supply in auth_struct
+	 */
+	int                 in_qos;
+	struct list_head    qos_list;
+};
+#endif
+
 struct task_struct {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	/*
diff --git a/include/linux/sched/auth_ctrl.h b/include/linux/sched/auth_ctrl.h
new file mode 100644
index 000000000..7301cacc2
--- /dev/null
+++ b/include/linux/sched/auth_ctrl.h
@@ -0,0 +1,123 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * include/linux/sched/auth_ctrl.h
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ */
+
+#ifndef _AUTH_CTRL_H
+#define _AUTH_CTRL_H
+
+#include <linux/fs.h>
+
+#define ROOT_UID   0
+#define SYSTEM_UID 1000
+
+#define SUPER_UID SYSTEM_UID
+#define RESOURCE_SCHEDULE_SERVICE_UID 1096
+#define super_uid(uid) (uid == ROOT_UID || uid == SYSTEM_UID || uid == RESOURCE_SCHEDULE_SERVICE_UID)
+
+enum ioctl_abi_format_auth{
+	AUTH_IOCTL_ABI_ARM32,
+	AUTH_IOCTL_ABI_AARCH64,
+};
+
+enum auth_ctrl_cmdid {
+	BASIC_AUTH_CTRL = 1,
+	AUTH_CTRL_MAX_NR
+};
+
+#define AUTH_CTRL_IPC_MAGIG	0xCD
+
+#define	BASIC_AUTH_CTRL_OPERATION \
+	_IOWR(AUTH_CTRL_IPC_MAGIG, BASIC_AUTH_CTRL, struct auth_ctrl_data)
+
+enum auth_flag_type {
+#ifdef CONFIG_RTG_AUTHORITY
+	RTG_AUTH_FLAG,
+#endif
+#ifdef CONFIG_QOS_AUTHORITY
+	QOS_AUTH_FLAG,
+#endif
+};
+
+#define INVALIED_AUTH_FLAG	0x00000000
+
+struct auth_ctrl_data {
+	unsigned int pid;
+
+	/*
+	 * type:  operation type, see auth_manipulate_type, valid range [1, AUTH_MAX_NR)
+	 *
+	 * rtg_ua_flag: authority flag for RTG, see AF_RTG_ALL
+	 *
+	 * qos_ua_flag: authority flag for QOS, see AF_QOS_ALL
+	 *
+	 * status: current status for uid, use to match qos policy, see auth_status and
+	 * qos_policy_type, valid range [1, AUTH_STATUS_MAX_NR - 1)
+	 *
+	 */
+	unsigned int type;
+	unsigned int rtg_ua_flag;
+	unsigned int qos_ua_flag;
+	unsigned int status;
+};
+
+enum auth_err_no {
+	ARG_INVALID = 1,
+	THREAD_EXITING,
+	DIRTY_QOS_POLICY,
+	PID_NOT_AUTHORIZED,
+	PID_NOT_FOUND,
+	PID_DUPLICATE,
+	PID_NOT_EXIST,
+	INVALID_AUTH,
+	ALREADY_RT_TASK,
+	QOS_THREAD_NUM_EXCEED_LIMIT,
+};
+
+enum auth_manipulate_type {
+	AUTH_ENABLE = 1,
+	AUTH_DELETE,
+	AUTH_GET,
+	AUTH_SWITCH,
+	AUTH_MAX_NR,
+};
+
+#ifndef CONFIG_QOS_POLICY_MAX_NR
+#define QOS_STATUS_COUNT 5
+#else
+#define QOS_STATUS_COUNT CONFIG_QOS_POLICY_MAX_NR
+#endif
+
+/* keep match with qos_policy_type */
+enum auth_status {
+	/* reserved fo QOS_POLICY_DEFAULT, no qos supply in this status */
+	AUTH_STATUS_DISABLED = 1,
+
+	/* reserved for ROOT and SYSTEM */
+	AUTH_STATUS_SYSTEM_SERVER = 2,
+
+	/*
+	 * these space for user specific status
+	 * range (AUTH_STATUS_SYSTEM_SERVER, AUTH_STATUS_DEAD)
+	 *
+	 * initial the policy in matching index of qos_policy_array first before use
+	 * see ctrl_qos_policy
+	 */
+
+	/* reserved for destorying auth_struct*/
+	AUTH_STATUS_DEAD = QOS_STATUS_COUNT,
+
+	AUTH_STATUS_MAX_NR = QOS_STATUS_COUNT + 1,
+};
+
+struct auth_struct;
+long auth_ctrl_ioctl(int abi, struct file *file, unsigned int cmd, unsigned long arg);
+void get_auth_struct(struct auth_struct *auth);
+void put_auth_struct(struct auth_struct *auth);
+struct auth_struct *get_authority(struct task_struct *p);
+bool check_authorized(unsigned int func_id, unsigned int type);
+
+#endif /* _AUTH_CTRL_H */
+
diff --git a/include/linux/sched/qos_auth.h b/include/linux/sched/qos_auth.h
new file mode 100644
index 000000000..f89a91194
--- /dev/null
+++ b/include/linux/sched/qos_auth.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * include/linux/sched/qos_auth.h
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ */
+
+#ifndef _QOS_AUTH_H
+#define _QOS_AUTH_H
+
+#include <linux/sched.h>
+#include <linux/sched/auth_ctrl.h>
+
+/*
+ * QOS authority flags for SYSTEM or ROOT
+ *
+ * keep sync with qos_ctrl_cmdid
+ * when add a new cmd to qos_ctrl_cmdid
+ * keep new_flag = (old_flag << 1) + 1
+ * up to now, next flag value is 0x0007
+ */
+#define AF_QOS_ALL		0x0003
+
+/*
+ * delegated authority for normal uid
+ * trim access range for QOS
+ */
+#define AF_QOS_DELEGATED	0x0001
+
+bool check_authorized(unsigned int func_id, unsigned int type);
+
+#endif /* _QOS_AUTH_H */
+
diff --git a/include/linux/sched/qos_ctrl.h b/include/linux/sched/qos_ctrl.h
new file mode 100644
index 000000000..28b930079
--- /dev/null
+++ b/include/linux/sched/qos_ctrl.h
@@ -0,0 +1,129 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * include/linux/sched/qos_ctrl.h
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ */
+
+#ifndef _QOS_CTRL_H
+#define _QOS_CTRL_H
+
+#include <linux/sched.h>
+#include <linux/fs.h>
+
+enum ioctl_abi_format_qos{
+	QOS_IOCTL_ABI_ARM32,
+	QOS_IOCTL_ABI_AARCH64,
+};
+
+enum qos_ctrl_cmdid {
+	QOS_CTRL = 1,
+	QOS_POLICY,
+	QOS_CTRL_MAX_NR
+};
+
+#define QOS_CTRL_IPC_MAGIG	0xCC
+
+#define QOS_CTRL_BASIC_OPERATION \
+	_IOWR(QOS_CTRL_IPC_MAGIG, QOS_CTRL, struct qos_ctrl_data)
+#define QOS_CTRL_POLICY_OPERATION \
+	_IOWR(QOS_CTRL_IPC_MAGIG, QOS_POLICY, struct qos_policy_datas)
+
+#define NO_QOS -1
+#define NR_QOS 7
+#define NR_RT_QOS 2
+#define MIN_RT_QOS_LEVEL (NR_QOS - NR_RT_QOS)
+
+#define QOS_NUM_MAX 2000
+
+enum qos_manipulate_type {
+	QOS_APPLY = 1,
+	QOS_LEAVE,
+	QOS_GET,
+	QOS_OPERATION_CMD_MAX_NR,
+};
+
+#ifndef CONFIG_QOS_POLICY_MAX_NR
+#define QOS_POLICYS_COUNT 5
+#else
+#define QOS_POLICYS_COUNT CONFIG_QOS_POLICY_MAX_NR
+#endif
+
+/*
+ * keep match with auth_status
+ *
+ * range (QOS_POLICY_SYSTEM, QOS_POLICY_MAX_NR) could defined by user
+ * use ctrl_qos_policy
+ */
+enum qos_policy_type {
+	QOS_POLICY_DEFAULT = 1,    /* reserved for "NO QOS" */
+	QOS_POLICY_SYSTEM  = 2,    /* reserved for ROOT and SYSTEM */
+	QOS_POLICY_MAX_NR = QOS_POLICYS_COUNT,
+};
+
+struct qos_ctrl_data {
+	int pid;
+
+	/*
+	 * type:  operation type, see qos_manipulate_type
+	 * level: valid from 1 to NR_QOS. Larger value, more aggressive supply
+	 */
+	unsigned int type;
+
+	/*
+	 * user space level, range from [1, NR_QOS]
+	 *
+	 * NOTICE!!!:
+	 * minus 1 before use in kernel, so the kernel range is [0, NR_QOS)
+	 */
+	unsigned int level;
+
+	int qos;
+};
+
+struct qos_policy_data {
+	int nice;
+	int latency_nice;
+	int uclamp_min;
+	int uclamp_max;
+	int rt_sched_priority;
+	int policy;
+};
+
+#define QOS_FLAG_NICE			0x01
+#define QOS_FLAG_LATENCY_NICE		0x02
+#define QOS_FLAG_UCLAMP			0x04
+#define QOS_FLAG_RT			0x08
+
+#define QOS_FLAG_ALL	(QOS_FLAG_NICE			| \
+			 QOS_FLAG_LATENCY_NICE		| \
+			 QOS_FLAG_UCLAMP		| \
+			 QOS_FLAG_RT)
+
+struct qos_policy_datas {
+	/*
+	 * policy_type: id for qos policy, valid from [1, QOS_POLICY_MAX_NR)
+	 * policy_flag: control valid sched attr for policy, QOS_FLAG_ALL for whole access
+	 * policys:     sched params for specific level qos, minus 1 for matching struct in kerenl
+	 */
+	int policy_type;
+	unsigned int policy_flag;
+	struct qos_policy_data policys[NR_QOS];
+};
+
+struct auth_struct;
+
+int qos_apply(struct qos_ctrl_data *data);
+int qos_leave(struct qos_ctrl_data *data);
+int qos_get(struct qos_ctrl_data *data);
+
+void qos_switch(struct auth_struct *auth, int target_status);
+
+void init_task_qos(struct task_struct *p);
+void sched_exit_qos_list(struct task_struct *p);
+void remove_qos_tasks(struct auth_struct *auth);
+
+long do_qos_ctrl_ioctl(int abi, struct file *file, unsigned int cmd, unsigned long arg);
+
+#endif /* _QOS_CTRL_H */
+
diff --git a/include/linux/sched/rtg_auth.h b/include/linux/sched/rtg_auth.h
new file mode 100644
index 000000000..c9326f759
--- /dev/null
+++ b/include/linux/sched/rtg_auth.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * include/linux/sched/rtg_auth.h
+ *
+ * Copyright (c) 2022 Huawei Device Co., Ltd.
+ */
+
+#ifndef _RTG_AUTH_H
+#define _RTG_AUTH_H
+
+#include <linux/sched.h>
+#include <linux/sched/auth_ctrl.h>
+
+/*
+ * RTG authority flags for SYSTEM or ROOT
+ *
+ * keep sync with rtg_sched_cmdid
+ * when add a new cmd to rtg_sched_cmdid
+ * keep new_flag = (old_flag << 1) + 1
+ * up to now, next flag value is 0x3fff
+ */
+#define AF_RTG_ALL		0x1fff
+
+/*
+ * delegated authority for normal uid
+ * trim access range for RTG
+ */
+#define AF_RTG_DELEGATED	0x1fff
+
+bool check_authorized(unsigned int func_id, unsigned int type);
+
+#endif /* _RTG_AUTH_H */
+
diff --git a/kernel/exit.c b/kernel/exit.c
index 590fb2fa6..011600b20 100755
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -70,6 +70,13 @@
 #include <linux/sysfs.h>
 #include <linux/user_events.h>
 
+#ifdef CONFIG_QOS_CTRL
+#include <linux/sched/qos_ctrl.h>
+#endif
+
+#include <linux/uaccess.h>
+#include <asm/unistd.h>
+#include <asm/mmu_context.h>
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
 #include <asm/mmu_context.h>
@@ -831,6 +838,10 @@ void __noreturn do_exit(long code)
 	io_uring_files_cancel();
 	exit_signals(tsk);  /* sets PF_EXITING */
 
+#ifdef CONFIG_QOS_CTRL
+	sched_exit_qos_list(tsk);
+#endif
+
 	trace_android_vh_exit_check(current);
 
 	/* sync mm's RSS info before statistics gathering */
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 2adea38f9..9c793a8f9 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -37,6 +37,9 @@
 #include <linux/sched/rseq_api.h>
 #include <linux/sched/rt.h>
 
+#ifdef CONFIG_QOS_CTRL
+#include <linux/sched/qos_ctrl.h>
+#endif
 #include <linux/blkdev.h>
 #include <linux/context_tracking.h>
 #include <linux/cpuset.h>
@@ -4884,6 +4887,10 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 
 	trace_android_rvh_sched_fork(p);
 
+#ifdef CONFIG_QOS_CTRL
+	init_task_qos(p);
+#endif
+
 	__sched_fork(clone_flags, p);
 	/*
 	 * We mark the process as NEW here. This guarantees that
