diff --git a/include/linux/kcompress.h b/include/linux/kcompress.h
new file mode 100644
index 000000000..d83830df7
--- /dev/null
+++ b/include/linux/kcompress.h
@@ -0,0 +1,19 @@
+#ifndef _LINUX_KCOMPRESS_H
+#define _LINUX_KCOMPRESS_H
+
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+
+#define KCOMPRESS_FIFO_SIZE 256
+
+struct kfifo;
+
+struct kcompress_data {
+    wait_queue_head_t kcompressd_wait;
+    struct task_struct *kcompressd;
+    struct kfifo *kcompress_fifo;
+    spinlock_t kcompress_fifo_lock;
+};
+
+#endif /* _LINUX_KCOMPRESS_H */
diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h
index dfabbcb8e..deffd5fc5 100644
--- a/include/linux/memcontrol.h
+++ b/include/linux/memcontrol.h
@@ -367,6 +367,13 @@ enum page_memcg_data_flags {
 static inline bool folio_memcg_kmem(struct folio *folio);
 
 void do_traversal_all_lruvec(void);
+extern unsigned int bucket_order __read_mostly;
+void unpack_shadow(void *shadow, int *memcgidp, pg_data_t **pgdat,
+			    unsigned long *evictionp, bool *workingsetp);
+int mem_cgroup_move_account(struct folio *folio,
+			    bool compound,
+			    struct mem_cgroup *from,
+			    struct mem_cgroup *to);
 
 /*
  * After the initialization objcg->memcg is always pointing at
@@ -698,7 +705,7 @@ static inline int mem_cgroup_charge(struct folio *folio, struct mm_struct *mm,
 
 int mem_cgroup_swapin_charge_folio(struct folio *folio, struct mm_struct *mm,
 				  gfp_t gfp, swp_entry_t entry);
-void mem_cgroup_swapin_uncharge_swap(swp_entry_t entry);
+void mem_cgroup_swapin_uncharge_swap(swp_entry_t entry, unsigned int nr_pages);
 
 void __mem_cgroup_uncharge(struct folio *folio);
 
@@ -723,6 +730,15 @@ static inline void mem_cgroup_uncharge_list(struct list_head *page_list)
 	__mem_cgroup_uncharge_list(page_list);
 }
 
+void __mem_cgroup_uncharge_folios(struct folio_batch *folios);
+static inline void mem_cgroup_uncharge_folios(struct folio_batch *folios)
+{
+	if (mem_cgroup_disabled())
+		return;
+	__mem_cgroup_uncharge_folios(folios);
+}
+
+void mem_cgroup_replace_folio(struct folio *old, struct folio *new);
 void mem_cgroup_migrate(struct folio *old, struct folio *new);
 
 /**
@@ -1160,7 +1176,8 @@ void folio_copy_memcg(struct folio *folio);
 unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,
 						gfp_t gfp_mask,
 						unsigned long *total_scanned);
-
+						
+extern int mem_cgroup_init(void);
 #else /* CONFIG_MEMCG */
 
 #define MEM_CGROUP_ID_SHIFT	0
@@ -1205,6 +1222,14 @@ static inline void do_traversal_all_lruvec(void)
 {
 }
 
+static inline int mem_cgroup_move_account(struct folio *folio,
+					  bool compound,
+					  struct mem_cgroup *from,
+					  struct mem_cgroup *to)
+{
+	return 0;
+}
+
 static inline bool mem_cgroup_is_root(struct mem_cgroup *memcg)
 {
 	return true;
@@ -1267,7 +1292,7 @@ static inline int mem_cgroup_swapin_charge_folio(struct folio *folio,
 	return 0;
 }
 
-static inline void mem_cgroup_swapin_uncharge_swap(swp_entry_t entry)
+static inline void mem_cgroup_swapin_uncharge_swap(swp_entry_t entry, unsigned int nr)
 {
 }
 
@@ -1279,6 +1304,15 @@ static inline void mem_cgroup_uncharge_list(struct list_head *page_list)
 {
 }
 
+static inline void mem_cgroup_uncharge_folios(struct folio_batch *folios)
+{
+}
+
+static inline void mem_cgroup_replace_folio(struct folio *old,
+		struct folio *new)
+{
+}
+
 static inline void mem_cgroup_migrate(struct folio *old, struct folio *new)
 {
 }
@@ -1596,6 +1630,8 @@ unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,
 {
 	return 0;
 }
+
+static inline int mem_cgroup_init(void) { return 0; }
 #endif /* CONFIG_MEMCG */
 
 static inline void __inc_lruvec_kmem_state(void *p, enum node_stat_item idx)
@@ -1659,18 +1695,18 @@ static inline struct lruvec *folio_lruvec_relock_irq(struct folio *folio,
 	return folio_lruvec_lock_irq(folio);
 }
 
-/* Don't lock again iff page's lruvec locked */
-static inline struct lruvec *folio_lruvec_relock_irqsave(struct folio *folio,
-		struct lruvec *locked_lruvec, unsigned long *flags)
+/* Don't lock again iff folio's lruvec locked */
+static inline void folio_lruvec_relock_irqsave(struct folio *folio,
+		struct lruvec **lruvecp, unsigned long *flags)
 {
-	if (locked_lruvec) {
-		if (folio_matches_lruvec(folio, locked_lruvec))
-			return locked_lruvec;
+	if (*lruvecp) {
+		if (folio_matches_lruvec(folio, *lruvecp))
+			return;
 
-		unlock_page_lruvec_irqrestore(locked_lruvec, *flags);
+		unlock_page_lruvec_irqrestore(*lruvecp, *flags);
 	}
 
-	return folio_lruvec_lock_irqsave(folio, flags);
+	*lruvecp = folio_lruvec_lock_irqsave(folio, flags);
 }
 
 #ifdef CONFIG_CGROUP_WRITEBACK
@@ -1894,6 +1930,7 @@ static inline void count_objcg_event(struct obj_cgroup *objcg,
 bool obj_cgroup_may_zswap(struct obj_cgroup *objcg);
 void obj_cgroup_charge_zswap(struct obj_cgroup *objcg, size_t size);
 void obj_cgroup_uncharge_zswap(struct obj_cgroup *objcg, size_t size);
+bool mem_cgroup_zswap_writeback_enabled(struct mem_cgroup *memcg);
 #else
 static inline bool obj_cgroup_may_zswap(struct obj_cgroup *objcg)
 {
@@ -1907,6 +1944,11 @@ static inline void obj_cgroup_uncharge_zswap(struct obj_cgroup *objcg,
 					     size_t size)
 {
 }
+static inline bool mem_cgroup_zswap_writeback_enabled(struct mem_cgroup *memcg)
+{
+	/* if zswap is disabled, do not block pages going to the swapping device */
+	return true;
+}
 #endif
 
 #endif /* _LINUX_MEMCONTROL_H */
diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h
index a5077cf27..99b378d49 100644
--- a/include/linux/mmzone.h
+++ b/include/linux/mmzone.h
@@ -23,6 +23,7 @@
 #include <linux/page-flags.h>
 #include <linux/local_lock.h>
 #include <linux/android_kabi.h>
+#include <linux/kcompress.h>
 #include <asm/page.h>
 
 /* Free memory management - zoned buddy allocator.  */
@@ -1435,7 +1436,7 @@ typedef struct pglist_data {
 	struct memory_failure_stats mf_stats;
 #endif
 
-	ANDROID_KABI_RESERVE(1);
+	ANDROID_KABI_USE(1, struct kcompress_data *kcompress);
 } pg_data_t;
 
 #define node_present_pages(nid)	(NODE_DATA(nid)->node_present_pages)
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 1df4f0f0d..a1d2105a0 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -718,5 +718,7 @@ static inline bool mem_cgroup_swap_full(struct folio *folio)
 }
 #endif
 
+extern int vm_kcompressd;
+
 #endif /* __KERNEL__*/
 #endif /* _LINUX_SWAP_H */
diff --git a/include/linux/zswap.h b/include/linux/zswap.h
index 2a60ce39c..1cb6cbdea 100644
--- a/include/linux/zswap.h
+++ b/include/linux/zswap.h
@@ -15,6 +15,8 @@ bool zswap_load(struct folio *folio);
 void zswap_invalidate(int type, pgoff_t offset);
 void zswap_swapon(int type);
 void zswap_swapoff(int type);
+bool zswap_is_enabled(void);
+bool zswap_never_enabled(void);
 
 #else
 
@@ -32,6 +34,16 @@ static inline void zswap_invalidate(int type, pgoff_t offset) {}
 static inline void zswap_swapon(int type) {}
 static inline void zswap_swapoff(int type) {}
 
+static inline bool zswap_is_enabled(void)
+{
+    return false;
+}
+
+static inline bool zswap_never_enabled(void)
+{
+    return true;
+}
+
 #endif
 
 #endif /* _LINUX_ZSWAP_H */
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 354a2d294..7cab4789b 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -63,7 +63,7 @@
 #include <linux/mount.h>
 #include <linux/userfaultfd_k.h>
 #include <linux/pid.h>
-
+#include <linux/kcompress.h>
 #include "../lib/kstrtox.h"
 
 #include <linux/uaccess.h>
@@ -2046,6 +2046,9 @@ static struct ctl_table kern_table[] = {
 	{ }
 };
 
+int vm_kcompressd = 24;
+static int SYSCTL_KCOMPRESS_FIFO_SIZE = KCOMPRESS_FIFO_SIZE;
+
 static struct ctl_table vm_table[] = {
 	{
 		.procname	= "overcommit_memory",
@@ -2096,6 +2099,15 @@ static struct ctl_table vm_table[] = {
 		.extra1		= SYSCTL_ZERO,
 		.extra2		= SYSCTL_TWO_HUNDRED,
 	},
+	{
+		.procname	= "kcompressd",
+		.data		= &vm_kcompressd,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler = proc_dointvec_minmax,
+		.extra1		= SYSCTL_ZERO,
+		.extra2		= &SYSCTL_KCOMPRESS_FIFO_SIZE,
+	},	
 #ifdef CONFIG_NUMA
 	{
 		.procname	= "numa_stat",
diff --git a/mm/mm_init.c b/mm/mm_init.c
index a0b3b08f9..98790108c 100644
--- a/mm/mm_init.c
+++ b/mm/mm_init.c
@@ -27,6 +27,7 @@
 #include <linux/swap.h>
 #include <linux/cma.h>
 #include <linux/crash_dump.h>
+#include <linux/kfifo.h>
 #include "internal.h"
 #include "slab.h"
 #include "shuffle.h"
@@ -1458,6 +1459,9 @@ static void __meminit pgdat_init_internals(struct pglist_data *pgdat)
 	pgdat_init_kcompactd(pgdat);
 
 	init_waitqueue_head(&pgdat->kswapd_wait);
+	
+	/* kcompress will be initialized later in kswapd_run() */
+	pgdat->kcompress = NULL;
 	init_waitqueue_head(&pgdat->pfmemalloc_wait);
 
 	for (i = 0; i < NR_VMSCAN_THROTTLE; i++)
diff --git a/mm/page_io.c b/mm/page_io.c
index dbd78a5b1..e18b42a2a 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -25,6 +25,9 @@
 #include <linux/sched/task.h>
 #include <linux/delayacct.h>
 #include <linux/zswap.h>
+#include <linux/memcontrol.h>
+#include <linux/cpumask.h>
+#include <linux/kfifo.h>
 #include "swap.h"
 
 static void __end_swap_bio_write(struct bio *bio)
@@ -172,6 +175,114 @@ int generic_swapfile_activate(struct swap_info_struct *sis,
 	goto out;
 }
 
+/*
+ * do_swapout() - Write a folio to swap space
+ * @folio: The folio to write out
+ *
+ * This function writes the folio to swap space, either using zswap or
+ * synchronous write. It ensures that the folio is unlocked and the
+ * reference count is decremented after the operation.
+ */
+static inline void do_swapout(struct folio *folio)
+{
+	struct writeback_control wbc = {
+		.sync_mode = WB_SYNC_NONE,
+		.nr_to_write = SWAP_CLUSTER_MAX,
+		.range_start = 0,
+		.range_end = LLONG_MAX,
+		.for_reclaim = 1,
+	};
+
+	if (zswap_store(folio)) {
+		count_mthp_stat(folio_order(folio), MTHP_STAT_SWPOUT);
+		folio_unlock(folio);
+	} else {
+		__swap_writepage(&folio->page, &wbc); /* Implies folio_unlock(folio) */
+	}
+
+	/* Decrement the folio reference count */
+	folio_put(folio);
+}
+
+/*
+ * kcompressd_store() - Off-load folio compression to kcompressd
+ * @folio: The folio to compress
+ *
+ * This function attempts to off-load the compression of the folio to
+ * kcompressd. If kcompressd is not available or the folio cannot be
+ * compressed, it falls back to synchronous write.
+ *
+ * Returns true if the folio was successfully queued for compression,
+ * false otherwise.
+ */
+static bool kcompressd_store(struct folio *folio)
+{
+	pg_data_t *pgdat = NODE_DATA(numa_node_id());
+	unsigned int ret, sysctl_kcompressd = vm_kcompressd;
+	struct folio *head = NULL;
+
+	/* Comprehensive NULL checks to prevent crashes */
+	if (!pgdat || !pgdat->kcompress || !pgdat->kcompress->kcompress_fifo)
+		return false;
+
+	/* Only kswapd can use kcompressd */
+	if (!current_is_kswapd())
+		return false;
+
+	/* kcompressd must be enabled and running */
+	if (!sysctl_kcompressd || unlikely(!pgdat->kcompress->kcompressd))
+		return false;
+
+	/* We can only off-load anon folios */
+	if (!folio_test_anon(folio))
+		return false;
+
+	/* Fall back to synchronously return AOP_WRITEPAGE_ACTIVATE */
+	if (!mem_cgroup_zswap_writeback_enabled(folio_memcg(folio)))
+		return false;
+
+	/* Swap device must be sync-efficient */
+	if (!zswap_is_enabled() &&
+	    !data_race(swp_swap_info(folio->swap)->flags & SWP_SYNCHRONOUS_IO))
+		return false;
+
+	/* If the kcompress_fifo is full, we must swap out the head
+	 * folio to make space for the new folio.
+	 */
+	scoped_guard(spinlock_irqsave, &pgdat->kcompress->kcompress_fifo_lock) {
+		if (kfifo_len(pgdat->kcompress->kcompress_fifo) >= sysctl_kcompressd * sizeof(folio)) {
+			if (unlikely(!kfifo_out(pgdat->kcompress->kcompress_fifo, &head, sizeof(folio)))) {
+				/* Can't dequeue the head folio. Fall back to synchronous write. */
+				return false;
+			}
+		}
+	}
+
+	/* Increment the folio reference count to avoid it being freed */
+	folio_get(folio);
+
+	/* Enqueue the folio for compression */
+	scoped_guard(spinlock_irqsave, &pgdat->kcompress->kcompress_fifo_lock) {
+		ret = kfifo_in(pgdat->kcompress->kcompress_fifo, &folio, sizeof(folio));
+	}
+	
+	if (likely(ret)) {
+		/* We successfully enqueued the folio. wake up kcompressd */
+		wake_up_interruptible(&pgdat->kcompress->kcompressd_wait);
+	} else {
+		/* Enqueue failed, so we must cancel the reference count */
+		folio_put(folio);
+	}
+
+	/* If we had to swap out the head folio, do it now.
+	 * This will block until the folio is written out.
+	 */
+	if (head)
+		do_swapout(head);
+
+	return ret;
+}
+
 /*
  * We may have stale swap cache pages in memory: notice
  * them here and get rid of the unnecessary final write.
@@ -195,6 +306,15 @@ int swap_writepage(struct page *page, struct writeback_control *wbc)
 		folio_unlock(folio);
 		return ret;
 	}
+	
+	/*
+	 * Compression within zswap and zram might block rmap, unmap
+	 * of both file and anon pages, try to do compression async
+	 * if possible
+	 */
+	if (kcompressd_store(folio))
+		return 0;
+	
 	if (zswap_store(folio)) {
 		folio_start_writeback(folio);
 		folio_unlock(folio);
@@ -205,6 +325,48 @@ int swap_writepage(struct page *page, struct writeback_control *wbc)
 	return 0;
 }
 
+/*
+ * kcompressd() - Kernel thread for compressing folios
+ * @p: Pointer to pg_data_t structure
+ *
+ * This function runs in a kernel thread and waits for folios to be
+ * queued for compression. It processes the folios by calling do_swapout()
+ * on them, which handles the actual writing to swap space.
+ */
+int kcompressd(void *p)
+{
+	pg_data_t *pgdat = (pg_data_t *)p;
+	struct folio *folio;
+	
+	/* Validate pgdat and kcompress structure */
+	if (!pgdat || !pgdat->kcompress || !pgdat->kcompress->kcompress_fifo) {
+		pr_err("kcompressd: Invalid pgdat or kcompress structure\n");
+		return -EINVAL;
+	}
+	
+	/* kcompressd runs with PF_MEMALLOC and PF_KSWAPD flags set to
+	 * allow it to allocate memory for compression without being
+	 * restricted by the current memory allocation context.
+	 * Also PF_KSWAPD prevents Intel Graphics driver from crashing
+	 * the system in i915_gem_shrinker.c:i915_gem_shrinker_scan()
+	 */
+	current->flags |= PF_MEMALLOC | PF_KSWAPD;
+
+	while (!kthread_should_stop()) {
+		wait_event_interruptible(pgdat->kcompress->kcompressd_wait,
+				!kfifo_is_empty(pgdat->kcompress->kcompress_fifo) ||
+				kthread_should_stop());
+
+		if (kthread_should_stop())
+			break;
+
+		while (kfifo_out_locked(pgdat->kcompress->kcompress_fifo,
+				&folio, sizeof(folio), &pgdat->kcompress->kcompress_fifo_lock))
+			do_swapout(folio);
+	}
+	return 0;
+}
+
 static inline void count_swpout_vm_event(struct folio *folio)
 {
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
diff --git a/mm/swap.h b/mm/swap.h
index 693d1b281..a1591ec67 100644
--- a/mm/swap.h
+++ b/mm/swap.h
@@ -18,6 +18,7 @@ static inline void swap_read_unplug(struct swap_iocb *plug)
 void swap_write_unplug(struct swap_iocb *sio);
 int swap_writepage(struct page *page, struct writeback_control *wbc);
 void __swap_writepage(struct page *page, struct writeback_control *wbc);
+int kcompressd(void *p);
 
 /* linux/mm/swap_state.c */
 /* One swap address space for each 64M swap space */
@@ -148,5 +149,11 @@ static inline unsigned int folio_swap_flags(struct folio *folio)
 {
 	return 0;
 }
+
+static inline int kcompressd(void *p)
+{
+	return 0;
+}
+
 #endif /* CONFIG_SWAP */
 #endif /* _MM_SWAP_H */
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 4dcd272cd..e519b10de 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -17,7 +17,9 @@
 #include <linux/module.h>
 #include <linux/gfp.h>
 #include <linux/kernel_stat.h>
+#include <linux/kfifo.h>
 #include <linux/swap.h>
+#include <linux/zswap.h>
 #include <linux/pagemap.h>
 #include <linux/init.h>
 #include <linux/highmem.h>
@@ -8043,6 +8045,58 @@ unsigned long shrink_all_memory(unsigned long nr_to_reclaim)
 }
 #endif /* CONFIG_HIBERNATION */
 
+/*
+ * kcompress_init_for_node() - Initialize kcompress for a NUMA node
+ * @pgdat: The pg_data_t structure for the node
+ * @nid: Node ID
+ *
+ * This function initializes kcompress structures after the system
+ * is fully initialized, following Linux kernel best practices.
+ */
+static int kcompress_init_for_node(pg_data_t *pgdat, int nid)
+{
+	/* Already initialized or disabled */
+	if (pgdat->kcompress)
+		return 0;
+
+	pgdat->kcompress = kzalloc(sizeof(struct kcompress_data), GFP_KERNEL);
+	if (!pgdat->kcompress) {
+		pr_warn("Failed to allocate kcompress_data for node %d\n", nid);
+		return -ENOMEM;
+	}
+
+	/* Initialize synchronization primitives */
+	init_waitqueue_head(&pgdat->kcompress->kcompressd_wait);
+	spin_lock_init(&pgdat->kcompress->kcompress_fifo_lock);
+	pgdat->kcompress->kcompressd = NULL;
+	
+	/* Allocate FIFO structure */
+	pgdat->kcompress->kcompress_fifo = kzalloc(sizeof(struct kfifo), GFP_KERNEL);
+	if (!pgdat->kcompress->kcompress_fifo) {
+		pr_warn("Failed to allocate kcompress_fifo for node %d\n", nid);
+		goto cleanup_kcompress;
+	}
+
+	/* Initialize the FIFO with proper size */
+	if (kfifo_alloc(pgdat->kcompress->kcompress_fifo,
+			KCOMPRESS_FIFO_SIZE * sizeof(struct folio *),
+			GFP_KERNEL)) {
+		pr_warn("Failed to initialize kcompress_fifo for node %d\n", nid);
+		goto cleanup_fifo_struct;
+	}
+	
+	return 0;
+
+cleanup_fifo_struct:
+	kfree(pgdat->kcompress->kcompress_fifo);
+	pgdat->kcompress->kcompress_fifo = NULL;
+cleanup_kcompress:
+	kfree(pgdat->kcompress);
+	pgdat->kcompress = NULL;
+	pr_info("kcompressd disabled for node %d due to initialization failure\n", nid);
+	return -ENOMEM;
+}
+
 /*
  * This kswapd start function will be called by init and node-hot-add.
  */
@@ -8058,11 +8112,92 @@ void __meminit kswapd_run(int nid)
 			BUG_ON(system_state < SYSTEM_RUNNING);
 			pr_err("Failed to start kswapd on node %d\n", nid);
 			pgdat->kswapd = NULL;
+			goto out;
 		}
+		/* 
+		 * Initialize kcompress now that the system is fully running
+		 * This follows the kernel pattern of delayed initialization
+		 */
+		if (kcompress_init_for_node(pgdat, nid) == 0) {
+			/* Only create kcompressd if initialization succeeded */
+			pgdat->kcompress->kcompressd = kthread_create_on_node(
+				kcompressd, pgdat, nid, "kcompressd%d", nid);
+
+			printk(KERN_INFO "Kcompressd-Unofficial 0.5 by Masahito Suzuki (forked from Kcompressd by Qun-Wei Lin from MediaTek)\n");
+
+ 			if (IS_ERR(pgdat->kcompress->kcompressd)) {
+				pr_warn("Failed to start kcompressd on node %d: %ld\n",
+ 				       nid, PTR_ERR(pgdat->kcompress->kcompressd));
+ 				pgdat->kcompress->kcompressd = NULL;
+				/* kswapd continues without kcompressd */
+ 			} else {
+				pr_info("kcompressd started successfully on node %d\n", nid);
+ 				wake_up_process(pgdat->kcompress->kcompressd);
+ 			}
+		} else {
+			pr_info("kcompressd disabled on node %d\n", nid);
+ 		}
 	}
+	
+out:
 	pgdat_kswapd_unlock(pgdat);
 }
 
+/*
+ * kcompress_cleanup() - Clean up kcompress resources for a node
+ * @pgdat: The pg_data_t structure for the node
+ *
+ * This function safely cleans up all kcompress-related resources,
+ * ensuring proper shutdown of the kcompressd thread and deallocation
+ * of memory structures.
+ */
+static void kcompress_cleanup(pg_data_t *pgdat)
+{
+	if (!pgdat || !pgdat->kcompress)
+		return;
+
+	/* Stop kcompressd thread if it exists */
+	if (pgdat->kcompress->kcompressd) {
+		kthread_stop(pgdat->kcompress->kcompressd);
+		pgdat->kcompress->kcompressd = NULL;
+	}
+
+	/* Process any remaining folios in the FIFO before cleanup */
+	if (pgdat->kcompress->kcompress_fifo) {
+		struct folio *folio;
+		
+		/* Drain the FIFO and process remaining folios synchronously */
+		while (kfifo_out_locked(pgdat->kcompress->kcompress_fifo,
+					&folio, sizeof(folio), 
+					&pgdat->kcompress->kcompress_fifo_lock)) {
+			/* Process the folio synchronously before cleanup */
+			if (zswap_store(folio)) {
+				count_mthp_stat(folio_order(folio), MTHP_STAT_SWPOUT);
+				folio_unlock(folio);
+			} else {
+				struct writeback_control wbc = {
+					.sync_mode = WB_SYNC_NONE,
+					.nr_to_write = SWAP_CLUSTER_MAX,
+					.range_start = 0,
+					.range_end = LLONG_MAX,
+					.for_reclaim = 1,
+				};
+				__swap_writepage(&folio->page, &wbc);
+			}
+			folio_put(folio);
+		}
+
+		/* Free FIFO resources */
+		kfifo_free(pgdat->kcompress->kcompress_fifo);
+		kfree(pgdat->kcompress->kcompress_fifo);
+		pgdat->kcompress->kcompress_fifo = NULL;
+	}
+
+	/* Free the main kcompress structure */
+	kfree(pgdat->kcompress);
+	pgdat->kcompress = NULL;
+}
+
 /*
  * Called by memory hotplug when all memory in a node is offlined.  Caller must
  * be holding mem_hotplug_begin/done().
@@ -8078,6 +8213,10 @@ void __meminit kswapd_stop(int nid)
 		kthread_stop(kswapd);
 		pgdat->kswapd = NULL;
 	}
+	
+	/* Clean up kcompress resources */
+	kcompress_cleanup(pgdat);
+	
 	pgdat_kswapd_unlock(pgdat);
 }
 
