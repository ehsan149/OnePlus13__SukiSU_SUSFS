diff --git a/block/Kconfig.iosched b/block/Kconfig.iosched
index 27f11320b..7e773b7f3 100644
--- a/block/Kconfig.iosched
+++ b/block/Kconfig.iosched
@@ -16,6 +16,20 @@ config MQ_IOSCHED_KYBER
 	  synchronous writes, it will self-tune queue depths to achieve that
 	  goal.
 
+config MQ_IOSCHED_ADIOS
+	tristate "Adaptive Deadline I/O scheduler"
+	default y
+	help
+	  The Adaptive Deadline I/O Scheduler (ADIOS) is a multi-queue I/O
+	  scheduler with learning-based adaptive latency control.
+
+config MQ_IOSCHED_DEFAULT_ADIOS
+	bool "Enable ADIOS I/O scheduler as default MQ I/O scheduler"
+	depends on MQ_IOSCHED_ADIOS=y
+	default y
+	help
+	  Enable the ADIOS I/O scheduler as the default scheduler for MQ I/O.
+	  
 config IOSCHED_BFQ
 	tristate "BFQ I/O scheduler"
 	select BLK_ICQ
diff --git a/block/Makefile b/block/Makefile
index 46ada9dc8..2fd10afba 100644
--- a/block/Makefile
+++ b/block/Makefile
@@ -23,6 +23,7 @@ obj-$(CONFIG_BLK_CGROUP_IOLATENCY)	+= blk-iolatency.o
 obj-$(CONFIG_BLK_CGROUP_IOCOST)	+= blk-iocost.o
 obj-$(CONFIG_MQ_IOSCHED_DEADLINE)	+= mq-deadline.o
 obj-$(CONFIG_MQ_IOSCHED_KYBER)	+= kyber-iosched.o
+obj-$(CONFIG_MQ_IOSCHED_ADIOS)	+= adios.o
 bfq-y				:= bfq-iosched.o bfq-wf2q.o bfq-cgroup.o
 obj-$(CONFIG_IOSCHED_BFQ)	+= bfq.o
 
diff --git a/block/adios.c b/block/adios.c
new file mode 100644
index 000000000..58abf3c5e
--- /dev/null
+++ b/block/adios.c
@@ -0,0 +1,247 @@
+// ==================== START: FINAL FULL VERSION OF adios.c ====================
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Adaptive Deadline I/O Scheduler (ADIOS)
+ * Copyright (C) 2025 Masahito Suzuki / Ported by E.S.H
+ */
+#include <linux/bio.h>
+#include <linux/blkdev.hh>
+#include <linux/compiler.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/math.h>
+#include <linux/module.h>
+#include <linux/rbtree.h>
+#include <linux/sbitmap.h>
+#include <linux/slab.h>
+#include <linux/timekeeping.h>
+#include <linux/percpu.h>
+#include <linux/string.h>
+
+#include "elevator.h"
+#include "blk.h"
+#include "blk-mq.h"
+#include "blk-mq-sched.h"
+
+#define ADIOS_VERSION "2.2.1"
+
+enum adios_op_type { ADIOS_READ = 0, ADIOS_WRITE = 1, ADIOS_DISCARD = 2, ADIOS_OTHER = 3, ADIOS_OPTYPES = 4 };
+
+// For simplicity, we keep these hardcoded for the final test. Sysfs can be added later.
+static u64 default_global_latency_window = 32000000ULL;
+static u8  default_bq_refill_below_ratio = 25;
+static u64 default_latency_target[ADIOS_OPTYPES] = { [ADIOS_READ] = 1ULL*NSEC_PER_MSEC, [ADIOS_WRITE] = 2000ULL*NSEC_PER_MSEC, [ADIOS_DISCARD] = 8000ULL*NSEC_PER_MSEC };
+static u32 default_batch_limit[ADIOS_OPTYPES] = { [ADIOS_READ] = 24, [ADIOS_WRITE] = 48, [ADIOS_DISCARD] = 1, [ADIOS_OTHER] = 1 };
+static u32 default_dl_prio[2] = {7, 0};
+static const int adios_prio_to_weight[40] = { 88761, 71755, 56483, 46273, 36291, 29154, 23254, 18705, 14949, 11916, 9548, 7620, 6100, 4904, 3906, 3121, 2501, 1991, 1586, 1277, 1024, 820, 655, 526, 423, 335, 272, 215, 172, 137, 110, 87, 70, 56, 45, 36, 29, 23, 18, 15 };
+
+#define ADIOS_BQ_PAGES 2
+#define ADIOS_MAX_INSERTS_PER_LOCK 16
+
+struct adios_data {
+	spinlock_t pq_lock; struct list_head prio_queue;
+	struct rb_root_cached dl_tree[2]; spinlock_t lock;
+	u8 dl_queued; s64 dl_bias; s32 dl_prio[2];
+	u64 global_latency_window; u64 latency_target[ADIOS_OPTYPES];
+	u32 batch_limit[ADIOS_OPTYPES]; u32 async_depth; u8 bq_refill_below_ratio;
+	bool bq_page; bool more_bq_ready;
+	struct list_head batch_queue[ADIOS_BQ_PAGES][ADIOS_OPTYPES];
+	u32 batch_count[ADIOS_BQ_PAGES][ADIOS_OPTYPES]; spinlock_t bq_lock;
+	atomic64_t total_pred_lat;
+	struct kmem_cache *rq_data_pool; struct kmem_cache *dl_group_pool;
+	struct request_queue *queue;
+};
+
+struct dl_group { struct rb_node node; struct list_head rqs; u64 deadline; };
+struct adios_rq_data { struct list_head *dl_group; struct list_head dl_node; struct request *rq; u64 deadline; u64 pred_lat; u32 block_size; };
+
+static inline struct adios_rq_data *get_rq_data(struct request *rq) { return rq->elv.priv[0]; }
+static u8 adios_optype(struct request *rq) { switch (rq->cmd_flags & REQ_OP_MASK) { case REQ_OP_READ: return ADIOS_READ; case REQ_OP_WRITE: return ADIOS_WRITE; case REQ_OP_DISCARD: return ADIOS_DISCARD; default: return ADIOS_OTHER; } }
+static inline u8 adios_optype_not_read(struct request *rq) { return (rq->cmd_flags & REQ_OP_MASK) != REQ_OP_READ; }
+
+static void del_from_dl_tree(struct adios_data *ad, bool dl_idx, struct request *rq) {
+	struct rb_root_cached *root = &ad->dl_tree[dl_idx];
+	struct adios_rq_data *rd = get_rq_data(rq);
+	struct dl_group *dlg = container_of(rd->dl_group, struct dl_group, rqs);
+	list_del(&rd->dl_node);
+	if (list_empty(&dlg->rqs)) { rb_erase_cached(&dlg->node, root); kmem_cache_free(ad->dl_group_pool, dlg); }
+	rd->dl_group = NULL;
+	if (RB_EMPTY_ROOT(&ad->dl_tree[dl_idx].rb_root)) ad->dl_queued &= ~(1 << dl_idx);
+}
+
+static void add_to_dl_tree(struct adios_data *ad, bool dl_idx, struct request *rq) {
+	struct rb_root_cached *root = &ad->dl_tree[dl_idx];
+	struct rb_node **link = &(root->rb_root.rb_node), *parent = NULL;
+	bool leftmost = true;
+	struct adios_rq_data *rd = get_rq_data(rq);
+	struct dl_group *dlg;
+
+	rd->block_size = blk_rq_bytes(rq);
+	rd->pred_lat = 0; // Simplified
+	rd->deadline = rq->start_time_ns + ad->latency_target[adios_optype(rq)] + rd->pred_lat;
+
+	while (*link) {
+		dlg = rb_entry(*link, struct dl_group, node);
+		parent = *link;
+		if (rd->deadline < dlg->deadline) { link = &((*link)->rb_left); }
+		else { link = &((*link)->rb_right); leftmost = false; }
+	}
+
+	dlg = kmem_cache_zalloc(ad->dl_group_pool, GFP_ATOMIC);
+	if (!dlg) return;
+	dlg->deadline = rd->deadline;
+	INIT_LIST_HEAD(&dlg->rqs);
+	rb_link_node(&dlg->node, parent, link);
+	rb_insert_color_cached(&dlg->node, root, leftmost);
+	
+	list_add_tail(&rd->dl_node, &dlg->rqs);
+	rd->dl_group = &dlg->rqs;
+	ad->dl_queued |= 1 << dl_idx;
+}
+
+static void remove_request(struct adios_data *ad, struct request *rq) {
+	struct adios_rq_data *rd = get_rq_data(rq);
+	list_del_init(&rq->queuelist);
+	if (rd && rd->dl_group) del_from_dl_tree(ad, adios_optype_not_read(rq), rq);
+	elv_rqhash_del(rq->q, rq);
+	if (rq->q->last_merge == rq) rq->q->last_merge = NULL;
+}
+
+static struct adios_rq_data *get_dl_first_rd(struct adios_data *ad, bool idx) {
+	struct rb_node *first = rb_first_cached(&ad->dl_tree[idx]);
+	if (!first) return NULL;
+	struct dl_group *dl_group = rb_entry(first, struct dl_group, node);
+	return list_first_entry(&dl_group->rqs, struct adios_rq_data, dl_node);
+}
+
+static bool fill_batch_queues(struct adios_data *ad, u64 current_lat) {
+	struct adios_rq_data *rd;
+	u32 count = 0;
+	u8 optype;
+	bool page = !ad->bq_page, dl_idx, bias_idx, reduce_bias;
+	memset(&ad->batch_count[page], 0, sizeof(ad->batch_count[page]));
+	scoped_guard(spinlock_irqsave, &ad->lock) {
+		while (true) {
+			if (!ad->dl_queued) break;
+			dl_idx = ad->dl_queued >> 1;
+			rd = get_dl_first_rd(ad, dl_idx);
+			bias_idx = ad->dl_bias < 0;
+			if (ad->dl_queued == 0x3) {
+				struct adios_rq_data *trd[2] = {get_dl_first_rd(ad, 0), rd};
+				rd = trd[bias_idx];
+				reduce_bias = (trd[bias_idx]->deadline > trd[!bias_idx]->deadline);
+			} else { reduce_bias = (bias_idx == dl_idx); }
+			optype = adios_optype(rd->rq);
+			if (count && (ad->batch_count[page][optype] >= ad->batch_limit[optype] || (current_lat + rd->pred_lat) > ad->global_latency_window)) break;
+			if (reduce_bias) { s64 sign = ((int)bias_idx << 1) - 1; if (unlikely(!rd->pred_lat)) ad->dl_bias = sign; else ad->dl_bias += sign * (s64)((rd->pred_lat * adios_prio_to_weight[ad->dl_prio[bias_idx] + 20]) >> 10); }
+			remove_request(ad, rd->rq);
+			list_add_tail(&rd->rq->queuelist, &ad->batch_queue[page][optype]);
+			atomic64_add(rd->pred_lat, &ad->total_pred_lat);
+			current_lat += rd->pred_lat;
+			ad->batch_count[page][optype]++;
+			count++;
+		}
+	}
+	if (count) ad->more_bq_ready = true;
+	return count;
+}
+
+static struct request *dispatch_from_bq(struct adios_data *ad) {
+	struct request *rq = NULL;
+	u64 tpl;
+	guard(spinlock_irqsave)(&ad->bq_lock);
+	tpl = atomic64_read(&ad->total_pred_lat);
+	if (!ad->more_bq_ready && (!tpl || tpl < div_u64(ad->global_latency_window * ad->bq_refill_below_ratio, 100))) fill_batch_queues(ad, tpl);
+again:
+	for (u8 i = 0; i < ADIOS_OPTYPES; i++) {
+		if (!list_empty(&ad->batch_queue[ad->bq_page][i])) {
+			rq = list_first_entry(&ad->batch_queue[ad->bq_page][i], struct request, queuelist);
+			list_del_init(&rq->queuelist);
+			return rq;
+		}
+	}
+	if (ad->more_bq_ready) { ad->more_bq_ready = false; ad->bq_page = !ad->bq_page; goto again; }
+	return NULL;
+}
+
+static struct request *dispatch_from_pq(struct adios_data *ad) { struct request *rq = NULL; guard(spinlock_irqsave)(&ad->pq_lock); if (!list_empty(&ad->prio_queue)) { rq = list_first_entry(&ad->prio_queue, struct request, queuelist); list_del_init(&rq->queuelist); } return rq; }
+static struct request *adios_dispatch_request(struct blk_mq_hw_ctx *hctx) { struct adios_data *ad = hctx->queue->elevator->elevator_data; struct request *rq; rq = dispatch_from_pq(ad); if (rq) goto found; rq = dispatch_from_bq(ad); if (!rq) return NULL; found: rq->rq_flags |= RQF_STARTED; return rq; }
+static void adios_completed_request(struct request *rq, u64 now) { struct adios_data *ad = rq->q->elevator->elevator_data; struct adios_rq_data *rd = get_rq_data(rq); atomic64_sub(rd->pred_lat, &ad->total_pred_lat); }
+
+static void insert_request(struct blk_mq_hw_ctx *hctx, struct request *rq, blk_insert_t insert_flags, struct list_head *free) {
+	struct adios_data *ad = hctx->queue->elevator->elevator_data;
+	if (insert_flags & BLK_MQ_INSERT_AT_HEAD) { scoped_guard(spinlock_irqsave, &ad->pq_lock) list_add_tail(&rq->queuelist, &ad->prio_queue); return; }
+	if (blk_mq_sched_try_insert_merge(hctx->queue, rq, free)) return;
+	add_to_dl_tree(ad, adios_optype_not_read(rq), rq);
+	if (rq_mergeable(rq)) { elv_rqhash_add(hctx->queue, rq); if (!hctx->queue->last_merge) hctx->queue->last_merge = rq; }
+}
+
+static void adios_insert_requests(struct blk_mq_hw_ctx *hctx, struct list_head *list, blk_insert_t insert_flags) {
+	struct adios_data *ad = hctx->queue->elevator->elevator_data;
+	struct request *rq; bool stop = false; LIST_HEAD(free);
+	do { scoped_guard(spinlock_irqsave, &ad->lock) for (int i = 0; i < ADIOS_MAX_INSERTS_PER_LOCK; i++) { if (list_empty(list)) { stop = true; break; } rq = list_first_entry(list, struct request, queuelist); list_del_init(&rq->queuelist); insert_request(hctx, rq, insert_flags, &free); } } while (!stop);
+	blk_mq_free_requests(&free);
+}
+
+static void adios_prepare_request(struct request *rq) { struct adios_data *ad = rq->q->elevator->elevator_data; rq->elv.priv[0] = kmem_cache_zalloc(ad->rq_data_pool, GFP_ATOMIC); }
+static void adios_finish_request(struct request *rq) { struct adios_data *ad = rq->q->elevator->elevator_data; if (rq->elv.priv[0]) { kmem_cache_free(ad->rq_data_pool, rq->elv.priv[0]); rq->elv.priv[0] = NULL; } }
+static bool adios_bio_merge(struct request_queue *q, struct bio *bio, unsigned int nr_segs) { struct adios_data *ad = q->elevator->elevator_data; struct request *free = NULL; bool ret; scoped_guard(spinlock_irqsave, &ad->lock) ret = blk_mq_sched_try_merge(q, bio, nr_segs, &free); if (free) blk_mq_free_request(free); return ret; }
+static void adios_request_merged(struct request_queue *q, struct request *req, enum elv_merge type) { struct adios_data *ad = q->elevator->elevator_data; bool dl_idx = adios_optype_not_read(req); del_from_dl_tree(ad, dl_idx, req); add_to_dl_tree(ad, dl_idx, req); }
+static void adios_merged_requests(struct request_queue *q, struct request *req, struct request *next) { struct adios_data *ad = q->elevator->elevator_data; lockdep_assert_held(&ad->lock); remove_request(ad, next); }
+static bool adios_has_work(struct blk_mq_hw_ctx *hctx) { struct adios_data *ad = hctx->queue->elevator->elevator_data; return !list_empty_careful(&ad->prio_queue) || !list_empty_careful(&ad->batch_queue[0][0]) || !list_empty_careful(&ad->batch_queue[0][1]) || ad->dl_queued; }
+
+static void adios_init_icq(struct io_cq *icq) { /* Empty */ }
+static void adios_exit_icq(struct io_cq *icq) { /* Empty */ }
+static void adios_depth_updated(struct blk_mq_hw_ctx *hctx) { struct adios_data *ad = hctx->queue->elevator->elevator_data; ad->async_depth = hctx->queue->nr_requests; sbitmap_queue_min_shallow_depth(&hctx->sched_tags->bitmap_tags, 1); }
+static int adios_init_hctx(struct blk_mq_hw_ctx *hctx, unsigned int hctx_idx) { adios_depth_updated(hctx); return 0; }
+
+static void adios_exit_sched(struct elevator_queue *e) { struct adios_data *ad = e->elevator_data; kmem_cache_destroy(ad->rq_data_pool); kmem_cache_destroy(ad->dl_group_pool); kfree(ad); }
+static int adios_init_sched(struct request_queue *q, struct elevator_type *e) {
+	struct adios_data *ad;
+	struct elevator_queue *eq = elevator_alloc(q, e);
+	if (!eq) return -ENOMEM;
+	ad = kzalloc_node(sizeof(*ad), GFP_KERNEL, q->node);
+	if (!ad) { kobject_put(&eq->kobj); return -ENOMEM; }
+	ad->rq_data_pool = kmem_cache_create("adios_rq_data", sizeof(struct adios_rq_data), 0, SLAB_HWCACHE_ALIGN, NULL);
+	ad->dl_group_pool = kmem_cache_create("adios_dl_group", sizeof(struct dl_group), 0, SLAB_HWCACHE_ALIGN, NULL);
+	if (!ad->rq_data_pool || !ad->dl_group_pool) { if (ad->rq_data_pool) kmem_cache_destroy(ad->rq_data_pool); if (ad->dl_group_pool) kmem_cache_destroy(ad->dl_group_pool); kfree(ad); kobject_put(&eq->kobj); return -ENOMEM; }
+	eq->elevator_data = ad; ad->queue = q;
+	memcpy(ad->latency_target, default_latency_target, sizeof(default_latency_target));
+	memcpy(ad->batch_limit, default_batch_limit, sizeof(default_batch_limit));
+	memcpy(ad->dl_prio, default_dl_prio, sizeof(default_dl_prio));
+    ad->global_latency_window = default_global_latency_window;
+    ad->bq_refill_below_ratio = default_bq_refill_below_ratio;
+	for (int i = 0; i < ADIOS_BQ_PAGES; i++) for (int j = 0; j < ADIOS_OPTYPES; j++) INIT_LIST_HEAD(&ad->batch_queue[i][j]);
+	INIT_LIST_HEAD(&ad->prio_queue);
+	ad->dl_tree[0] = RB_ROOT_CACHED; ad->dl_tree[1] = RB_ROOT_CACHED;
+	spin_lock_init(&ad->lock); spin_lock_init(&ad->pq_lock); spin_lock_init(&ad->bq_lock);
+	blk_queue_flag_set(QUEUE_FLAG_SQ_SCHED, q);
+	q->elevator = eq;
+	return 0;
+}
+
+static ssize_t adios_version_show(struct elevator_queue *e, char *page) { return sprintf(page, "%s\n", ADIOS_VERSION); }
+static struct elv_fs_entry adios_sched_attrs[] = { __ATTR(adios_version, 0444, adios_version_show, NULL), __ATTR_NULL };
+
+static struct elevator_type mq_adios = {
+	.ops = {
+		.insert_requests	= adios_insert_requests, .dispatch_request	= adios_dispatch_request,
+		.prepare_request    = adios_prepare_request, .finish_request     = adios_finish_request,
+		.request_merged     = adios_request_merged, .requests_merged    = adios_merged_requests,
+        .bio_merge          = adios_bio_merge,
+		.has_work		    = adios_has_work, .exit_sched		= adios_exit_sched,
+		.init_sched		    = adios_init_sched, .init_icq           = adios_init_icq,
+		.exit_icq           = adios_exit_icq, .init_hctx          = adios_init_hctx,
+        .depth_updated      = adios_depth_updated, .completed_request = adios_completed_request,
+	},
+    .icq_size = sizeof(struct io_cq), .icq_align = __alignof__(struct io_cq),
+    .elevator_attrs = adios_sched_attrs, .elevator_name = "adios", .elevator_owner = THIS_MODULE,
+};
+
+static int __init adios_init(void) { return elv_register(&mq_adios); }
+static void __exit adios_exit(void) { elv_unregister(&mq_adios); }
+module_init(adios_init); module_exit(adios_exit);
+MODULE_AUTHOR("E.S.H"); MODULE_LICENSE("GPL"); MODULE_DESCRIPTION("ADIOS I/O Scheduler");
+// ==================== END: FINAL FULL VERSION OF adios.c ====================
+\ No newline at end of file
\ No newline at end of file
